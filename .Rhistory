p_load("shinyjs")
p_load("choroplethr")
p_load("choroplethrMaps")
p_load("DescTools")
p_load("readxl")
p_load("devtools")
p_load("ggplot2")
p_load("plotly")
p_load("DT")
library(pacman)
p_load("pacman")
p_load("tidyverse")
p_load("knitr")
p_load("markdown")
p_load("data.table")
p_load("sqldf")
p_load("ggplot2")
p_load("lubridate")
p_load("foreach")
p_load("RSQLite")
p_load("shiny")
p_load("shinyjs")
p_load("choroplethr")
p_load("choroplethrMaps")
p_load("DescTools")
p_load("readxl")
p_load("devtools")
p_load("ggplot2")
p_load("plotly")
p_load("DT")
runApp('9-data-products/week-4')
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
shiny::runApp()
setwd("C:/dev/r-course/9-data-products/week-4")
unzip(zipfile  = "./data/lending-club-loan-data.zip")
unzip(zipfile  = "./data/lending-club-loan-data.zip", exdir=paste(getwd(), "data"))
suppressMessages(rm(list = ls()))
unzip(zipfile  = "./data/lending-club-loan-data.zip", exdir=paste(getwd(), "data"))
outdir <- paste(getwd(), "data")
outdir
unzip(zipfile  = "./data/lending-club-loan-data.zip", exdir=paste(getwd(), "/data"))
outdir <- paste(getwd(), "/data")
unzip(zipfile  = "./data/lending-club-loan-data.zip", exdir=outdir)
unzip(zipfile  = "./data/lending-club-loan-data.zip", outdir)
switch(Sys.info()[['sysname']],
Windows= {suppressMessages(setwd("C:/dev/r-course/9-data-products/week-4"))},
Linux  = {suppressMessages(setwd("~/srv/connect/apps/loan_book_analyser"))},
Darwin = {print("I'm a Mac.")})
outdir <- paste(getwd(), "/data")
unzip(zipfile  = "./data/lending-club-loan-data.zip", outdir)
shiny::runApp()
outdir <- paste(getwd(), "/data")
paste(zipfile, "lending-club-loan-data.zip")
paste(outdir, "lending-club-loan-data.zip")
paste(trimws(outdir), "lending-club-loan-data.zip")
outdir <- paste(trimws(getwd()), "/data")
paste(trimws(outdir), "lending-club-loan-data.zip")
outdir <- paste(trimws(getwd()), "data")
outdir
outdir <- paste(trimws(getwd()), "//data")
outdir
outdir <- paste0(trimws(getwd()), "data")
outdir
outdir <- paste0(trimws(getwd()), "/data")
outdir
zippedFile <- paste0(trimws(outdir), "lending-club-loan-data.zip")
zippedFile
outdir <- paste0(trimws(getwd()), "/data")
outdir
zippedFile <- paste0(trimws(outdir), "lending-club-loan-data.zip")
zippedFile
outdir <- paste0(trimws(getwd()), "/data/")
outdir
zippedFile <- paste0(trimws(outdir), "lending-club-loan-data.zip")
zippedFile
unzip(zipfile  = zippedFile, outdir)
unzip(zippedFile, outdir)
zippedFile
outdir
unzip(zippedFile, outdir)
unzip(zippedFile)
?unzip
unzip(zippedFile, exdir=outdir)
unzip(zipfile=zippedFile, exdir=outdir)
unzip(zipfile=zippedFile,exdir = outdir)
outdir <- paste0(trimws(getwd()), "/data")
outdir
zippedFile <- paste0(trimws(outdir), "/lending-club-loan-data.zip")
zippedFile
unzip(zipfile=zippedFile,exdir = outdir)
shiny::runApp()
shiny::runApp()
shiny::runApp()
runApp()
runApp()
runApp()
source('C:/dev/r-course/9-data-products/week-4/index.R', echo=TRUE)
install.packages("rsconnect")
source('C:/dev/r-course/9-data-products/week-4/index.R', echo=TRUE)
source('C:/dev/r-course/9-data-products/week-4/index.R', echo=TRUE)
source('C:/dev/r-course/9-data-products/week-4/index.R', echo=TRUE)
shiny::runApp()
runApp()
shiny::runApp()
source('C:/dev/r-course/9-data-products/week-4/index.R', echo=TRUE)
source("C:/dev/r-course/10-capstone/quiz1.R", encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
p_load("stringr")
suppressMessages(setwd("c:/dev/r-course/10-capstone"))
blogs <- readLines("final/en_US/en_US.blogs.txt")
blogs <- readLines("final/en_US/en_US.blogs.txt")
blogs <- readLines("final/en_US/en_US.blogs.txt", local = locale(encoding = "latin1"))
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
unzip(zipfile = "c:/dev/r-course/10-capstone/data.zip", exdir = "c:/dev/r-course/10-capstone")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
<<<<<<< HEAD
sessionInfo()
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
=======
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
Sys.getlocale()
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
library(tm)
?tm
???tm
??tm
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
help.search()
help
help.start
help.start()
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
repo <- new("TextRepository", .Data = list(blogs,news,twitter))
?tm
tmIndex
TermDocMatrlibrary(tm)
library(tm)
blogs <- read_file("final/en_US/en_US.blogs.txt")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
summary(data.blogs)
DescTools::Desc(data.blogs, plotit = TRUE)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
src <- DirSource("c:/dev/r-course/10-capstone/final")
corpus <- Corpus(src)
corpus[[1]]$content
strwrap(corpus[[1]])
class(corpus)
corpus
src <- DirSource("c:/dev/r-course/10-capstone/final/en_US/")
corpus <- Corpus(src)
?Corpus
corpus <- Corpus(c(data.blogs, data.news,data.twitter))
lst <- as.list(c(data.blogs, data.news, data.twitter))
corpus <- Corpus(lst)
df.blogs <- data.frame(text = unlist(sapply(data.blogs, `[`, "content")), stringsAsFactors = F)
df.twitter <- data.frame(text = unlist(sapply(data.twitter, `[`, "content")), stringsAsFactors = F)
df.news <- data.frame(text = unlist(sapply(data.news, `[`, "content")), stringsAsFactors = F)
corpus <- Corpus(c(df.blogs,df.news,df.twitter))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
install_packages("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust","cluster", "igraph", "fpc")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
lst <- list("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc")
do.call("install_extra_packages", lst)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
lst <- list("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc") install_extra_packages(lst)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
df.blogs <- sample(data.frame(text = unlist(sapply(data.blogs, `[`, "content")), stringsAsFactors = F), 1000)
df.blogs <- sample(data.frame(text = unlist(sapply(data.blogs, `[`, "content")), stringsAsFactors = F), 30)
sample.blogs <- sample(data.frame(text = unlist(sapply(data.blogs, `[`, "content")), stringsAsFactors = F), 100) sample.news <- sample(data.frame(text = unlist(sapply(data.news, `[`, "content")), stringsAsFactors = F), 100) sample.twitter <- sample(data.frame(text = unlist(sapply(data.twitter, `[`, "content")), stringsAsFactors = F), 100) sample.all <- c(df.blogs, df.news, df.twitter)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
corpus <- Corpus(VectorSource(list(sample.all)))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
sample.blogs <- sample(data.frame(text = unlist(sapply(data.blogs, `[`, "content")), stringsAsFactors = F), 100) sample.news <- sample(data.frame(text = unlist(sapply(data.news, `[`, "content")), stringsAsFactors = F), 100) sample.twitter <- sample(data.frame(text = unlist(sapply(data.twitter, `[`, "content")), stringsAsFactors = F), 100) sample.all <- c(sample.blogs, sample.news, sample.twitter)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
data <- readLines(con)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
data.all <- c(data.blogs, data.news, data.twitter)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
uniGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1)) biGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2)) triGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3)) uniGramMatrix <- TermDocumentMatrix(documents, control = list(tokenize = uniGramTokenizer)) biGramMatrix <- TermDocumentMatrix(documents, control = list(tokenize = biGramTokenizer)) triGramMatrix <- TermDocumentMatrix(documents, control = list(tokenize = triGramTokenizer))
uniGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1)) biGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2)) triGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3)) uniGramMatrix <- TermDocumentMatrix(documents, control = list(tokenize = uniGramTokenizer)) biGramMatrix <- TermDocumentMatrix(documents, control = list(tokenize = biGramTokenizer)) triGramMatrix <- TermDocumentMatrix(documents, control = list(tokenize = triGramTokenizer))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
sample.all <- sample(c(sample.blogs, sample.news, sample.twitter), 5000, replace = F)
sample.blogs <- sample(data.frame(text = unlist(sapply(data.blogs, `[`, "content")), stringsAsFactors = F), 20000) sample.news <- sample(data.frame(text = unlist(sapply(data.news, `[`, "content")), stringsAsFactors = F), 20000) sample.twitter <- sample(data.frame(text = unlist(sapply(data.twitter, `[`, "content")), stringsAsFactors = F), 20000) sample.all <- sample(c(sample.blogs, sample.news, sample.twitter), 5000, replace = F)
sample.blogs <- sample(data.frame(text = unlist(sapply(data.blogs, `[`, "content")), stringsAsFactors = F), 20000) sample.news <- sample(data.frame(text = unlist(sapply(data.news, `[`, "content")), stringsAsFactors = F), 20000) sample.twitter <- sample(data.frame(text = unlist(sapply(data.twitter, `[`, "content")), stringsAsFactors = F), 20000) sample.all <- sample(c(sample.blogs, sample.news, sample.twitter), 5000, replace = F) remove(sample.blogs) remove(sample.news) remove(sample.twitter)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
sample.all <- c(sample.blogs, sample.news, sample.twitter)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
corpus <- Corpus(VectorSource(list(sample.blogs, sample.news, sample.twitter)))
remove(data.blogs) remove(data.news) remove(data.twitter) remove(sample.blogs) remove(sample.news) remove(sample.twitter) remove(sample.all)
corpus <- tm_map(corpus, tolower) corpus <- tm_map(corpus, removePunctuation) corpus <- tm_map(corpus, removeNumbers) corpus <- tm_map(corpus, removeWords, stopwords("english")) corpus <- tm_map(corpus, stripWhitespace)   corpus <- tm_map(corpus, toEmpty, "#\\w+")   corpus <- tm_map(corpus, toEmpty, "(\\b\\S+\\@\\S+\\..{1,3}(\\s)?\\b)")   corpus <- tm_map(corpus, toEmpty, "@\\w+")   corpus <- tm_map(corpus, toEmpty, "http[^[:space:]]*")   corpus <- tm_map(corpus, toSpace, "/|@|\\|") writeCorpus(corpus, filenames = "corpus.txt")
# clean  ##  custom content transformers toEmpty <- content_transformer(function(x, pattern) gsub(pattern, "", x, fixed = TRUE)) toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x, fixed = TRUE)) corpus <- tm_map(corpus, tolower) corpus <- tm_map(corpus, removePunctuation) corpus <- tm_map(corpus, removeNumbers) corpus <- tm_map(corpus, removeWords, stopwords("english")) corpus <- tm_map(corpus, stripWhitespace)   corpus <- tm_map(corpus, toEmpty, "#\\w+")   corpus <- tm_map(corpus, toEmpty, "(\\b\\S+\\@\\S+\\..{1,3}(\\s)?\\b)")   corpus <- tm_map(corpus, toEmpty, "@\\w+")   corpus <- tm_map(corpus, toEmpty, "http[^[:space:]]*")   corpus <- tm_map(corpus, toSpace, "/|@|\\|")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
freqTerms <- findFreqTerms(matrix.uni)
freqTerms <- findFreqTerms(matrix.uni, lowfreq = 3000)
freqTerms <- findFreqTerms(matrix.uni, lowfreq = 5000)
freqTerms <- findFreqTerms(matrix.uni, lowfreq = 4000)
list_freqs <- lapply(corpus$dimnames$Docs,               function(i) findFreqTerms(corpus[corpus$dimnames$Docs == i], 2000))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
df <- as.data.frame(as.matrix(corpus))
df <- as.data.frame(as.matrix(freqTerms))
df <- as.data.frame(as.matrix(dtm$dimnames$Docs))
df <- as.data.frame(as.matrix(corpus[1]))
tm <- DocumentTermMatrix(corpus)
dtm <- DocumentTermMatrix(corpus)
df <- as.data.frame(as.matrix(dtm))
df <- data.frame(t(df))
require(ggplot2) ggplot(df, aes(X127, X144)) +     geom_text(label = rownames(df),            position = position_jitter())
df <- names("word", "blog_count", "news_count", "twitter_count")
df <- colnames("word", "blog_count", "news_count", "twitter_count")
cols <- c("word", "blog_count", "news_count", "twitter_count")
colnames(df) <- cols
cols <- c("blog_count", "news_count", "twitter_count") colnames(df) <- cols
top5.blogwords <- sqldf("select * from df order by blog_count desc limit 5")
df <- as.data.frame(as.matrix(dtm))
p_load("quanteda")
summary(corpus)
dtm <- DocumentTermMatrix(corpus)
plot(dtm, terms = findFreqTerms(dtm, lowfreq = 2000)[1:25], corThreshold = 0.5)
p_load("Rgraphviz")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
setDT(df, keep.rownames = TRUE)[]
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
cols <- c("word", "blog_count", "news_count", "twitter_count")
plot(dtm, terms = findFreqTerms(dtm, lowfreq = 2000)[1:25], corThreshold = 0.5)
df <- as.data.frame(as.matrix(dtm)) # and transpose for plotting df <- data.frame(t(df)) setDT(df, keep.rownames = TRUE)[] cols <- c("word", "blog_count", "news_count", "twitter_count") colnames(df) <- cols
top5.blogwords <- sqldf("select * from df order by blog_count desc limit 5")
top5.newswords <- sqldf("select word, news_count from df order by news_count desc limit 5")
top5.twitterwords <- sqldf("select word,twitter_count from df order by twitter_count desc limit 5")
top5.blogwords <- sqldf("select word, blog_count from df order by blog_count desc limit 5") top5.newswords <- sqldf("select word, news_count from df order by news_count desc limit 5") top5.twitterwords <- sqldf("select word,twitter_count from df order by twitter_count desc limit 5")
summary(corpus)
myCorpus <- corpus(corpus)
myCorpus <- quanteda::corpus(corpus)
ggplot(top5.blogwords, aes(x = word, fill = blog_count)) +     geom_histogram(position = "identity", bins = 20, show.legend = FALSE) +     facet_wrap(~blog_count, ncol = 1)
install.packages("Rcpp", lib="C:/Users/chris/Documents/R/win-library/3.4")
ggplot(top5.blogwords, aes(x = word, fill = blog_count)) +     geom_histogram(position = "identity", bins = 20, show.legend = FALSE) +     facet_wrap(~blog_count, ncol = 1)
ggplot(top5.blogwords, aes(x = word, fill = blog_count)) +     geom_histogram(position = "identity", bins = 20, show.legend = FALSE,stat = "count") +     facet_wrap(~blog_count, ncol = 1)
myCorpus <- quanteda::corpus(corpus) # build a new corpus from the texts
p_load("Rcpp")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
install.packages("Rcpp")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
corpus.data <- Corpus(VectorSource(lst)) corpus.summary <- quanteda::corpus(tmVCorpus(lst))
corpus.summary <- quanteda::corpus(VCorpus(lst))
inspect(corpus.data)
nTerms(corpus.data)
nDocs(corpus.data)
meta(corpus.data)
meta(corpus.data)
meta(corpus.data[1])
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
matrix.uni <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.uni))
matrix.bi <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.bi))
list_freqs <- lapply(corpus.data$dimnames$Docs,               function(i) findFreqTerms(corpus.data[corpus.data$dimnames$Docs == i], 2000))
freq.terms <- findFreqTerms(matrix.uni, lowfreq = 4000) freq.expressions <- findFreqTerms(bi, lowfreq = 4000)
freq.terms <- findFreqTerms(matrix.uni, lowfreq = 4000) freq.expressions <- findFreqTerms(matrix.bi, lowfreq = 4000)
nTerms(dtm)
dtm <- DocumentTermMatrix(corpus)
#https://stackoverflow.com/questions/17294824/counting-words-in-a-single-document-from-corpus-in-r-and-putting-it-in-dataframe dtm <- DocumentTermMatrix(corpus.data)
nTerms(dtm)
matrix.tri <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.tri))
tokenizer.tri <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
matrix.tri <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.tri))
freq.terms <- findFreqTerms(matrix.uni, lowfreq = 4000) freq.expressions <- findFreqTerms(matrix.bi, lowfreq = 4000)
freq.terms <- findFreqTerms(matrix.uni, 160)
freq.terms <- findFreqTerms(matrix.uni, 10) freq.expressions <- findFreqTerms(matrix.bi, 10)
list_freqs()
# find freq words for each doc, one by one list_freqs <- lapply(corpus.data$dimnames$Docs,               function(i) findFreqTerms(corpus.data[corpus.data$dimnames$Docs == i], 2000))
# find freq words for each doc, one by one list_freqs <- lapply(corpus.data$dimnames$Docs,               function(i) findFreqTerms(corpus.data[corpus.data$dimnames$Docs == i], 2000))
list_freqs()
list_freqs
#Tokenize sample into Unigrams, Bigrams and Trigrams tokenizer.uni <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1)) tokenizer.bi <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2)) tokenizer.tri <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3)) matrix.uni <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.uni)) matrix.bi <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.bi)) freq.terms <- findFreqTerms(matrix.uni, 10) freq.expressions <- findFreqTerms(matrix.bi, 10)
plot(dtm, terms = freq.terms[1:5], corThreshold = 0.5)
freq.terms <- findFreqTerms(matrix.uni, 10)
dtm <- DocumentTermMatrix(corpus.data)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
p_load("tidytext")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
tidy_books %>%     anti_join(stop_words) %>%     count(df$word) %>%     with(wordcloud(df$word, n, max.words = 100))
p_load("tidytext")
tidy_books %>%     anti_join(stop_words) %>%     count(df$word) %>%     with(wordcloud(df$word, n, max.words = 100))
p_load("tidytext")
tidy_books %>%     anti_join(stop_words) %>%     count(df$word) %>%     with(wordcloud(df$word, n, max.words = 100))
cols <- c("word", "blog_count", "news_count", "twitter_count") colnames(df) <- cols top.blogwords <- sqldf("select word, blog_count from df order by blog_count desc limit 100") top.newswords <- sqldf("select word, news_count from df order by news_count desc limit 100") top.twitterwords <- sqldf("select word,twitter_count from df order by twitter_count desc limit 100")
top.blogwords %>%     count(df$word) %>%     with(wordcloud(df$word, n, max.words = 100))
cols <- c("word", "blog_count", "news_count", "twitter_count") colnames(df) <- cols top.blogwords <- sqldf("select word, blog_count from df order by blog_count desc limit 100") top.newswords <- sqldf("select word, news_count from df order by news_count desc limit 100") top.twitterwords <- sqldf("select word,twitter_count from df order by twitter_count desc limit 100")
df %>%     count(df$word) %>%     with(wordcloud(df$word, n, max.words = 100))
top.blogwords <- sqldf("select word, blog_count from df order by blog_count desc limit 5") top.newswords <- sqldf("select word, news_count from df order by news_count desc limit 5") top.twitterwords <- sqldf("select word,twitter_count from df order by twitter_count desc limit 5")
nTerms(dtm)
sample.data <- bind_rows(sample.blogs %>%                       mutate(person = "blogs"),                         sample.news %>%                       mutate(person = "news"),                         sample.twitter %>%                       mutate(person = "twitter"),                       ) %>%                       mutate(timestamp = ymd_hms(timestamp))
#download_zip_files() # read files and adjust for encoding data.blogs <- read_file("final/en_US/en_US.blogs.txt") data.news <- read_file("final/en_US/en_US.news.txt") data.twitter <- read_file("final/en_US/en_US.twitter.txt") #data.all <- c(data.blogs, data.news, data.twitter) #sample data to speed things up sample.blogs <- sample(data.blogs, 20000) sample.news <- sample(data.news, 20000) sample.twitter <- sample(data.twitter, 20000) sample.all <- sample(c(sample.blogs, sample.news, sample.twitter), size = 10000, replace = TRUE) sample.data <- bind_rows(sample.blogs %>%                       mutate(person = "blogs"),                         sample.news %>%                       mutate(person = "news"),                         sample.twitter %>%                       mutate(person = "twitter"),                       ) %>%                       mutate(timestamp = ymd_hms(timestamp))
sample.data <- bind_rows(sample.blogs %>%                       mutate(person = "blogs"),                         sample.news %>%                       mutate(person = "news"),                         sample.twitter %>%                       mutate(person = "twitter"),                       ) %>%                       mutate(timestamp = ymd_hms(timestamp))
sample.data <- bind_rows(sample.blogs %>%                       mutate(person = "blogs"),                         sample.news %>%                       mutate(person = "news"),                         sample.twitter %>%                       mutate(person = "twitter"),                       ))
sample.data <- bind_rows(sample.blogs %>%                       mutate(person = "blogs"),                         sample.news %>%                       mutate(person = "news"),                         sample.twitter %>%                       mutate(person = "twitter")                       )
sample.data <- bind_rows(sample.blogs %>%                       mutate(person = "blogs"),                         sample.news %>%                       mutate(person = "news"),                         sample.twitter %>%                       mutate(person = "twitter")                       )
sample.data <- bind_rows(sample.blogs %>%                       mutate(source = "blogs"),                         sample.news %>%                         mutate(source = "news"),                         sample.twitter %>%                         mutate(source = "twitter")                       )
sample.data <- bind_rows(sample.blogs %>%                       mutate(source = "blogs"),                         sample.news %>%                         mutate(source = "news"),                         sample.twitter %>%                       mutate(source = "twitter")                       )
sample.data <- bind_rows(sample.blogs %>%                       mutate(x = "blogs"),                         sample.news %>%                         mutate(x = "news"),                         sample.twitter %>%                       mutate(x = "twitter")                       )
df %>%     count(df$word) %>%     with(wordcloud(df$word, n, max.words = 100))
warnings()p_load("revealjs")
using("revealjs")
install.packages("revealjs", type = "source")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
par(mfrow=c(1,1))
wordcloud(matrix.uni, freq.uni, scale=c(9,1), max.words=40, random.order=FALSE, colors=brewer.pal(7, "Dark2"))
top.blogwords$document <- "blog"
top.newswords$document <- "news"
top.twitterwords$document <- "twitter"
wordcloud(freq.uni, matrix.uni, scale=c(9,1), max.words=40, random.order=FALSE, colors=brewer.pal(7, "Dark2"))
wordcloud(top.blogwords, 1, scale = c(9, 1), max.words = 40, random.order = FALSE, colors = brewer.pal(7, "Dark2"))
w
wordcloud(top.blogwords, top.blogwords$blog_count, scale = c(9, 1), max.words = 40, random.order = FALSE, colors = brewer.pal(7, "Dark2"))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
dtm <- DocumentTermMatrix(corpus.data)     m <- as.matrix(dtm)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), freq = v)     head(d, 10)
dtm <- TermDocumentMatrix(corpus.data)     m <- as.matrix(dtm)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), freq = v)     head(d, 10)
delim <- " \\r\\n\\t.,;:\"()?!"     tokenizer.uni <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))     tokenizer.bi <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2), delimiters = delim)     tokenizer.tri <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3), delimiters = delim)     matrix.uni <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.uni))     matrix.bi <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.bi))     matrix.tri <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.tri))     freq.uni <- findFreqTerms(matrix.uni, 5,5)     freq.bi <- findFreqTerms(matrix.bi,  5,5)     freq.tri <- findFreqTerms(matrix.bi,  5,5)
Uni_gram <- NGramTokenizer(corpus.data, Weka_control(min = 1, max = 1)) Unigram = data.frame(table(Uni_gram)) Unigram = Unigram[order(Unigram$Freq, decreasing = T),] Ug <- Unigram[1:40,]
Uni_gram <- NGramTokenizer(corpus.data, Weka_control(min = 1, max = 1))
delim <- " \\r\\n\\t.,;:\"()?!"     tokenizer.uni <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))     tokenizer.bi <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2), delimiters = delim)     tokenizer.tri <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3), delimiters = delim) dtm.uni <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.uni)) dtm.bi <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.bi)) dtm.tri <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.tri)) freq.uni <- findFreqTerms(dtm.uni, 5,5) freq.bi <- findFreqTerms(dtm.bi,  5,5) freq.tri <- findFreqTerms(dtm.bi, 5, 5)
dtm <- TermDocumentMatrix(corpus.data)     m <- as.matrix(dtm)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), freq = v)     head(d, 10)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
delim <- " \\r\\n\\t.,;:\"()?!"     tokenizer.uni <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))     tokenizer.bi <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2), delimiters = delim)     tokenizer.tri <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3), delimiters = delim)
dtm.uni <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.uni))     dtm.bi <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.bi))     dtm.tri <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.tri))
freq.uni <- findFreqTerms(dtm.uni, 5,5)     freq.bi <- findFreqTerms(dtm.bi,  5,5)     freq.tri <- findFreqTerms(dtm.tri, 5, 5)
Uni_gram <- NGramTokenizer(corpus.data, Weka_control(min = 1, max = 1))     Unigram = data.frame(table(Uni_gram))     Unigram = Unigram[order(Unigram$Freq, decreasing = T),] Ug <- Unigram[1:40,]
Uni_gram <- NGramTokenizer(corpus.data, Weka_control(min = 1, max = 1))
delim <- " \\r\\n\\t.,;:\"()?!"     tokenizer.uni <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))     tokenizer.bi <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2), delimiters = delim)     tokenizer.tri <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3), delimiters = delim)     tdm.uni <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.uni))     tdm.bi <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.bi))     tdm.tri <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.tri))
tdm <- TermDocumentMatrix(corpus.data, control = list(tokenize = NGramTokenizer))
tdm <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.uni))
findFreqTerms(tdm, lowfreq = 2)
corpus.data <- tm_map(corpus.data, PlainTextDocument)
delim <- " \\r\\n\\t.,;:\"()?!"     tokenizer.uni <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))     tokenizer.bi <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2), delimiters = delim)     tokenizer.tri <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3), delimiters = delim)     tdm.uni <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.uni))     tdm.bi <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.bi))     tdm.tri <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.tri))     freq.uni <- findFreqTerms(dtm.uni, 5,5)     freq.bi <- findFreqTerms(dtm.bi,  5,5)     freq.tri <- findFreqTerms(dtm.tri, 5, 5)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
freq.uni <- findFreqTerms(tdm.uni, 5,5)     freq.bi <- findFreqTerms(tdm.bi,  5,5)     freq.tri <- findFreqTerms(tdm.tri, 5, 5)
Uni_gram <- NGramTokenizer(corpus.data, Weka_control(min = 1, max = 1))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
delim <- " \\r\\n\\t.,;:\"()?!"     tokenizer.uni <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))     tokenizer.bi <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2), delimiters = delim)     tokenizer.tri <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3), delimiters = delim)     tdm.uni <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.uni))     tdm.bi <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.bi))     tdm.tri <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.tri))     freq.uni <- findFreqTerms(tdm.uni, 5,5)     freq.bi <- findFreqTerms(tdm.bi,  5,5)     freq.tri <- findFreqTerms(tdm.tri, 5, 5)
Uni_gram <- NGramTokenizer(corpus.data, Weka_control(min = 1, max = 1)) Unigram = data.frame(table(Uni_gram)) Unigram = Unigram[order(Unigram$Freq, decreasing = T),] Ug <- Unigram[1:40,]
p_load("stringi")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
Uni_gram <-  NGramTokenizer(corpus.data, Weka_control(min = 1, max = 1))
Uni_gram <-  NGramTokenizer(corpus.data, Weka_control(min = 1, max = 1))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
p_load("tm")
## create a UnigramTokenizer (RWeka) UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1)) ## create a BigramTokenizer (RWeka) BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2)) ## load the english documents en_texts <- VCorpus(DirSource(directory = "data/en_US/small", encoding = "UTF-8"),                               readerControl = list(language = "en")) ## get rid of extra white spaces, stopwords, DON'T STEM YET, switch to lowercase en_texts <- tm_map(x = en_texts, FUN = removePunctuation) en_texts <- tm_map(x = en_texts, FUN = removeWords, words = stopwords(kind = "en")) en_texts <- tm_map(x = en_texts, FUN = stripWhitespace) en_texts <- tm_map(x = en_texts, FUN = tolower) ## create a TermDocumentMatrix   ## NOTE - without the "options" underneath, the TermDocumentMatrix call crashes -  ## (looks like a parallel processing issue) options(mc.cores = 1) tdmUnigram <- TermDocumentMatrix(en_texts, control = list(tokenizer = UnigramTokenizer)) tdmBigram <- TermDocumentMatrix(en_texts, control = list(tokenizer = BigramTokenizer))
## create a UnigramTokenizer (RWeka) UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1)) ## create a BigramTokenizer (RWeka) BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2)) ## get rid of extra white spaces, stopwords, DON'T STEM YET, switch to lowercase corpus.data <- tm_map(x = corpus.data, FUN = removePunctuation) corpus.data <- tm_map(x = corpus.data, FUN = removeWords, words = stopwords(kind = "en")) corpus.data <- tm_map(x = corpus.data, FUN = stripWhitespace) corpus.data <- tm_map(x = corpus.data, FUN = tolower) ## create a TermDocumentMatrix   ## NOTE - without the "options" underneath, the TermDocumentMatrix call crashes -  ## (looks like a parallel processing issue) options(mc.cores = 1) tdmUnigram <- TermDocumentMatrix(corpus.data, control = list(tokenizer = UnigramTokenizer)) tdmBigram <- TermDocumentMatrix(corpus.data, control = list(tokenizer = BigramTokenizer))
lst <- list(sample.blogs, sample.news, sample.twitter)     corpus.data <- Corpus(VectorSource(lst))     toEmpty <- content_transformer(function(x, pattern) gsub(pattern, "", x, fixed = TRUE))     toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x, fixed = TRUE))     corpus.data <- tm_map(corpus.data, tolower)     corpus.data <- tm_map(corpus.data, removePunctuation)     corpus.data <- tm_map(corpus.data, removeNumbers)     corpus.data <- tm_map(corpus.data, removeWords, stopwords("english"))     corpus.data <- tm_map(corpus.data, stripWhitespace)     corpus.data <- tm_map(corpus.data, PlainTextDocument)     corpus.data <- tm_map(corpus.data, toEmpty, "#\\w+")       corpus.data <- tm_map(corpus.data, toEmpty, "(\\b\\S+\\@\\S+\\..{1,3}(\\s)?\\b)")       corpus.data <- tm_map(corpus.data, toEmpty, "@\\w+")       corpus.data <- tm_map(corpus.data, toEmpty, "http[^[:space:]]*")       corpus.data <- tm_map(corpus.data, toSpace, "/|@|\\|")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
p_load("RWeka")
install.packages("RWeka")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
options(mc.cores = 1) tdmUnigram <- TermDocumentMatrix(corpus.data, control = list(tokenizer = UnigramTokenizer)) tdmBigram <- TermDocumentMatrix(corpus.data, control = list(tokenizer = BigramTokenizer))
Uni_gram <- NGramTokenizer(Cleandata, Weka_control(min = 1, max = 1)) Unigram = data.frame(table(Uni_gram)) Unigram = Unigram[order(Unigram$Freq, decreasing = T),] Ug <- Unigram[1:40,]
Uni_gram <- NGramTokenizer(Cleandata, Weka_control(min = 1, max = 1))
?RWeka
delim <- " \\r\\n\\t.,;:\"()?!"     tokenizer.uni <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))     tokenizer.bi <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2), delimiters = delim)     tokenizer.tri <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3), delimiters = delim)     tdm.uni <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.uni))     tdm.bi <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.bi))     tdm.tri <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.tri))     freq.uni <- findFreqTerms(tdm.uni, 5,5)     freq.bi <- findFreqTerms(tdm.bi,  5,5)     freq.tri <- findFreqTerms(tdm.tri, 5, 5)
barplot(tdm.uni, names.arg = freq.uni, cex.names = .7, col = heat.colors(40), main = c("Frequency of one word"), las = 2)
Uni_gram <- tokenizer.uni(corpus.data)
library(rJava)
p_load("rJava")
.jinit(parameters = "-Xmx128g")
options(mc.cores = 1)
Uni_gram <- tokenizer.uni(corpus.data)
delim <- " \\r\\n\\t.,;:\"()?!"     gram1Tokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 1, max = 1)) }     gram2Tokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2)) }     gram3Tokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3)) }     tdm1 <- TermDocumentMatrix(corpus, control = list(tokenize = gram1Tokenizer))     tdm2 <- TermDocumentMatrix(corpus, control = list(tokenize = gram2Tokenizer))     tdm3 <- TermDocumentMatrix(corpus, control = list(tokenize = gram3Tokenizer))     gram1freq <- data.frame(word = tdm1$dimnames$Terms, freq = rowSums(sparseMatrix(i = tdm1$i, j = tdm1$j, x = tdm1$v)))     gram1freq <- arrange(gram1freq, desc(freq))     gram2freq <- data.frame(word = tdm2$dimnames$Terms, freq = rowSums(sparseMatrix(i = tdm2$i, j = tdm2$j, x = tdm2$v)))     gram2freq <- arrange(gram2freq, desc(freq))     gram3freq <- data.frame(word = tdm3$dimnames$Terms, freq = rowSums(sparseMatrix(i = tdm3$i, j = tdm3$j, x = tdm3$v)))     gram3freq <- arrange(gram3freq, desc(freq))
delim <- " \\r\\n\\t.,;:\"()?!"     gram1Tokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 1, max = 1)) }     gram2Tokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2)) }     gram3Tokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3)) }     tdm1 <- TermDocumentMatrix(corpus.data, control = list(tokenize = gram1Tokenizer))     tdm2 <- TermDocumentMatrix(corpus.data, control = list(tokenize = gram2Tokenizer))     stdm3 <- TermDocumentMatrix(corpus.data, control = list(tokenize = gram3Tokenizer))     gram1freq <- data.frame(word = tdm1$dimnames$Terms, freq = rowSums(sparseMatrix(i = tdm1$i, j = tdm1$j, x = tdm1$v)))     gram1freq <- arrange(gram1freq, desc(freq))     gram2freq <- data.frame(word = tdm2$dimnames$Terms, freq = rowSums(sparseMatrix(i = tdm2$i, j = tdm2$j, x = tdm2$v)))     gram2freq <- arrange(gram2freq, desc(freq))     gram3freq <- data.frame(word = tdm3$dimnames$Terms, freq = rowSums(sparseMatrix(i = tdm3$i, j = tdm3$j, x = tdm3$v)))     gram3freq <- arrange(gram3freq, desc(freq))
p_load("Matrix")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
install.packages("rJava", dependencies = TRUE)
install.packages("RWeka", dependencies = TRUE)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
data.frame(gram1 = gram1freq$word[1:10], gram2 = gram2freq$word[1:10], gram3 = gram3freq$word[1:10])
data.graph <-    data.frame(gram1 = gram1freq$word[1:10], gram2 = gram2freq$word[1:10], gram3 = gram3freq$word[1:10])
par(mfrow=c(1,1))     wordcloud(data.graph$gram1, scale = c(9, 1), max.words = 40, random.order = FALSE, colors = brewer.pal(7, "Dark2"))
wordcloud(data.graph$gram2, scale = c(9, 1), max.words = 40, random.order = FALSE, colors = brewer.pal(7, "Dark2"))
wordcloud(data.graph$gram3, scale = c(9, 1), max.words = 40, random.order = FALSE, colors = brewer.pal(7, "Dark2"))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
g1 <- ggplot(data.graph[0], aes(x = reorder(data.graph[0]$word, data.graph[0]$freq), y = data.graph[0]$freq)) +     geom_bar(stat = "identity", fill = "red") +     ggtitle("1-gram") +     xlab("1-grams") + ylab("Frequency")
g1
g1 <- ggplot(data.graph[0], aes(x = word, y = freq)) +     geom_bar(stat = "identity", fill = "red") +     ggtitle("1-gram") +     xlab("1-grams") + ylab("Frequency") g1
data.graph[0] <- data.frame(word = unlist(word))
data.graph[0]
data.graph$gram1
gram1freq <= sqldf("select * from gram1freq where freq > 4000")
gram1freq <- sqldf("select * from gram1freq where freq > 4000")
g1 <- ggplot(gram1freq, aes(x = word, y = freq)) +     geom_bar(stat = "identity", fill = "red") +     ggtitle("1-gram") +     xlab("1-grams") + ylab("Frequency")
g1
gram2freq <- sqldf("select * from gram2freq where freq > 4000 order by freq desc")
g2 <- ggplot(gram2freq, aes(x = word, y = freq)) +     geom_bar(stat = "identity", fill = "green") +     ggtitle("2-gram") +     xlab("2-grams") + ylab("Frequency") g1
g2 <- ggplot(gram2freq, aes(x = word, y = freq)) +     geom_bar(stat = "identity", fill = "green") +     ggtitle("2-gram") +     xlab("2-grams") + ylab("Frequency") g2
g1 <- ggplot(gram1freq, aes(x = word, y = freq)) +     geom_bar(stat = "identity", fill = "red") +     ggtitle("1-gram") +     xlab("1-grams") + ylab("Frequency") g1
gram3freq <- sqldf("select * from gram3freq where freq > 4000 order by freq desc")
g3 <- ggplot(gram3freq, aes(x = word, y = freq)) +     geom_bar(stat = "identity", fill = "blue") +     ggtitle("3-gram") +     xlab("3-grams") + ylab("Frequency") g3
gram3freq <- data.frame(word = tdm3$dimnames$Terms, freq = rowSums(sparseMatrix(i = tdm3$i, j = tdm3$j, x = tdm3$v)))     gram3freq <- sqldf("select * from gram3freq where freq > 4000 order by freq desc")
g3 <- ggplot(gram3freq, aes(x = word, y = freq)) +     geom_bar(stat = "identity", fill = "blue") +     ggtitle("3-gram") +     xlab("3-grams") + ylab("Frequency") g3
wordcloud(names(gram1freq), gram1freq, min.freq = 25, max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
wordcloud(gram1freq@names, gram1freq, min.freq = 25, max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
wordcloud(gram1freq@word, gram1freq@freq, min.freq = 25, max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
wordcloud(gram1freq@word, gram1freq@freq,  max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
wordcloud(gram1freq@word, max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
wordcloud(gram1freq@word, gram1freq@freq,  max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
wordcloud(gram1freq$word, gram1freq$freq,  max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
wordcloud(gram2freq$word, gram2freq$freq, max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "yellow"))
wordcloud(gram1freq$word, gram1freq$freq,  max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "red"))
wordcloud(gram1freq$word, gram1freq$freq,  max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Accent"))
wordcloud(gram1freq$word, gram1freq$freq,  max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Set1"))
wordcloud(gram2freq$word, gram2freq$freq, max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Set2"))
wordcloud(gram3freq$word, gram3freq$freq, max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Seet3"))
wordcloud(gram3freq$word, gram3freq$freq, max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Set3"))
suppressMessages(setwd("c:/dev/r-course/10-capstone/"))
covert_rmd_to_r <- function() {     library(knitr)     purl("milestone-report.Rmd") }
covert_rmd_to_r()
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
f1 <- findFreqTerms(tdm3)
f1 <- findFreqTerms(tdm3, 5)
freq.train <- dtm.train[, f1] freq.test <- dtm.test[, f1]
t1 <- (tdm3$nrow * .7)
dtm.train <- tdm3[1:t1,0]
count.train <- (tdm3$nrow * .7) # 70 % count.test <- tdm3$nrow-count.train
count.train <- (tdm3$nrow * .7) # 70 % count.test <- tdm3$nrow-count.train dtm.train <- tdm3[1:count.train, ] dtm.test <- tdm3[1:count.test, ]
freq.train <- dtm.train[, f1] freq.test <- dtm.test[, f1]
f1 <- findFreqTerms(tdm3, 5) count.train <- (tdm3$nrow * .7) # 70 % dtm.train <- tdm3[1:count.train, ] dtm.test <- tdm3[count.train:tdm3$nrow,]
f1 <- findFreqTerms(tdm3, 5) count.train <- (tdm3$nrow * .7) # 70 % dtm.train <- tdm3[1:count.train, ] dtm.test <- tdm3[count.train:tdm3$nrow,] dtm.train.labels <- tdm3[1:count.train,] dtm.test.labels <- tdm3[count.train:tdm3$nrow,] freq.train <- dtm.train[, f1] freq.test <- dtm.test[, f1]
dtm.test.labels <- tdm3[count.train:tdm3$nrow,]
dtm.train.labels <- tdm3[1:count.train,]
using("SparseM")
dtm_matrix = as.matrix.csr(as.matrix(dtm))
dtm_matrix = as.matrix.csr(as.matrix(tdm3))
svm_model <- svm(dtm_matrix, classvec, kernel = "linear")
using("enc2utf8")
dtm_matrix = as.matrix.csr(as.matrix(tdm3))
dtm.train.labels <- tdm3[1:count.train,]$dimnames$Terms
 # convert dtm to dtm_matrix using sparse storage dtm_matrix = as.matrix.csr(as.matrix(tdm3)) #specify the features, vector to be predicted, and kernel method in the svm model svm_model <- svm(dtm_matrix, dtm.train.labels, kernel = "linear") summary(svm_model) #inspect results pred <- predict(svm_model, dtm) table(pred, classvec)
# convert dtm to dtm_matrix using sparse storage dtm_matrix = as.matrix(dtm.train) #specify the features, vector to be predicted, and kernel method in the svm model svm_model <- svm(dtm_matrix, dtm.train.labels, kernel = "linear") summary(svm_model) #inspect results pred <- predict(svm_model, dtm) table(pred, classvec)
dtm_matrix = as.matrix(dtm.train)
svm_model <- svm(dtm_matrix, dtm.train.labels, kernel = "linear")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
update.packages("tm", checkBuilt = TRUE)
install.packages("filehash", lib="C:/Users/chris/Documents/R/win-library/3.4")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
corpus.data <- PCorpus(VectorSource(group_texts), dbControl = list(dbName = "pcorpus.db", dbType = "DB1")))
corpus.data <- PCorpus(VectorSource(group_texts), dbControl = list(dbName = "pcorpus.db", dbType = "DB1"))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
using("caret")
using("caret") # Convert to a data.frame for training and assign a classification (factor) to each document. train <- as.matrix(tdm3) train <- cbind(train, c(0, 1)) colnames(train)[ncol(train)] <- 'y' train <- as.data.frame(train) train$y <- as.factor(train$y) # Train. fit <- train(y ~ ., data = train, method = 'bayesglm') # Check accuracy on training. predict(fit, newdata = train) # Test data. data2 <- c('Bats eat bugs.') corpus <- VCorpus(VectorSource(data2)) tdm <- DocumentTermMatrix(corpus, control = list(dictionary = Terms(tdm), removePunctuation = TRUE, stopwords = TRUE, stemming = TRUE, removeNumbers = TRUE)) test <- as.matrix(tdm) # Check accuracy on test. predict(fit, newdata = test)
 # Convert to a data.frame for training and assign a classification (factor) to each document. train <- as.matrix(tdm3) train <- cbind(train, c(0, 1)) colnames(train)[ncol(train)] <- 'y' train <- as.data.frame(train) train$y <- as.factor(train$y)
fit <- train(y ~ ., data = train, method = 'bayesglm')
predict(fit, newdata = train)
train <- as.matrix(dtm.train) train <- cbind(train, c(0, 1)) colnames(train)[ncol(train)] <- 'y' train <- as.data.frame(train) train$y <- as.factor(train$y)
using("ngram")
install.packages("ngram")
create_ngrams <- function(sentence_splits, ngram_size = 2) {     ngrams <- c()     for (sentence in sentence_splits) {         sentence <- Trim(sentence)         if ((nchar(sentence) > 0) && (sapply(gregexpr("\\W+", sentence), length) >= ngram_size)) {             ngs <- ngram(sentence, n = ngram_size)             ngrams <- c(ngrams, get.ngrams(ngs))         }     }     return(ngrams) }
n2 <- create_ngrams(corpus.data, ngram_size = 2) n3 <- create_ngrams(corpus.data, ngram_size = 3) n4 <- create_ngrams(corpus.data, ngram_size = 4) n5 <- create_ngrams(corpus.data, ngram_size = 5)
create_ngrams <- function(sentence_splits, ngram_size = 2) {     ngrams <- c()     for (sentence in sentence_splits) {         sentence <- trim(sentence)         if ((nchar(sentence) > 0) && (sapply(gregexpr("\\W+", sentence), length) >= ngram_size)) {             ngs <- ngram(sentence, n = ngram_size)             ngrams <- c(ngrams, get.ngrams(ngs))         }     }     return(ngrams) } n2 <- create_ngrams(corpus.data, ngram_size = 2) n3 <- create_ngrams(corpus.data, ngram_size = 3) n4 <- create_ngrams(corpus.data, ngram_size = 4) n5 <- create_ngrams(corpus.data, ngram_size = 5)
trim <- function(x) {     # http://stackoverflow.com/questions/2261079/how-to-trim-leading-and-trailing-whitespace-in-r     gsub("(^[[:space:]]+|[[:space:]]+$)", "", x) }
n2 <- create_ngrams(corpus.data, ngram_size = 2) n3 <- create_ngrams(corpus.data, ngram_size = 3) n4 <- create_ngrams(corpus.data, ngram_size = 4) n5 <- create_ngrams(corpus.data, ngram_size = 5)
create_ngrams <- function(sentence_splits, ngram_size = 2) {     ngrams <- c()     for (sentence in sentence_splits) {         sentence <- trim(sentence)         if ((nchar(sentence) > 0) && (sapply(gregexpr("\\W+", sentence), length) >= ngram_size)) {             ngs <- ngram(sentence, n = ngram_size)             ngrams <- c(ngrams, get.ngrams(ngs))         }     }     return(ngrams) } n2 <- create_ngrams(corpus.data, ngram_size = 2) n3 <- create_ngrams(corpus.data, ngram_size = 3) n4 <- create_ngrams(corpus.data, ngram_size = 4) n5 <- create_ngrams(corpus.data, ngram_size = 5)
using("foreach")
create_ngrams <- function(sentence_splits, ngram_size = 2) {     ngrams <- c()     for (sentence in sentence_splits) {         sentence <- trim(sentence)         if ((nchar(sentence) > 0) && (sapply(gregexpr("\\W+", sentence), length) >= ngram_size)) {             ngs <- ngram(sentence, n = ngram_size)             ngrams <- c(ngrams, get.ngrams(ngs))         }     }     return(ngrams) } n2 <- create_ngrams(corpus.data, ngram_size = 2) n3 <- create_ngrams(corpus.data, ngram_size = 3) n4 <- create_ngrams(corpus.data, ngram_size = 4) n5 <- create_ngrams(corpus.data, ngram_size = 5)
n2 <- create_ngrams(sample.all, ngram_size = 2) n3 <- create_ngrams(sample.all, ngram_size = 3) n4 <- create_ngrams(sample.all, ngram_size = 4) n5 <- create_ngrams(sample.all, ngram_size = 5)
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
using("ANLP")
install.packages("ANLP")
install.packages("ANLP")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
n2 <- buildNgramModel(2)
n2()
n2(data.blogs)
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
n2 <- buildNgramModel(2) tdm2 <- n2(data.all)
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
tdm <- generateTDM(data.all,3, N, isTrace = F)
tdm <- generateTDM(data.all,3,T)
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
tdm2 <- n2(sample.all)
class(tdm2)
n3 <- buildNgramModel(3)
tdm2 <- n3(sample.all)
tdm2 <- n2sample.all) tdm3 <- n3(sample.all)
tdm2 <- n2(sample.all) tdm3 <- n3(sample.all)
tdm <- generateTDM(sample.all)
tdm2 <- generateTDM(sample.all, 2, T)
test <- "If this isn't the cutest thing you've ever seen, then you must be"
lst <- c(tdm2,tdm3)
tdm2 <- generateTDM(sample.all, 2, T) tdm3 <- generateTDM(sample.all, 3, T) lst <- c(tdm2,tdm3) test <- "If this isn't the cutest thing you've ever seen, then you must be" predict_Backoff(test,lst,T)
lst <- as.vector(c(tdm2,tdm3))
predict_Backoff(test,lst,T)
test <- cleanTextData( "If this isn't the cutest thing you've ever seen, then you must be") predict_Backoff(test,lst,T)
lst <- as.list(c(tdm2,tdm3))
test <- cleanTextData( "If this isn't the cutest thing you've ever seen, then you must be") predict_Backoff(test,lst,T)
predict_Backoff(test,tdm3,T)
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
predict_Backoff(test,lst,T)
test <- cleanTextData( "If this isn't the cutest thing you've ever seen, then you must be") predict_Backoff(test,lst,T)
detach(package:neuralnet)
sessionInfo()
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
require(ngram)
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
# notice the trailing space at end to avoid picking last word word <- 'bacon ' matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(paste0('\\<', word), sentence)) {         print(sentence)         matches <- c(matches, sentence)     } } # find highest probability word precision_match <- c() for (a_match in matches) {     # how many spaces in from of search word     precision_match <- c(precision_match, nchar(strsplit(x = a_match, split = word)[[1]][[1]])) } # use highest number and a random of highest for multiples best_matched_sentence <- sample(matches[precision_match == max(precision_match)], size = 1) print(best_matched_sentence)
best_matched_sentence <- sample(matches[precision_match == max(precision_match)], size = 1)
# notice the trailing space at end to avoid picking last word word <- "If this isn' t the cutest thing you 've ever seen, then you must be " matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(paste0('\\<', word), sentence)) {         print(sentence)         matches <- c(matches, sentence)     } } # find highest probability word precision_match <- c() for (a_match in matches) {     # how many spaces in from of search word     precision_match <- c(precision_match, nchar(strsplit(x = a_match, split = word)[[1]][[1]])) } # use highest number and a random of highest for multiples best_matched_sentence <- sample(matches[precision_match == max(precision_match)], size = 1) print(best_matched_sentence)
# notice the trailing space at end to avoid picking last word word <- "then you must be " matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(paste0('\\<', word), sentence)) {         print(sentence)         matches <- c(matches, sentence)     } } # find highest probability word precision_match <- c() for (a_match in matches) {     # how many spaces in from of search word     precision_match <- c(precision_match, nchar(strsplit(x = a_match, split = word)[[1]][[1]])) } # use highest number and a random of highest for multiples best_matched_sentence <- sample(matches[precision_match == max(precision_match)], size = 1) print(best_matched_sentence)
matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(paste0('\\<', word), sentence)) {         print(sentence)         matches <- c(matches, sentence)     } }
word <- "must be " matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(paste0('\\<', word), sentence)) {         print(sentence)         matches <- c(matches, sentence)     } }
word <- "must be " matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(paste0('\\<', word), sentence)) {         print(sentence)         matches <- c(matches, sentence)     } }
# notice the trailing space at end to avoid picking last word word <- "must be " matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(word, sentence)) {         print(sentence)         matches <- c(matches, sentence)     } }
word <- "must be" matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(word, sentence)) {         print(sentence)         matches <- c(matches, sentence)     } }
word <- "dust them off and be on my"
matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(word, sentence)) {         print(sentence)         matches <- c(matches, sentence)     } }
# find highest probability word precision_match <- c() for (a_match in matches) {     # how many spaces in from of search word     precision_match <- c(precision_match, nchar(strsplit(x = a_match, split = word)[[1]][[1]])) } # use highest number and a random of highest for multiples best_matched_sentence <- sample(matches[precision_match == max(precision_match)], size = 1)
word <- "be on my " matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(word, sentence)) {         print(sentence)         matches <- c(matches, sentence)     } } # find highest probability word precision_match <- c() for (a_match in matches) {     # how many spaces in from of search word     precision_match <- c(precision_match, nchar(strsplit(x = a_match, split = word)[[1]][[1]])) } # use highest number and a random of highest for multiples best_matched_sentence <- sample(matches[precision_match == max(precision_match)], size = 1)
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
library(stringr)
 predictNgrams <- function(input) {     ## clean input text     # remove numbers, punctuations     word <- gsub("[^a-zA-Z\n\']", " ", input)     # convert all words to lowercase     word <- tolower(word)     # remove extra spaces     trim <- function(x) return(gsub("^ *|(?<= ) | *$", "", x, perl = T))     word <- trim(word)     str <- unlist(str_split(word, " "))     len <- length(str)     predict <- c()     if (len >= 3) {         ##trigram          W1 <- str[len - 2];         W2 <- str[len - 1];         W3 <- str[len]         ngram <- n3[list(W1, W2, W3)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##bigram             ngram <- n3[list(W2, W3)]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }         if (length(predict) < 6) {             ##unigram             ngram <- n2[list(W3)]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 2) {         W1 <- str[len - 1];         W2 <- str[len]         ngram <- n3[list(W1, W2)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##unigram             ngram <- n2[list(W2)]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 1) {         W1 <- str[len]         ngram <- n2[list(W1)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)     }     predict <- predict[!is.na(predict)]     if (length(predict) < 5) {         predict <- c(predict, Unigrams_model$pred)     }     return(predict[1:5]) }
predictNgrams(word)
  ngram <- n3[[list(W2, W3)]]
predictNgrams <- function(input) {     ## clean input text     # remove numbers, punctuations     word <- gsub("[^a-zA-Z\n\']", " ", input)     # convert all words to lowercase     word <- tolower(word)     # remove extra spaces     trim <- function(x) return(gsub("^ *|(?<= ) | *$", "", x, perl = T))     word <- trim(word)     str <- unlist(str_split(word, " "))     len <- length(str)     predict <- c()     if (len >= 3) {         ##trigram          W1 <- str[len - 2];         W2 <- str[len - 1];         W3 <- str[len]         ngram <- n3[list(W1, W2, W3)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##bigram             ngram <- n3[[list(W2, W3)]]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }         if (length(predict) < 6) {             ##unigram             ngram <- n2[[list(W2, W3)]]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 2) {         W1 <- str[len - 1];         W2 <- str[len]         ngram <- n3[list(W1, W2)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##unigram             ngram <- n2[list(W2)]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 1) {         W1 <- str[len]         ngram <- n2[list(W1)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)     }     predict <- predict[!is.na(predict)]     if (length(predict) < 5) {         predict <- c(predict, Unigrams_model$pred)     }     return(predict[1:5]) }
predictNgrams(word)
n2()$predict
n2[]$predict
n2$predict
predictNgrams(word)
predictNgrams <- function(input) {     ## clean input text     # remove numbers, punctuations     word <- gsub("[^a-zA-Z\n\']", " ", input)     # convert all words to lowercase     word <- tolower(word)     # remove extra spaces     trim <- function(x) return(gsub("^ *|(?<= ) | *$", "", x, perl = T))     word <- trim(word)     str <- unlist(str_split(word, " "))     len <- length(str)     predict <- c()     if (len >= 3) {         ##trigram          W1 <- str[len - 2];         W2 <- str[len - 1];         W3 <- str[len]         ngram <- n3[W1, W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##bigram             ngram <- n3[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }         if (length(predict) < 6) {             ##unigram             ngram <- n2[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 2) {         W1 <- str[len - 1];         W2 <- str[len]         ngram <- n2[W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##unigram             ngram <- n2[list(W2)]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 1) {         W1 <- str[len]         ngram <- n2[list(W1)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)     }     predict <- predict[!is.na(predict)]     if (length(predict) < 5) {         predict <- c(predict, Unigrams_model$pred)     }     return(predict[1:5]) }
predictNgrams(word)
 predictNgrams <- function(input) {     ## clean input text     # remove numbers, punctuations     word <- gsub("[^a-zA-Z\n\']", " ", input)     # convert all words to lowercase     word <- tolower(word)     # remove extra spaces     trim <- function(x) return(gsub("^ *|(?<= ) | *$", "", x, perl = T))     word <- trim(word)     str <- unlist(str_split(word, " "))     len <- length(str)     predict <- c()     if (len > 3) {         ##trigram          W1 <- str[len - 2];         W2 <- str[len - 1];         W3 <- str[len]         ngram <- n3[W1, W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##bigram             ngram <- n3[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }         if (length(predict) < 6) {             ##unigram             ngram <- n2[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 2) {         W1 <- str[len - 1];         W2 <- str[len]         ngram <- n2[W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##unigram             ngram <- n2[list(W2)]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 1) {         W1 <- str[len]         ngram <- n2[list(W1)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)     }     predict <- predict[!is.na(predict)]     if (length(predict) < 5) {         predict <- c(predict, Unigrams_model$pred)     }     return(predict[1:5]) } predictNgrams(word)
n1 <- buildNgramModel(1) 
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
doit()
n1()
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
predictNgrams <- function(input) {     ## clean input text     # remove numbers, punctuations     word <- gsub("[^a-zA-Z\n\']", " ", input)     # convert all words to lowercase     word <- tolower(word)     # remove extra spaces     trim <- function(x) return(gsub("^ *|(?<= ) | *$", "", x, perl = T))     word <- trim(word)     str <- unlist(str_split(word, " "))     len <- length(str)     predict <- c()     if (len > 3) {         ##trigram          W1 <- str[len - 2];         W2 <- str[len - 1];         W3 <- str[len]         ngram <- n3[W1, W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##bigram             ngram <- n3[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }         if (length(predict) < 6) {             ##unigram             ngram <- n1[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 2) {         W1 <- str[len - 1];         W2 <- str[len]         ngram <- n2[W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##unigram             ngram <- n1[list(W2)]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 1) {         W1 <- str[len]         ngram <- n2[list(W1)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)     }     predict <- predict[!is.na(predict)]     if (length(predict) < 5) {         predict <- c(predict, n1$pred)     }     return(predict[1:5]) } predictNgrams(word)
  predict <- predict[!is.na(predict)]
    if (length(predict) < 5) {         predict <- c(predict, n1$pred)     }
predictNgrams <- function(input) {     ## clean input text     # remove numbers, punctuations     word <- gsub("[^a-zA-Z\n\']", " ", input)     # convert all words to lowercase     word <- tolower(word)     # remove extra spaces     trim <- function(x) return(gsub("^ *|(?<= ) | *$", "", x, perl = T))     word <- trim(word)     str <- unlist(str_split(word, " "))     len <- length(str)     predict <- c()     if (len > 3) {         ##trigram          W1 <- str[len - 2];         W2 <- str[len - 1];         W3 <- str[len]         ngram <- n3[W1, W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##bigram             ngram <- n3[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }         if (length(predict) < 6) {             ##unigram             ngram <- n1[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 2) {         W1 <- str[len - 1];         W2 <- str[len]         ngram <- n2[W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##unigram             ngram <- n1[list(W2)]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 1) {         W1 <- str[len]         ngram <- n2[list(W1)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)     }   #  predict <- predict[!is.na(predict)]     if (length(predict) < 5) {         predict <- c(predict, n1$pred)     }     return(predict[1:5]) } predictNgrams(word)
predictNgrams <- function(input) {     ## clean input text     # remove numbers, punctuations     word <- gsub("[^a-zA-Z\n\']", " ", input)     # convert all words to lowercase     word <- tolower(word)     # remove extra spaces     trim <- function(x) return(gsub("^ *|(?<= ) | *$", "", x, perl = T))     word <- trim(word)     str <- unlist(str_split(word, " "))     len <- length(str)     predict <- c()     if (len > 3) {         ##trigram          W1 <- str[len - 2];         W2 <- str[len - 1];         W3 <- str[len]         ngram <- n3[W1, W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##bigram             ngram <- n3[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }         if (length(predict) < 6) {             ##unigram             ngram <- n1[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 2) {         W1 <- str[len - 1];         W2 <- str[len]         ngram <- n2[W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##unigram             ngram <- n1[list(W2)]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 1) {         W1 <- str[len]         ngram <- n2[list(W1)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)     }   #  predict <- predict[!is.na(predict)]     if (length(predict) < 5) {         predict <- c(predict, n1)     }     return(predict[1:5]) } predictNgrams(word)
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
class(n2)
tdm2 <- generateTDM(sample.all, 2, T) tdm3 <- generateTDM(sample.all, 3, T)
tdm2 <- generateTDM(n_all, 2, T) tdm3 <- generateTDM(n_all, 3, T)
tdm2 <- generateTDM(sample.all, 2, T)
tokenize.ngram <- function(x, min.gram, max.gram) {     NGramTokenizer(x, Weka_control(min = min.gram, max = min.gram)) } tdm1 <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenize.ngram(1))) tdm2 <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenize.ngram(2))) tdm3 <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenize.ngram(3)))
source("C:/dev/r-course/10-capstone/project_1__setup.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
group_texts <- list(sample.blogs, sample.news, sample.twitter) group_texts <- tolower(group_texts) group_texts <- removeNumbers(group_texts) group_texts <- removePunctuation(group_texts, preserve_intra_word_dashes = TRUE) group_texts <- gsub("http[[:alnum:]]*", "", group_texts) group_texts <- stripWhitespace(group_texts) group_texts <- gsub("\u0092", "'", group_texts) group_texts <- gsub("\u0093|\u0094", "", group_texts)
group_texts <- tm_map(group_texts, qdap::clean) group_texts <- tm_map(group_texts, qdap::scrubber) group_texts <- tm_map(group_texts, qdap::replace_symbol)
group_texts <- sent_detect_nlp(group_texts)
corpus.data <- PCorpus(VectorSource(group_texts), dbControl = list(dbName = "pcorpus.db", dbType = "DB1")) toEmpty <- content_transformer(function(x, pattern) gsub(pattern, "", x, fixed = TRUE)) toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x, fixed = TRUE)) corpus.data <- tm_map(corpus.data, toEmpty, "#\\w+") corpus.data <- tm_map(corpus.data, toEmpty, "(\\b\\S+\\@\\S+\\..{1,3}(\\s)?\\b)") corpus.data <- tm_map(corpus.data, toEmpty, "@\\w+") corpus.data <- tm_map(corpus.data, toEmpty, "http[^[:space:]]*") corpus.data <- tm_map(corpus.data, toSpace, "/|@|\\|")
save_file("https://goo.gl/To9w5B", "bad_word_list.txt") bad_words <- readLines("./bad_word_list.txt") corpus.data <- tm_map(corpus.data, removeWords, bad_words) corpus.data <- tm_map(corpus.data, stemDocument)
source('c:/dev/r-course/10-capstone/project_4__ngram_generation.r')
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_cleanup.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_cleanup.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_cleanup.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_cleanup.r", echo = TRUE, encoding = "Windows-1252")
library(tm)
using("filehashSQLite")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
   db <- dbInit("pcorpus")
 db <- dbInit(database_name)
group_texts <- removeNumbers(group_texts)     group_texts <- removePunctuation(group_texts, preserve_intra_word_dashes = TRUE)     group_texts <- gsub("http[[:alnum:]]*", "", group_texts)     group_texts <- stripWhitespace(group_texts)     group_texts <- gsub("\u0092", "'", group_texts)     group_texts <- gsub("\u0093|\u0094", "", group_texts)     corpus.data <- PCorpus(VectorSource(group_texts), dbControl = list(dbName = database_name, dbType = "SQLite"))     toEmpty <- content_transformer(function(x, pattern) gsub(pattern, "", x, fixed = TRUE))     toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x, fixed = TRUE))     corpus.data <- tm_map(corpus.data, toEmpty, "#\\w+")     corpus.data <- tm_map(corpus.data, toEmpty, "(\\b\\S+\\@\\S+\\..{1,3}(\\s)?\\b)")     corpus.data <- tm_map(corpus.data, toEmpty, "@\\w+")     corpus.data <- tm_map(corpus.data, toEmpty, "http[^[:space:]]*")     corpus.data <- tm_map(corpus.data, toSpace, "/|@|\\|")     save_file("https://goo.gl/To9w5B", "bad_word_list.txt")     bad_words <- readLines("./bad_word_list.txt")     corpus.data <- tm_map(corpus.data, removeWords, bad_words)     corpus.data <- tm_map(corpus.data, stemDocument)     writeCorpus(corpus.data,path = ".", filenames = database_name)
 group_texts <- list(sample.blogs, sample.news, sample.twitter)     group_texts <- sent_detect_nlp(group_texts)
    dbCreate(database_name, "SQLite")     db <- dbInit(database_name, "SQLite")
database_name <- "pcorpus.db"
    dbCreate(database_name, "SQLite")     db <- dbInit(database_name, "SQLite")
    corpus.data <- PCorpus(VectorSource(group_texts), dbControl = list(dbName = database_name, dbType = "SQLite"))
    corpus.data <- PCorpus(VectorSource(group_texts), dbControl = list(dbName = database_name, dbType = "SQLite"))
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
 corpus.data <- PCorpus(VectorSource(group_texts), dbControl = list(dbName = database_name, dbType = "DB1"))
db <- dbInit(database_name)     corpus.data <- dbLoad(db)
    db <- dbInit(database_name)     corpus.data <- dbLoad(db)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
 db <- dbInit(db = database_name)
  db <- dbInit(db = database_file_path)
database_name <- "sqldb_pcorpus_mydata"
db <- dbInit(db = database_file_path)
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
    corpus.data <- PCorpus(VectorSource(group_texts), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))     dbCreate(database_name, "SQLite")     db <- dbInit(database_name, "SQLite")
  db <- dbInit(database_name, "SQLite")
 db.ListTables()
corpus.data.ListTables()
class(db)
dbGetDBIVersion()
db.dbGetDBIVersion()
db.GetDBIVersion()
dbListTables(db)
show(db)
dbFetch(db, "900")
 corpus.data <- dbInit(database_name, "SQLite")
    corpus.data <- dbInit(db = database_name)     corpus.data <- dbLoad(db)     corpus.data <- dbInit(database_name, "SQLite")     class(corpus.data)
>>>>>>> 5db9279125fa37c48a4b1df208d6738e100916ff
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
 cat("database--> creation started...")     group_texts <- list(sample.blogs, sample.news, sample.twitter)     group_texts <- sent_detect_nlp(group_texts)     group_texts <- tolower(group_texts)     group_texts <- removeNumbers(group_texts)     group_texts <- removePunctuation(group_texts, preserve_intra_word_dashes = TRUE)     group_texts <- gsub("http[[:alnum:]]*", "", group_texts)     group_texts <- stripWhitespace(group_texts)     group_texts <- gsub("\u0092", "'", group_texts)     group_texts <- gsub("\u0093|\u0094", "", group_texts)
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
group_texts <- data.all
    group_texts <- sent_detect_nlp(group_texts)
object.size(data.all)
source("C:/dev/r-course/10-capstone/project_2__data_retrieval.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_2__data_retrieval.r", echo = TRUE, encoding = "Windows-1252")
is.na(data.all)
length(data.all) == 0
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
dbDisconnect()
dbDisconnect(db)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
file.remove("C:\dev\r-course\10-capstone\sqldb_pcorpus_mydata")
file.remove("sqldb_pcorpus_mydata")
suppressMessages(setwd("c:/dev/r-course/10-capstone/"))
file.remove("sqldb_pcorpus_mydata")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
db <- dbInit(database_name, "SQLite")
dbListTables(db)
dbInsert(db, "test", "hi there")
dbFetch(db, "900")
dbFetch(db)
dbFetch(db, "5")
dbFetch(db, "-1")
dbGetInfo(db)
  corpus.data <- dbLoad(db)
using("MicrosoftML")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
corpus.data <- PCorpus(DataframeSource(dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
corpus.data[[1]]
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
dbDisconnect(db)
db <- dbInit(database_name, "SQLite")
dbDisconnect(db)
corpus.data <- PCorpus(VectorSource(dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
db <- dbInit(database_name, "SQLite")
    corpus.data <- PCorpus(VectorSource(dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
corpus.data <- PCorpus(dbLoad(db), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
rm(`1037`, envir = as.environment(".GlobalEnv"))
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
corpus.data <- PCorpus(DataframeSource(data.all), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
corpus.data <- PCorpus(VectorSource(data.all), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
corpus.data <- PCorpus(DataframeSource(dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
db <- dbInit(database_name, "SQLite")
    corpus.data <- PCorpus(DataframeSource(dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
corpus.data <- PCorpus(VectorSource(dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
corpus.data <- PCorpus(VectorSource(dbLoad(db)), readerControl = list(language = "en"))
data.all <- as.data.frame(data.all)
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
db <- dbInit(database_name, "SQLite")
dbDisconnect(db)
dbDisconnect(db)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
db <- dbInit(database_name, "SQLite")
corpus.data <- PCorpus(DataframeSource(dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite",))
corpus.data <- dbLoad(db)
dbDisconnect(db)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
corpus.data <- PCorpus(VectorSource(data.all), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
 corpus.data <- PCorpus(VectorSource(data.all), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
dbDisconnect(db)
db <- dbInit(database_name, "SQLite")
dbDisconnect(db)
corpus.data <- PCorpus(VectorSource(data.all), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
warnings()
corpus.data2 <- dbLoad(db)
db <- dbInit(database_name, "SQLite")
corpus.data2 <- dbLoad(db)
 dbDisconnect(db)
identical(corpus.data, corpus.data2)
p_load("openNLP")
gc()
using("textreg")
using("textreg")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
data.all <- sent_detect_nlp(data.all)
 data.all <- tolower(data.all)     data.all <- removeNumbers(data.all)     data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)     data.all <- gsub("http[[:alnum:]]*", "", data.all)     data.all <- stripWhitespace(data.all)     data.all <- gsub("\u0092", "'", data.all)     data.all <- gsub("\u0093|\u0094", "", data.all)
data.all <- tolower(data.all)
    data.all <- removeNumbers(data.all)
    data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)
    data.all <- gsub("http[[:alnum:]]*", "", data.all)
    data.all <- stripWhitespace(data.all)
    data.all <- gsub("\u0092", "'", data.all)
    data.all <- gsub("\u0093|\u0094", "", data.all)
    data.all <- gsub("\u0093|\u0094", "", data.all)
corpus.data <- build.corpus(corpus.data, labeling, banned = bad_words, verbosity = 1, token.type = "word")
corpus.data <- build.corpus(data.all, banned = bad_words, verbosity = 1, token.type = "word")
corpus.data <- build.corpus(data.all,labeling = c(TRUE), banned = bad_words, verbosity = 1, token.type = "word")
corpus.data <- clean.text(corpus.data)
corpus.data
  corpus.data <- clean.text(corpus.data)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
corpus.original <- PCorpus(VectorSource(data.all), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
save.corpus.to.files(corpus.original)
corpus.stemmed <- stem.corpus(convert.tm.to.character(corpus.original))
corpus.original <- build.corpus(data.all, labeling = c(TRUE), banned = bad_words, verbosity = 1, token.type = "word")
phrases(corpus.original)
corpus.original
is.textreg.corpus(corpus.original)
ccc <- textreg(corpus.stemmed,TRUE)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
    corpus.textreg <- textreg(data.all, labeling = c(TRUE), banned = bad_words, verbosity = 1, token.type = "word")
    corpus.stemmed <- stem.corpus(convert.tm.to.character(corpus.original))
corpus.data <- tm_map(corpus.data, removeWords, bad_words)     corpus.original <- clean.text(corpus.original)
warnings()
calc.loss(corpus.textreg)
ccc <- textreg(corpus.stemmed, c("news", "blogs", "tweets"))
corpus.result <- textreg(data.all, labeling = c(TRUE), banned = bad_words, verbosity = 1, token.type = "word")
print.textreg.corpus()
predict.textreg.result()
is.textreg.result(corpus.result)
make.appearance.matrix
(corpus.result)
make.appearance.matrix(corpus.result)
make.count.table(c("hello"),c(TRUE,FALSE),corpus.stemmed)
make.phrase.correlation.chart(corpus.result)
make.phrase.correlation.chart(corpus.result,FALSE,5,FALSE)
make.similarity.matrix(corpus.result)
phrase.count("I want a", corpus.original)
phrases(corpus.result)
make.phrase.correlation.chart(corpus.result, count = FALSE, num.groups = 5, use.corrplot = FALSE)
corpus.original <- PCorpus((dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite",))
db <- dbInit(database_name, "SQLite")
corpus.original <- PCorpus((dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite",))
corpus.original <- PCorpus(VectorSource(dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite",))
using("glue")
install.packages("glue")
corpus.original <- PCorpus(VectorSource(c(dbLoad(db))), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite",))
using("tau")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
 save_file("https://goo.gl/To9w5B", "bad_word_list.txt")     bad_words <- readLines("./bad_word_list.txt")     log("database--> creation started...")     data.all <- as.data.frame(data.all)     data.all <- sent_detect(data.all) #Detect and split sentences on endmark boundaries.     data.all <- tolower(data.all)     data.all <- removeNumbers(data.all)     data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)     data.all <- gsub("http[[:alnum:]]*", "", data.all)     data.all <- stripWhitespace(data.all)     data.all <- gsub("\u0092", "'", data.all)     data.all <- gsub("\u0093|\u0094", "", data.all)
corpus.original <- PCorpus(VectorSource(data.all), readerControl = list(language = "english"), dbControl = list(dbName = database_name, dbType = "SQLite"))
corpus.original <- suppressWarnings(PCorpus(VectorSource(c(dbLoad(db))), readerControl = list(language = "english"), dbControl = list(dbName = database_name, dbType = "SQLite")))
db <- dbInit(database_name, "SQLite")
    corpus.original <- suppressWarnings(PCorpus(VectorSource(c(dbLoad(db))), readerControl = list(language = "english"), dbControl = list(dbName = database_name, dbType = "SQLite")))
    rm(corpus.original)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
save_file("https://goo.gl/To9w5B", "bad_word_list.txt")     bad_words <- readLines("./bad_word_list.txt")     log("database--> creation started...")     data.all <- as.data.frame(data.all)     data.all <- sent_detect(data.all) #Detect and split sentences on endmark boundaries.
using("tm")
update.packages("tm")
save_file("https://goo.gl/To9w5B", "bad_word_list.txt")     bad_words <- readLines("./bad_word_list.txt")
    log("database--> creation started...")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
p_load("log4r")
using("futile.logger")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
db <- dbInit(database_name, "SQLite")
save_file("https://goo.gl/To9w5B", "bad_word_list.txt")     bad_words <- readLines("./bad_word_list.txt")     flog.info("database--> creation started...")     data.all <- as.data.frame(data.all)     data.all <- sent_detect(data.all) #Detect and split sentences on endmark boundaries.
   data.all <- tolower(data.all)     data.all <- removeNumbers(data.all)     data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)     data.all <- gsub("http[[:alnum:]]*", "", data.all)     data.all <- stripWhitespace(data.all)     data.all <- gsub("\u0092", "'", data.all)     data.all <- gsub("\u0093|\u0094", "", data.all)
    data.all <- tolower(data.all)
    data.all <- removeNumbers(data.all)
    data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)
    data.all <- gsub("http[[:alnum:]]*", "", data.all)
    data.all <- stripWhitespace(data.all)
    data.all <- gsub("\u0092", "'", data.all)
    data.all <- gsub("\u0093|\u0094", "", data.all)
    suppressWarnings(corpus.original <- PCorpus(VectorSource(data.all), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite")))
    toEmpty <- content_transformer(function(x, pattern) gsub(pattern, "", x, fixed = TRUE))
    toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x, fixed = TRUE))
    corpus.original <- tm_map(corpus.original, toEmpty, "#\\w+")
    corpus.original <- tm_map(corpus.original, toEmpty, "(\\b\\S+\\@\\S+\\..{1,3}(\\s)?\\b)")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "Windows-1252")
if (!file.exists("data/bad_words.RDS")) {     save_file("https://goo.gl/To9w5B", "bad_word_list.txt")     bad_words <- readLines("./bad_word_list.txt")     saveRDS("data/bad_words.RDS") } load("data/bad_words.RDS")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "Windows-1252")
    saveRDS(bad_words,"data/bad_words.RDS")
saveRDS(bad_words,"./data/bad_words.RDS")
saveRDS(bad_words, get_data_file_path("bad_words.RDS"))
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
load(get_data_file_path("bad_words.RDS"))
  bad_words <- readLines("./bad_word_list.txt")     save(bad_words, get_data_file_path("bad_words.RData"))
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
load(get_data_file_path("bad_words.RData"))
save(bad_words, get_data_file_path("bad_words.RData"))
save(bad_words,file =  get_data_file_path("bad_words.RData"))
load(get_data_file_path("bad_words.RData"))
if (!file.exists(get_data_file_path("data_cleaned.RData"))) {     flog.info("data cleaning...")     data.all <- as.data.frame(data.all)     data.all <- sent_detect(data.all) #Detect and split sentences on endmark boundaries.     data.all <- tolower(data.all)     data.all <- removeNumbers(data.all)     data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)     data.all <- stripWhitespace(data.all)     data.all <- gsub("\u0092|\u0093|\u0094", "", data.all)     data.all <- gsub("(ftp|http)(s?)://.*\\b", "", data.all) # urls     data.all <- gsub("\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\b", "", data.all) # emails      data.all <- gsub("[@][a - zA - Z0 - 9_]{1,15}", "", data.all) # twitter usernames      data.all <- gsub("RT |via", "", data.all) # twitter tags      save(data.all, file = get_data_file_path("data_cleaned.RData")) }
load(get_data_file_path("data_cleaned.RData"))
load(get_data_file_path("data_cleaned.RData"))
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "Windows-1252")
p_load("tm")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "Windows-1252")
require(ngram)
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
n1 <- ngram::ngram_asweka(data.all, min = 1, max = 1)
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
ds <- sapply(data.all, as.character) # a character vector
n1 <- ngram::ngram_asweka(ds, min = 1, max = 1)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
flog.warn(w)
 ds <- sapply(data.all, as.character) # a character vector
n1 <- ngram::ngram_asweka(ds, min = 1, max = 1)
ds <- data.all[]
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
ds <- as.character(data.all[,])
ds <- as.character(data.all[])
require(tidyr)
data.stringified <- paste(data.all, collapse = '')
length(data.stringified)
length(data.stringified[0])
length(data.stringified[1])
class(data.stringified)
n1 <- ngram::ngram_asweka(data.stringified, min = 1, max = 1)
n2 <- ngram::ngram_asweka(data.stringified, min = 2, max = 2)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
 data.all <- gsub("[\d-]", "", data.all) # remove numbers
  flog.info("data cleaning...")     #data.all <- as.data.frame(data.all)     data.all <- sent_detect(data.all) #Detect and split sentences on endmark boundaries.     data.all <- gsub("\u0092|\u0093|\u0094", "", data.all)     data.all <- gsub("(ftp|http)(s?)://.*\\b", "", data.all) # urls     data.all <- gsub("\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\b", "", data.all) # emails      data.all <- gsub("[@][a - zA - Z0 - 9_]{1,15}", "", data.all) # twitter usernames      data.all <- gsub("RT |via", "", data.all) # twitter tags      data.all <- gsub("[\d-]", "", data.all) # remove numbers     data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)     data.all <- stripWhitespace(data.all)     data.all <- tolower(data.all)     save(data.all, file = get_data_file_path("data.all.RData"))
 flog.info("data cleaning...")     #Create clean functions     remove_symbols <- function(corpus) gsub(perl = TRUE,                                         pattern = '[\\]\\[\\(\\)-/+;:#%$^\\*=^~\\{\\}/"<>«»_\\\\“\\”⁰•‘’–]',                                         replacement = "", corpus)     convert_to_period <- function(corpus) gsub(pattern = "[\\!\\?…]",                                            replacement = ".", corpus)     reduce_periods <- function(corpus) gsub(pattern = "[\\.]{2,}",                                         replacement = ".", corpus)     convert_to_and <- function(corpus) gsub(pattern = "&", replacement = " and ", corpus)     replace_numbers <- function(corpus) gsub(pattern = "[0-9]+",                                          replacement = "", corpus)     #data.all <- as.data.frame(data.all)     data.all <- sent_detect(data.all) #Detect and split sentences on endmark boundaries.     data.all <- convert_to_and(data.all)     data.all <- convert_to_period(data.all)     data.all <- remove_symbols(data.all)     data.all <- reduce_periods(data.all)     data.all <- replace_numbers(data.all)     data.all <- gsub("(ftp|http)(s?)://.*\\b", "", data.all) # urls     data.all <- gsub("\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\b", "", data.all) # emails      data.all <- gsub("[@][a - zA - Z0 - 9_]{1,15}", "", data.all) # twitter usernames      data.all <- gsub("RT |via", "", data.all) # twitter tags      data.all <- gsub("[\d-]", "", data.all) # remove numbers     data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)     data.all <- stripWhitespace(data.all)     data.all <- tolower(data.all)     save(data.all, file = get_data_file_path("data.all.RData"))
 flog.info("data cleaning...")     #Create clean functions     remove_symbols <- function(corpus) gsub(perl = TRUE,                                         pattern = '[\\]\\[\\(\\)-/+;:#%$^\\*=^~\\{\\}/"<>«»_\\\\“\\”⁰•‘’–]',                                         replacement = "", corpus)     convert_to_period <- function(corpus) gsub(pattern = "[\\!\\?…]",                                            replacement = ".", corpus)     reduce_periods <- function(corpus) gsub(pattern = "[\\.]{2,}",                                         replacement = ".", corpus)     convert_to_and <- function(corpus) gsub(pattern = "&", replacement = " and ", corpus)     replace_numbers <- function(corpus) gsub(pattern = "[0-9]+",                                          replacement = "", corpus)     #data.all <- as.data.frame(data.all)     data.all <- sent_detect(data.all) #Detect and split sentences on endmark boundaries.     data.all <- convert_to_and(data.all)     data.all <- convert_to_period(data.all)     data.all <- remove_symbols(data.all)     data.all <- reduce_periods(data.all)     data.all <- replace_numbers(data.all)     data.all <- gsub("(ftp|http)(s?)://.*\\b", "", data.all) # urls     data.all <- gsub("\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\b", "", data.all) # emails      data.all <- gsub("[@][a - zA - Z0 - 9_]{1,15}", "", data.all) # twitter usernames      data.all <- gsub("RT |via", "", data.all) # twitter tags      data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)     data.all <- stripWhitespace(data.all)     data.all <- tolower(data.all)     save(data.all, file = get_data_file_path("data.all.RData"))
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
babble(ng = ng, genlen = 12)
babble(ng = n1, genlen = 12)
get.phrasetable(n2)
class(n2)
ng1 <- get.ngrams(n1)
require(ngram)
ng1 <- get.ngrams(n1)
get.ngrams(ng)
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
class(n2)
nx <- ngram(n2)
nx
class(nx)
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
n2 <- ngram(ngram::ngram_asweka(data.stringified, min = 2, max = 2, sep = " "))
n2 <- ngram(ngram::ngram_asweka(data.stringified, min = 2, max = 2, sep = " "))     n3 <- ngram(ngram::ngram_asweka(data.stringified, min = 3, max = 3, sep = " "))     n4 <- ngram(ngram::ngram_asweka(data.stringified, min = 4, max = 4, sep = " "))     n5 <- ngram(ngram::ngram_asweka(data.stringified, min = 5, max = 5, sep = " "))
get.phrasetable(n2)
babble(n2, genlen = 5, seed = 2017)
babble(n2, genlen = 5, seed = 2017)
babble(n2, genlen = 3, seed = 2017)
get.ngrams()
get.ngrams(n2)
get.Pearsall Primary School
gc()
    rm(remove_symbols)     rm(reduce_periods)     rm(convert_to_and)     rm(convert_to_period)     rm(replace_numbers)
get.nextwords(n2)
wordcount(n2)
wordcount(data.stringified)
textcnt()
wordcount(n2, sep = " ", count.function = sum)
wordcount(data.stringified, sep = " ", count.function = sum)
n5$n
require(tau)
n2 <- textcnt(data.stringified,n=2,method = "ngram")
n2
n2 <- textcnt(data.stringified, n = 2, method = "string")
p_load("data.table")
n2 <- create_ngram(data.stringified, 2)
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
n2 <- sqldf("select * from n2 where freq > 100")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
n2 <- sqldf("select * from n2 where fpredictor <- function() {     sample1 <- function() {            cat("sample1")     }     sample2 <- function() {         cat("sample2")     } }req > 100")
sample1Fn <- function() {     cat("sample1") } predictor <- function() {     sample1 <- sample1Fn() }
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "utf-8")
predictor().sample1Fn()if (!file.exists(get_data_file_path("bad_words.RData"))) {     save_file("https://goo.gl/To9w5B", "bad_word_list.txt")     bad_words <- readLines("./bad_word_list.txt")     save(bad_words, file = get_data_file_path("bad_words.RData")) }
overwrite <- FALSE ## bad words if (!file.exists(get_data_file_path("bad_words.RData"))) {     save_file("https://goo.gl/To9w5B", "bad_word_list.txt")     bad_words <- readLines("./bad_word_list.txt")     save(bad_words, file = get_data_file_path("bad_words.RData")) }
load(get_data_file_path("bad_words.RData"))
  flog.info("data cleaning...")     #Create clean functions     remove_symbols <- function(corpus) gsub(perl = TRUE,                                         pattern = '[\\]\\[\\(\\)-/+;:#%$^\\*=^~\\{\\}/"<>«»_\\\\“\\”⁰•‘’–]',                                         replacement = "", corpus)     convert_to_period <- function(corpus) gsub(pattern = "[\\!\\?…]",                                            replacement = ".", corpus)     reduce_periods <- function(corpus) gsub(pattern = "[\\.]{2,}",                                         replacement = ".", corpus)     convert_to_and <- function(corpus) gsub(pattern = "&", replacement = " and ", corpus)     replace_numbers <- function(corpus) gsub(pattern = "[0-9]+",                                          replacement = "", corpus)     #data.all <- as.data.frame(data.all)     data.all <- sent_detect(data.all) #Detect and split sentences on endmark boundaries.     data.all <- convert_to_and(data.all)     data.all <- convert_to_period(data.all)     data.all <- remove_symbols(data.all)     data.all <- reduce_periods(data.all)     data.all <- replace_numbers(data.all)     data.all <- gsub("(ftp|http)(s?)://.*\\b", "", data.all) # urls     data.all <- gsub("\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\b", "", data.all) # emails      data.all <- gsub("[@][a - zA - Z0 - 9_]{1,15}", "", data.all) # twitter usernames      data.all <- gsub("RT |via", "", data.all) # twitter tags      data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)     data.all <- stripWhitespace(data.all)     data.all <- tolower(data.all)     save(data.all, file = get_data_file_path("data.all.RData"))     rm(remove_symbols)     rm(reduce_periods)     rm(convert_to_and)     rm(convert_to_period)     rm(replace_numbers)
load(get_data_file_path("data.all.RData"))
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
if (is.na(word_string) || length(word_string == 0)) ERROR("word_string NA or empty")
ERROR("word_string NA or empty")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function(sentence_string) {     if (is.na(sentence_string) || length(sentence_string == 0)) throw("sentence_string NA or empty")     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         result <- sqldf("select * from n2 where w1w2 like '%sentence_string' order by freq desc")         result     } } predictor("a")
 warning("sentence_string NA or empty")
throw()
throw
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function(sentence_string) {     if (is.na(sentence_string) || length(sentence_string == 0)) {         warning("sentence_string NA or empty")         stop()     }     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         result <- sqldf("select * from n2 where w1w2 like '%sentence_string' order by freq desc")         result     } } predictor("a")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function(sentence_string) {     flog.debug(paste("predictor input", sentence_string))     if (is.na(sentence_string) || length(sentence_string == 0)) {         warning("sentence_string NA or empty")         stop()     }     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         result <- sqldf("select * from n2 where w1w2 like '%sentence_string' order by freq desc")         result     } } predictor("a")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function(sentence_string) {     flog.debug(paste("predictor input", sentence_string))     sentence_string     if (is.na(sentence_string) || length(sentence_string == 0)) {         warning("sentence_string NA or empty")         stop()     }     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         result <- sqldf("select * from n2 where w1w2 like '%sentence_string' order by freq desc")         result     } } predictor("a")
predictor("dfgsdfga")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function(sentence_string) {     flog.debug(paste("predictor input", sentence_string))     #sentence_string     #if (is.na(sentence_string) || length(sentence_string == 0)) {         #warning("sentence_string NA or empty")         #stop()     #}     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         result <- sqldf("select * from n2 where w1w2 like '%sentence_string' order by freq desc")         result     } } predictor("dfgsdfga")
sql <- paste("select * from n2 where word like '%", sentence_string, "%'")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function(sentence_string) {     flog.debug(paste("predictor input", sentence_string))     #sentence_string     #if (is.na(sentence_string) || length(sentence_string == 0)) {         #warning("sentence_string NA or empty")         #stop()     #}     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         sql <- paste("select * from n2 where word like '%", sentence_string, "%'")         result <- sqldf(sql)         result     } } predictor("dfgsdfga")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function(sentence_string) {     flog.debug(paste("predictor input", sentence_string))     #sentence_string     #if (is.na(sentence_string) || length(sentence_string == 0)) {         #warning("sentence_string NA or empty")         #stop()     #}     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         sql <- paste("select * from n2 where w1w2 like '%", sentence_string, "%'")         result <- sqldf(sql)         result     } } predictor("dfgsdfga")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function(sentence_string) {     flog.debug(paste("predictor input", sentence_string))     sentence_string     #if (is.na(sentence_string) || length(sentence_string == 0)) {         #warning("sentence_string NA or empty")         #stop()     #}     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         sql <- paste("select * from n2 where w1w2 like '%", sentence_string, "%'")         result <- sqldf(sql)         result     } } predictor("dfgsdfga")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function(sentence_string) {     flog.info(paste("predictor input", sentence_string))    $ sentence_string     #if (is.na(sentence_string) || length(sentence_string == 0)) {         #warning("sentence_string NA or empty")         #stop()     #}     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         sql <- paste("select * from n2 where w1w2 like '%", sentence_string, "%'")         result <- sqldf(sql)         result     } } predictor("dfgsdfga")
sentence_string <- "a" #' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function() {     flog.info(paste("predictor input", sentence_string))    $ sentence_string     #if (is.na(sentence_string) || length(sentence_string == 0)) {         #warning("sentence_string NA or empty")         #stop()     #}     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         sql <- paste("select * from n2 where w1w2 like '%", sentence_string, " %'")         result <- sqldf(sql)         result     } } predictor("dfgsdfga")
sentence_string <- "a" #' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function() {     flog.info(paste("predictor input", sentence_string))    $ sentence_string     #if (is.na(sentence_string) || length(sentence_string == 0)) {         #warning("sentence_string NA or empty")         #stop()     #}     words = strsplit(sentence_string, " ")     if (length(words) == 1)     {         sql <- paste("select * from n2 where w1w2 like '%", sentence_string, " %'")         result <- sqldf(sql)         result     } } predictor("dfgsdfga")
sentence_string <- "a" #' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function() {     flog.info(paste("predictor input", sentence_string))     if (is.na(sentence_string) || length(sentence_string == 0)) {         warning("sentence_string NA or empty")         #stop()     }     words = strsplit(sentence_string, " ")     if (length(words) == 1)     {         sql <- paste("select * from n2 where w1w2 like '%", sentence_string, " %'")         result <- sqldf(sql)         result     } } predictor("dfgsdfga")
sentence_string <- "a" #' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function() {     flog.info(paste("predictor input", sentence_string))     if (is.na(sentence_string) || length(sentence_string == 0)) {         warning("sentence_string NA or empty")         #stop()     }     words = strsplit(sentence_string, " ")     if (length(words) == 1)     {         sql <- paste("select * from n2 where w1w2 like '%", sentence_string, " %'")         result <- sqldf(sql)         result     } } predictor()
sss <- "a" #' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function() {     flog.info(paste("predictor input", sss))     if (is.na(sss) || length(sss == 0)) {         warning("sentence_string NA or empty")         #stop()     }     words = strsplit(sss, " ")     if (length(words) == 1)     {         sql <- paste("select * from n2 where w1w2 like '%", words[1], " %'")         result <- sqldf(sql)         result     } } predictor()
sss <- "a" #' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function() {     flog.info(paste("predictor input", sss))     if (is.na(sss)) {         warning("sentence_string NA or empty")         #stop()     }     words = strsplit(sss, " ")     if (length(words) == 1)     {         sql <- paste("select * from n2 where w1w2 like '%", words[1], " %'")         result <- sqldf(sql)         result     } } predictor()
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence  #' #' @return #' @export #' #' @examples predictor <- function(sentence) {     flog.info(paste("predictor input", sentence))     if (is.na(sss)) {         warning("sentence_string NA or empty")         #stop()     }     words = strsplit(sentence, " ")     if (length(words) == 1)     {         sql <- paste("select * from n2 where w1w2 like '%", words[1], " %'")         result <- sqldf(sql)         result     } } predictor("beer")
source("C:/dev/r-course/10-capstone/project_1__setup.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence  #' #' @return #' @export #' #' @examples predictor <- function(sentence) {     flog.info(paste("predictor input", sentence))     if (is.na(sss)) {         warning("sentence_string NA or empty")         #stop()     }     words = strsplit(sentence, " ")     if (length(words) == 1)     {         sql <- paste("select * from n2 where w1w2 like '%", words[1], "%'")         flog.info(paste("predictor sql", sql))         result <- sqldf(sql)         result     } } predictor("beer")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence  #' #' @return #' @export #' #' @examples predictor <- function(sentence) {     flog.info(paste("predictor input", sentence))     if (is.na(sss)) {         warning("sentence_string NA or empty")         #stop()     }     words = strsplit(sentence, " ")     if (length(words) == 1)     {         sql <- paste0("select * from n2 where w1w2 like '%", words[1], "%'")         flog.info(paste("predictor sql", sql))         result <- sqldf(sql)         result     } } predictor("beer")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence  #' #' @return #' @export #' #' @examples predictor <- function(sentence) {     flog.info(paste("predictor input", sentence))     if (is.na(sss)) {         warning("sentence_string NA or empty")         #stop()     }     words = strsplit(sentence, " ")     if (length(words) == 1)     {         sql <- paste0("select * from n2 where w1w2 like '%", words[1], "%'")         flog.info(paste("predictor sql", sql)) 
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "utf-8")
