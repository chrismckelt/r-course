p_load("shinyjs")
p_load("choroplethr")
p_load("choroplethrMaps")
p_load("DescTools")
p_load("readxl")
p_load("devtools")
p_load("ggplot2")
p_load("plotly")
p_load("DT")
library(pacman)
p_load("pacman")
p_load("tidyverse")
p_load("knitr")
p_load("markdown")
p_load("data.table")
p_load("sqldf")
p_load("ggplot2")
p_load("lubridate")
p_load("foreach")
p_load("RSQLite")
p_load("shiny")
p_load("shinyjs")
p_load("choroplethr")
p_load("choroplethrMaps")
p_load("DescTools")
p_load("readxl")
p_load("devtools")
p_load("ggplot2")
p_load("plotly")
p_load("DT")
runApp('9-data-products/week-4')
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
shiny::runApp()
setwd("C:/dev/r-course/9-data-products/week-4")
unzip(zipfile  = "./data/lending-club-loan-data.zip")
unzip(zipfile  = "./data/lending-club-loan-data.zip", exdir=paste(getwd(), "data"))
suppressMessages(rm(list = ls()))
unzip(zipfile  = "./data/lending-club-loan-data.zip", exdir=paste(getwd(), "data"))
outdir <- paste(getwd(), "data")
outdir
unzip(zipfile  = "./data/lending-club-loan-data.zip", exdir=paste(getwd(), "/data"))
outdir <- paste(getwd(), "/data")
unzip(zipfile  = "./data/lending-club-loan-data.zip", exdir=outdir)
unzip(zipfile  = "./data/lending-club-loan-data.zip", outdir)
switch(Sys.info()[['sysname']],
Windows= {suppressMessages(setwd("C:/dev/r-course/9-data-products/week-4"))},
Linux  = {suppressMessages(setwd("~/srv/connect/apps/loan_book_analyser"))},
Darwin = {print("I'm a Mac.")})
outdir <- paste(getwd(), "/data")
unzip(zipfile  = "./data/lending-club-loan-data.zip", outdir)
shiny::runApp()
outdir <- paste(getwd(), "/data")
paste(zipfile, "lending-club-loan-data.zip")
paste(outdir, "lending-club-loan-data.zip")
paste(trimws(outdir), "lending-club-loan-data.zip")
outdir <- paste(trimws(getwd()), "/data")
paste(trimws(outdir), "lending-club-loan-data.zip")
outdir <- paste(trimws(getwd()), "data")
outdir
outdir <- paste(trimws(getwd()), "//data")
outdir
outdir <- paste0(trimws(getwd()), "data")
outdir
outdir <- paste0(trimws(getwd()), "/data")
outdir
zippedFile <- paste0(trimws(outdir), "lending-club-loan-data.zip")
zippedFile
outdir <- paste0(trimws(getwd()), "/data")
outdir
zippedFile <- paste0(trimws(outdir), "lending-club-loan-data.zip")
zippedFile
outdir <- paste0(trimws(getwd()), "/data/")
outdir
zippedFile <- paste0(trimws(outdir), "lending-club-loan-data.zip")
zippedFile
unzip(zipfile  = zippedFile, outdir)
unzip(zippedFile, outdir)
zippedFile
outdir
unzip(zippedFile, outdir)
unzip(zippedFile)
?unzip
unzip(zippedFile, exdir=outdir)
unzip(zipfile=zippedFile, exdir=outdir)
unzip(zipfile=zippedFile,exdir = outdir)
outdir <- paste0(trimws(getwd()), "/data")
outdir
zippedFile <- paste0(trimws(outdir), "/lending-club-loan-data.zip")
zippedFile
unzip(zipfile=zippedFile,exdir = outdir)
shiny::runApp()
shiny::runApp()
shiny::runApp()
runApp()
runApp()
runApp()
source('C:/dev/r-course/9-data-products/week-4/index.R', echo=TRUE)
install.packages("rsconnect")
source('C:/dev/r-course/9-data-products/week-4/index.R', echo=TRUE)
source('C:/dev/r-course/9-data-products/week-4/index.R', echo=TRUE)
source('C:/dev/r-course/9-data-products/week-4/index.R', echo=TRUE)
shiny::runApp()
runApp()
shiny::runApp()
source('C:/dev/r-course/9-data-products/week-4/index.R', echo=TRUE)
source("C:/dev/r-course/10-capstone/quiz1.R", encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
p_load("stringr")
suppressMessages(setwd("c:/dev/r-course/10-capstone"))
blogs <- readLines("final/en_US/en_US.blogs.txt")
blogs <- readLines("final/en_US/en_US.blogs.txt")
blogs <- readLines("final/en_US/en_US.blogs.txt", local = locale(encoding = "latin1"))
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
unzip(zipfile = "c:/dev/r-course/10-capstone/data.zip", exdir = "c:/dev/r-course/10-capstone")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
<<<<<<< HEAD
sessionInfo()
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
=======
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
Sys.getlocale()
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
library(tm)
?tm
???tm
??tm
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz1.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
help.search()
help
help.start
help.start()
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/week2/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
repo <- new("TextRepository", .Data = list(blogs,news,twitter))
?tm
tmIndex
TermDocMatrlibrary(tm)
library(tm)
blogs <- read_file("final/en_US/en_US.blogs.txt")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
summary(data.blogs)
DescTools::Desc(data.blogs, plotit = TRUE)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
src <- DirSource("c:/dev/r-course/10-capstone/final")
corpus <- Corpus(src)
corpus[[1]]$content
strwrap(corpus[[1]])
class(corpus)
corpus
src <- DirSource("c:/dev/r-course/10-capstone/final/en_US/")
corpus <- Corpus(src)
?Corpus
corpus <- Corpus(c(data.blogs, data.news,data.twitter))
lst <- as.list(c(data.blogs, data.news, data.twitter))
corpus <- Corpus(lst)
df.blogs <- data.frame(text = unlist(sapply(data.blogs, `[`, "content")), stringsAsFactors = F)
df.twitter <- data.frame(text = unlist(sapply(data.twitter, `[`, "content")), stringsAsFactors = F)
df.news <- data.frame(text = unlist(sapply(data.news, `[`, "content")), stringsAsFactors = F)
corpus <- Corpus(c(df.blogs,df.news,df.twitter))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
install_packages("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust","cluster", "igraph", "fpc")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
lst <- list("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc")
do.call("install_extra_packages", lst)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
lst <- list("tm", "SnowballCC", "RColorBrewer", "ggplot2", "wordcloud", "biclust", "cluster", "igraph", "fpc") install_extra_packages(lst)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
df.blogs <- sample(data.frame(text = unlist(sapply(data.blogs, `[`, "content")), stringsAsFactors = F), 1000)
df.blogs <- sample(data.frame(text = unlist(sapply(data.blogs, `[`, "content")), stringsAsFactors = F), 30)
sample.blogs <- sample(data.frame(text = unlist(sapply(data.blogs, `[`, "content")), stringsAsFactors = F), 100) sample.news <- sample(data.frame(text = unlist(sapply(data.news, `[`, "content")), stringsAsFactors = F), 100) sample.twitter <- sample(data.frame(text = unlist(sapply(data.twitter, `[`, "content")), stringsAsFactors = F), 100) sample.all <- c(df.blogs, df.news, df.twitter)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
corpus <- Corpus(VectorSource(list(sample.all)))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
sample.blogs <- sample(data.frame(text = unlist(sapply(data.blogs, `[`, "content")), stringsAsFactors = F), 100) sample.news <- sample(data.frame(text = unlist(sapply(data.news, `[`, "content")), stringsAsFactors = F), 100) sample.twitter <- sample(data.frame(text = unlist(sapply(data.twitter, `[`, "content")), stringsAsFactors = F), 100) sample.all <- c(sample.blogs, sample.news, sample.twitter)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
data <- readLines(con)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
data.all <- c(data.blogs, data.news, data.twitter)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
uniGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1)) biGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2)) triGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3)) uniGramMatrix <- TermDocumentMatrix(documents, control = list(tokenize = uniGramTokenizer)) biGramMatrix <- TermDocumentMatrix(documents, control = list(tokenize = biGramTokenizer)) triGramMatrix <- TermDocumentMatrix(documents, control = list(tokenize = triGramTokenizer))
uniGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1)) biGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2)) triGramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3)) uniGramMatrix <- TermDocumentMatrix(documents, control = list(tokenize = uniGramTokenizer)) biGramMatrix <- TermDocumentMatrix(documents, control = list(tokenize = biGramTokenizer)) triGramMatrix <- TermDocumentMatrix(documents, control = list(tokenize = triGramTokenizer))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
sample.all <- sample(c(sample.blogs, sample.news, sample.twitter), 5000, replace = F)
sample.blogs <- sample(data.frame(text = unlist(sapply(data.blogs, `[`, "content")), stringsAsFactors = F), 20000) sample.news <- sample(data.frame(text = unlist(sapply(data.news, `[`, "content")), stringsAsFactors = F), 20000) sample.twitter <- sample(data.frame(text = unlist(sapply(data.twitter, `[`, "content")), stringsAsFactors = F), 20000) sample.all <- sample(c(sample.blogs, sample.news, sample.twitter), 5000, replace = F)
sample.blogs <- sample(data.frame(text = unlist(sapply(data.blogs, `[`, "content")), stringsAsFactors = F), 20000) sample.news <- sample(data.frame(text = unlist(sapply(data.news, `[`, "content")), stringsAsFactors = F), 20000) sample.twitter <- sample(data.frame(text = unlist(sapply(data.twitter, `[`, "content")), stringsAsFactors = F), 20000) sample.all <- sample(c(sample.blogs, sample.news, sample.twitter), 5000, replace = F) remove(sample.blogs) remove(sample.news) remove(sample.twitter)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
sample.all <- c(sample.blogs, sample.news, sample.twitter)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
corpus <- Corpus(VectorSource(list(sample.blogs, sample.news, sample.twitter)))
remove(data.blogs) remove(data.news) remove(data.twitter) remove(sample.blogs) remove(sample.news) remove(sample.twitter) remove(sample.all)
corpus <- tm_map(corpus, tolower) corpus <- tm_map(corpus, removePunctuation) corpus <- tm_map(corpus, removeNumbers) corpus <- tm_map(corpus, removeWords, stopwords("english")) corpus <- tm_map(corpus, stripWhitespace)   corpus <- tm_map(corpus, toEmpty, "#\\w+")   corpus <- tm_map(corpus, toEmpty, "(\\b\\S+\\@\\S+\\..{1,3}(\\s)?\\b)")   corpus <- tm_map(corpus, toEmpty, "@\\w+")   corpus <- tm_map(corpus, toEmpty, "http[^[:space:]]*")   corpus <- tm_map(corpus, toSpace, "/|@|\\|") writeCorpus(corpus, filenames = "corpus.txt")
# clean  ##  custom content transformers toEmpty <- content_transformer(function(x, pattern) gsub(pattern, "", x, fixed = TRUE)) toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x, fixed = TRUE)) corpus <- tm_map(corpus, tolower) corpus <- tm_map(corpus, removePunctuation) corpus <- tm_map(corpus, removeNumbers) corpus <- tm_map(corpus, removeWords, stopwords("english")) corpus <- tm_map(corpus, stripWhitespace)   corpus <- tm_map(corpus, toEmpty, "#\\w+")   corpus <- tm_map(corpus, toEmpty, "(\\b\\S+\\@\\S+\\..{1,3}(\\s)?\\b)")   corpus <- tm_map(corpus, toEmpty, "@\\w+")   corpus <- tm_map(corpus, toEmpty, "http[^[:space:]]*")   corpus <- tm_map(corpus, toSpace, "/|@|\\|")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
freqTerms <- findFreqTerms(matrix.uni)
freqTerms <- findFreqTerms(matrix.uni, lowfreq = 3000)
freqTerms <- findFreqTerms(matrix.uni, lowfreq = 5000)
freqTerms <- findFreqTerms(matrix.uni, lowfreq = 4000)
list_freqs <- lapply(corpus$dimnames$Docs,               function(i) findFreqTerms(corpus[corpus$dimnames$Docs == i], 2000))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
df <- as.data.frame(as.matrix(corpus))
df <- as.data.frame(as.matrix(freqTerms))
df <- as.data.frame(as.matrix(dtm$dimnames$Docs))
df <- as.data.frame(as.matrix(corpus[1]))
tm <- DocumentTermMatrix(corpus)
dtm <- DocumentTermMatrix(corpus)
df <- as.data.frame(as.matrix(dtm))
df <- data.frame(t(df))
require(ggplot2) ggplot(df, aes(X127, X144)) +     geom_text(label = rownames(df),            position = position_jitter())
df <- names("word", "blog_count", "news_count", "twitter_count")
df <- colnames("word", "blog_count", "news_count", "twitter_count")
cols <- c("word", "blog_count", "news_count", "twitter_count")
colnames(df) <- cols
cols <- c("blog_count", "news_count", "twitter_count") colnames(df) <- cols
top5.blogwords <- sqldf("select * from df order by blog_count desc limit 5")
df <- as.data.frame(as.matrix(dtm))
p_load("quanteda")
summary(corpus)
dtm <- DocumentTermMatrix(corpus)
plot(dtm, terms = findFreqTerms(dtm, lowfreq = 2000)[1:25], corThreshold = 0.5)
p_load("Rgraphviz")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
setDT(df, keep.rownames = TRUE)[]
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
cols <- c("word", "blog_count", "news_count", "twitter_count")
plot(dtm, terms = findFreqTerms(dtm, lowfreq = 2000)[1:25], corThreshold = 0.5)
df <- as.data.frame(as.matrix(dtm)) # and transpose for plotting df <- data.frame(t(df)) setDT(df, keep.rownames = TRUE)[] cols <- c("word", "blog_count", "news_count", "twitter_count") colnames(df) <- cols
top5.blogwords <- sqldf("select * from df order by blog_count desc limit 5")
top5.newswords <- sqldf("select word, news_count from df order by news_count desc limit 5")
top5.twitterwords <- sqldf("select word,twitter_count from df order by twitter_count desc limit 5")
top5.blogwords <- sqldf("select word, blog_count from df order by blog_count desc limit 5") top5.newswords <- sqldf("select word, news_count from df order by news_count desc limit 5") top5.twitterwords <- sqldf("select word,twitter_count from df order by twitter_count desc limit 5")
summary(corpus)
myCorpus <- corpus(corpus)
myCorpus <- quanteda::corpus(corpus)
ggplot(top5.blogwords, aes(x = word, fill = blog_count)) +     geom_histogram(position = "identity", bins = 20, show.legend = FALSE) +     facet_wrap(~blog_count, ncol = 1)
install.packages("Rcpp", lib="C:/Users/chris/Documents/R/win-library/3.4")
ggplot(top5.blogwords, aes(x = word, fill = blog_count)) +     geom_histogram(position = "identity", bins = 20, show.legend = FALSE) +     facet_wrap(~blog_count, ncol = 1)
ggplot(top5.blogwords, aes(x = word, fill = blog_count)) +     geom_histogram(position = "identity", bins = 20, show.legend = FALSE,stat = "count") +     facet_wrap(~blog_count, ncol = 1)
myCorpus <- quanteda::corpus(corpus) # build a new corpus from the texts
p_load("Rcpp")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
install.packages("Rcpp")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
corpus.data <- Corpus(VectorSource(lst)) corpus.summary <- quanteda::corpus(tmVCorpus(lst))
corpus.summary <- quanteda::corpus(VCorpus(lst))
inspect(corpus.data)
nTerms(corpus.data)
nDocs(corpus.data)
meta(corpus.data)
meta(corpus.data)
meta(corpus.data[1])
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
matrix.uni <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.uni))
matrix.bi <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.bi))
list_freqs <- lapply(corpus.data$dimnames$Docs,               function(i) findFreqTerms(corpus.data[corpus.data$dimnames$Docs == i], 2000))
freq.terms <- findFreqTerms(matrix.uni, lowfreq = 4000) freq.expressions <- findFreqTerms(bi, lowfreq = 4000)
freq.terms <- findFreqTerms(matrix.uni, lowfreq = 4000) freq.expressions <- findFreqTerms(matrix.bi, lowfreq = 4000)
nTerms(dtm)
dtm <- DocumentTermMatrix(corpus)
#https://stackoverflow.com/questions/17294824/counting-words-in-a-single-document-from-corpus-in-r-and-putting-it-in-dataframe dtm <- DocumentTermMatrix(corpus.data)
nTerms(dtm)
matrix.tri <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.tri))
tokenizer.tri <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
matrix.tri <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.tri))
freq.terms <- findFreqTerms(matrix.uni, lowfreq = 4000) freq.expressions <- findFreqTerms(matrix.bi, lowfreq = 4000)
freq.terms <- findFreqTerms(matrix.uni, 160)
freq.terms <- findFreqTerms(matrix.uni, 10) freq.expressions <- findFreqTerms(matrix.bi, 10)
list_freqs()
# find freq words for each doc, one by one list_freqs <- lapply(corpus.data$dimnames$Docs,               function(i) findFreqTerms(corpus.data[corpus.data$dimnames$Docs == i], 2000))
# find freq words for each doc, one by one list_freqs <- lapply(corpus.data$dimnames$Docs,               function(i) findFreqTerms(corpus.data[corpus.data$dimnames$Docs == i], 2000))
list_freqs()
list_freqs
#Tokenize sample into Unigrams, Bigrams and Trigrams tokenizer.uni <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1)) tokenizer.bi <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2)) tokenizer.tri <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3)) matrix.uni <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.uni)) matrix.bi <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.bi)) freq.terms <- findFreqTerms(matrix.uni, 10) freq.expressions <- findFreqTerms(matrix.bi, 10)
plot(dtm, terms = freq.terms[1:5], corThreshold = 0.5)
freq.terms <- findFreqTerms(matrix.uni, 10)
dtm <- DocumentTermMatrix(corpus.data)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
p_load("tidytext")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
tidy_books %>%     anti_join(stop_words) %>%     count(df$word) %>%     with(wordcloud(df$word, n, max.words = 100))
p_load("tidytext")
tidy_books %>%     anti_join(stop_words) %>%     count(df$word) %>%     with(wordcloud(df$word, n, max.words = 100))
p_load("tidytext")
tidy_books %>%     anti_join(stop_words) %>%     count(df$word) %>%     with(wordcloud(df$word, n, max.words = 100))
cols <- c("word", "blog_count", "news_count", "twitter_count") colnames(df) <- cols top.blogwords <- sqldf("select word, blog_count from df order by blog_count desc limit 100") top.newswords <- sqldf("select word, news_count from df order by news_count desc limit 100") top.twitterwords <- sqldf("select word,twitter_count from df order by twitter_count desc limit 100")
top.blogwords %>%     count(df$word) %>%     with(wordcloud(df$word, n, max.words = 100))
cols <- c("word", "blog_count", "news_count", "twitter_count") colnames(df) <- cols top.blogwords <- sqldf("select word, blog_count from df order by blog_count desc limit 100") top.newswords <- sqldf("select word, news_count from df order by news_count desc limit 100") top.twitterwords <- sqldf("select word,twitter_count from df order by twitter_count desc limit 100")
df %>%     count(df$word) %>%     with(wordcloud(df$word, n, max.words = 100))
top.blogwords <- sqldf("select word, blog_count from df order by blog_count desc limit 5") top.newswords <- sqldf("select word, news_count from df order by news_count desc limit 5") top.twitterwords <- sqldf("select word,twitter_count from df order by twitter_count desc limit 5")
nTerms(dtm)
sample.data <- bind_rows(sample.blogs %>%                       mutate(person = "blogs"),                         sample.news %>%                       mutate(person = "news"),                         sample.twitter %>%                       mutate(person = "twitter"),                       ) %>%                       mutate(timestamp = ymd_hms(timestamp))
#download_zip_files() # read files and adjust for encoding data.blogs <- read_file("final/en_US/en_US.blogs.txt") data.news <- read_file("final/en_US/en_US.news.txt") data.twitter <- read_file("final/en_US/en_US.twitter.txt") #data.all <- c(data.blogs, data.news, data.twitter) #sample data to speed things up sample.blogs <- sample(data.blogs, 20000) sample.news <- sample(data.news, 20000) sample.twitter <- sample(data.twitter, 20000) sample.all <- sample(c(sample.blogs, sample.news, sample.twitter), size = 10000, replace = TRUE) sample.data <- bind_rows(sample.blogs %>%                       mutate(person = "blogs"),                         sample.news %>%                       mutate(person = "news"),                         sample.twitter %>%                       mutate(person = "twitter"),                       ) %>%                       mutate(timestamp = ymd_hms(timestamp))
sample.data <- bind_rows(sample.blogs %>%                       mutate(person = "blogs"),                         sample.news %>%                       mutate(person = "news"),                         sample.twitter %>%                       mutate(person = "twitter"),                       ) %>%                       mutate(timestamp = ymd_hms(timestamp))
sample.data <- bind_rows(sample.blogs %>%                       mutate(person = "blogs"),                         sample.news %>%                       mutate(person = "news"),                         sample.twitter %>%                       mutate(person = "twitter"),                       ))
sample.data <- bind_rows(sample.blogs %>%                       mutate(person = "blogs"),                         sample.news %>%                       mutate(person = "news"),                         sample.twitter %>%                       mutate(person = "twitter")                       )
sample.data <- bind_rows(sample.blogs %>%                       mutate(person = "blogs"),                         sample.news %>%                       mutate(person = "news"),                         sample.twitter %>%                       mutate(person = "twitter")                       )
sample.data <- bind_rows(sample.blogs %>%                       mutate(source = "blogs"),                         sample.news %>%                         mutate(source = "news"),                         sample.twitter %>%                         mutate(source = "twitter")                       )
sample.data <- bind_rows(sample.blogs %>%                       mutate(source = "blogs"),                         sample.news %>%                         mutate(source = "news"),                         sample.twitter %>%                       mutate(source = "twitter")                       )
sample.data <- bind_rows(sample.blogs %>%                       mutate(x = "blogs"),                         sample.news %>%                         mutate(x = "news"),                         sample.twitter %>%                       mutate(x = "twitter")                       )
df %>%     count(df$word) %>%     with(wordcloud(df$word, n, max.words = 100))
warnings()p_load("revealjs")
using("revealjs")
install.packages("revealjs", type = "source")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
par(mfrow=c(1,1))
wordcloud(matrix.uni, freq.uni, scale=c(9,1), max.words=40, random.order=FALSE, colors=brewer.pal(7, "Dark2"))
top.blogwords$document <- "blog"
top.newswords$document <- "news"
top.twitterwords$document <- "twitter"
wordcloud(freq.uni, matrix.uni, scale=c(9,1), max.words=40, random.order=FALSE, colors=brewer.pal(7, "Dark2"))
wordcloud(top.blogwords, 1, scale = c(9, 1), max.words = 40, random.order = FALSE, colors = brewer.pal(7, "Dark2"))
w
wordcloud(top.blogwords, top.blogwords$blog_count, scale = c(9, 1), max.words = 40, random.order = FALSE, colors = brewer.pal(7, "Dark2"))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
dtm <- DocumentTermMatrix(corpus.data)     m <- as.matrix(dtm)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), freq = v)     head(d, 10)
dtm <- TermDocumentMatrix(corpus.data)     m <- as.matrix(dtm)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), freq = v)     head(d, 10)
delim <- " \\r\\n\\t.,;:\"()?!"     tokenizer.uni <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))     tokenizer.bi <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2), delimiters = delim)     tokenizer.tri <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3), delimiters = delim)     matrix.uni <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.uni))     matrix.bi <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.bi))     matrix.tri <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.tri))     freq.uni <- findFreqTerms(matrix.uni, 5,5)     freq.bi <- findFreqTerms(matrix.bi,  5,5)     freq.tri <- findFreqTerms(matrix.bi,  5,5)
Uni_gram <- NGramTokenizer(corpus.data, Weka_control(min = 1, max = 1)) Unigram = data.frame(table(Uni_gram)) Unigram = Unigram[order(Unigram$Freq, decreasing = T),] Ug <- Unigram[1:40,]
Uni_gram <- NGramTokenizer(corpus.data, Weka_control(min = 1, max = 1))
delim <- " \\r\\n\\t.,;:\"()?!"     tokenizer.uni <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))     tokenizer.bi <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2), delimiters = delim)     tokenizer.tri <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3), delimiters = delim) dtm.uni <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.uni)) dtm.bi <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.bi)) dtm.tri <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.tri)) freq.uni <- findFreqTerms(dtm.uni, 5,5) freq.bi <- findFreqTerms(dtm.bi,  5,5) freq.tri <- findFreqTerms(dtm.bi, 5, 5)
dtm <- TermDocumentMatrix(corpus.data)     m <- as.matrix(dtm)     v <- sort(rowSums(m), decreasing = TRUE)     d <- data.frame(word = names(v), freq = v)     head(d, 10)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
delim <- " \\r\\n\\t.,;:\"()?!"     tokenizer.uni <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))     tokenizer.bi <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2), delimiters = delim)     tokenizer.tri <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3), delimiters = delim)
dtm.uni <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.uni))     dtm.bi <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.bi))     dtm.tri <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.tri))
freq.uni <- findFreqTerms(dtm.uni, 5,5)     freq.bi <- findFreqTerms(dtm.bi,  5,5)     freq.tri <- findFreqTerms(dtm.tri, 5, 5)
Uni_gram <- NGramTokenizer(corpus.data, Weka_control(min = 1, max = 1))     Unigram = data.frame(table(Uni_gram))     Unigram = Unigram[order(Unigram$Freq, decreasing = T),] Ug <- Unigram[1:40,]
Uni_gram <- NGramTokenizer(corpus.data, Weka_control(min = 1, max = 1))
delim <- " \\r\\n\\t.,;:\"()?!"     tokenizer.uni <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))     tokenizer.bi <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2), delimiters = delim)     tokenizer.tri <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3), delimiters = delim)     tdm.uni <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.uni))     tdm.bi <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.bi))     tdm.tri <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.tri))
tdm <- TermDocumentMatrix(corpus.data, control = list(tokenize = NGramTokenizer))
tdm <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.uni))
findFreqTerms(tdm, lowfreq = 2)
corpus.data <- tm_map(corpus.data, PlainTextDocument)
delim <- " \\r\\n\\t.,;:\"()?!"     tokenizer.uni <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))     tokenizer.bi <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2), delimiters = delim)     tokenizer.tri <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3), delimiters = delim)     tdm.uni <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.uni))     tdm.bi <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.bi))     tdm.tri <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.tri))     freq.uni <- findFreqTerms(dtm.uni, 5,5)     freq.bi <- findFreqTerms(dtm.bi,  5,5)     freq.tri <- findFreqTerms(dtm.tri, 5, 5)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
freq.uni <- findFreqTerms(tdm.uni, 5,5)     freq.bi <- findFreqTerms(tdm.bi,  5,5)     freq.tri <- findFreqTerms(tdm.tri, 5, 5)
Uni_gram <- NGramTokenizer(corpus.data, Weka_control(min = 1, max = 1))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
delim <- " \\r\\n\\t.,;:\"()?!"     tokenizer.uni <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))     tokenizer.bi <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2), delimiters = delim)     tokenizer.tri <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3), delimiters = delim)     tdm.uni <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.uni))     tdm.bi <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.bi))     tdm.tri <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.tri))     freq.uni <- findFreqTerms(tdm.uni, 5,5)     freq.bi <- findFreqTerms(tdm.bi,  5,5)     freq.tri <- findFreqTerms(tdm.tri, 5, 5)
Uni_gram <- NGramTokenizer(corpus.data, Weka_control(min = 1, max = 1)) Unigram = data.frame(table(Uni_gram)) Unigram = Unigram[order(Unigram$Freq, decreasing = T),] Ug <- Unigram[1:40,]
p_load("stringi")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
Uni_gram <-  NGramTokenizer(corpus.data, Weka_control(min = 1, max = 1))
Uni_gram <-  NGramTokenizer(corpus.data, Weka_control(min = 1, max = 1))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
p_load("tm")
## create a UnigramTokenizer (RWeka) UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1)) ## create a BigramTokenizer (RWeka) BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2)) ## load the english documents en_texts <- VCorpus(DirSource(directory = "data/en_US/small", encoding = "UTF-8"),                               readerControl = list(language = "en")) ## get rid of extra white spaces, stopwords, DON'T STEM YET, switch to lowercase en_texts <- tm_map(x = en_texts, FUN = removePunctuation) en_texts <- tm_map(x = en_texts, FUN = removeWords, words = stopwords(kind = "en")) en_texts <- tm_map(x = en_texts, FUN = stripWhitespace) en_texts <- tm_map(x = en_texts, FUN = tolower) ## create a TermDocumentMatrix   ## NOTE - without the "options" underneath, the TermDocumentMatrix call crashes -  ## (looks like a parallel processing issue) options(mc.cores = 1) tdmUnigram <- TermDocumentMatrix(en_texts, control = list(tokenizer = UnigramTokenizer)) tdmBigram <- TermDocumentMatrix(en_texts, control = list(tokenizer = BigramTokenizer))
## create a UnigramTokenizer (RWeka) UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1)) ## create a BigramTokenizer (RWeka) BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2)) ## get rid of extra white spaces, stopwords, DON'T STEM YET, switch to lowercase corpus.data <- tm_map(x = corpus.data, FUN = removePunctuation) corpus.data <- tm_map(x = corpus.data, FUN = removeWords, words = stopwords(kind = "en")) corpus.data <- tm_map(x = corpus.data, FUN = stripWhitespace) corpus.data <- tm_map(x = corpus.data, FUN = tolower) ## create a TermDocumentMatrix   ## NOTE - without the "options" underneath, the TermDocumentMatrix call crashes -  ## (looks like a parallel processing issue) options(mc.cores = 1) tdmUnigram <- TermDocumentMatrix(corpus.data, control = list(tokenizer = UnigramTokenizer)) tdmBigram <- TermDocumentMatrix(corpus.data, control = list(tokenizer = BigramTokenizer))
lst <- list(sample.blogs, sample.news, sample.twitter)     corpus.data <- Corpus(VectorSource(lst))     toEmpty <- content_transformer(function(x, pattern) gsub(pattern, "", x, fixed = TRUE))     toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x, fixed = TRUE))     corpus.data <- tm_map(corpus.data, tolower)     corpus.data <- tm_map(corpus.data, removePunctuation)     corpus.data <- tm_map(corpus.data, removeNumbers)     corpus.data <- tm_map(corpus.data, removeWords, stopwords("english"))     corpus.data <- tm_map(corpus.data, stripWhitespace)     corpus.data <- tm_map(corpus.data, PlainTextDocument)     corpus.data <- tm_map(corpus.data, toEmpty, "#\\w+")       corpus.data <- tm_map(corpus.data, toEmpty, "(\\b\\S+\\@\\S+\\..{1,3}(\\s)?\\b)")       corpus.data <- tm_map(corpus.data, toEmpty, "@\\w+")       corpus.data <- tm_map(corpus.data, toEmpty, "http[^[:space:]]*")       corpus.data <- tm_map(corpus.data, toSpace, "/|@|\\|")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
p_load("RWeka")
install.packages("RWeka")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
options(mc.cores = 1) tdmUnigram <- TermDocumentMatrix(corpus.data, control = list(tokenizer = UnigramTokenizer)) tdmBigram <- TermDocumentMatrix(corpus.data, control = list(tokenizer = BigramTokenizer))
Uni_gram <- NGramTokenizer(Cleandata, Weka_control(min = 1, max = 1)) Unigram = data.frame(table(Uni_gram)) Unigram = Unigram[order(Unigram$Freq, decreasing = T),] Ug <- Unigram[1:40,]
Uni_gram <- NGramTokenizer(Cleandata, Weka_control(min = 1, max = 1))
?RWeka
delim <- " \\r\\n\\t.,;:\"()?!"     tokenizer.uni <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))     tokenizer.bi <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2), delimiters = delim)     tokenizer.tri <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3), delimiters = delim)     tdm.uni <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.uni))     tdm.bi <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.bi))     tdm.tri <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenizer.tri))     freq.uni <- findFreqTerms(tdm.uni, 5,5)     freq.bi <- findFreqTerms(tdm.bi,  5,5)     freq.tri <- findFreqTerms(tdm.tri, 5, 5)
barplot(tdm.uni, names.arg = freq.uni, cex.names = .7, col = heat.colors(40), main = c("Frequency of one word"), las = 2)
Uni_gram <- tokenizer.uni(corpus.data)
library(rJava)
p_load("rJava")
.jinit(parameters = "-Xmx128g")
options(mc.cores = 1)
Uni_gram <- tokenizer.uni(corpus.data)
delim <- " \\r\\n\\t.,;:\"()?!"     gram1Tokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 1, max = 1)) }     gram2Tokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2)) }     gram3Tokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3)) }     tdm1 <- TermDocumentMatrix(corpus, control = list(tokenize = gram1Tokenizer))     tdm2 <- TermDocumentMatrix(corpus, control = list(tokenize = gram2Tokenizer))     tdm3 <- TermDocumentMatrix(corpus, control = list(tokenize = gram3Tokenizer))     gram1freq <- data.frame(word = tdm1$dimnames$Terms, freq = rowSums(sparseMatrix(i = tdm1$i, j = tdm1$j, x = tdm1$v)))     gram1freq <- arrange(gram1freq, desc(freq))     gram2freq <- data.frame(word = tdm2$dimnames$Terms, freq = rowSums(sparseMatrix(i = tdm2$i, j = tdm2$j, x = tdm2$v)))     gram2freq <- arrange(gram2freq, desc(freq))     gram3freq <- data.frame(word = tdm3$dimnames$Terms, freq = rowSums(sparseMatrix(i = tdm3$i, j = tdm3$j, x = tdm3$v)))     gram3freq <- arrange(gram3freq, desc(freq))
delim <- " \\r\\n\\t.,;:\"()?!"     gram1Tokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 1, max = 1)) }     gram2Tokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2)) }     gram3Tokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3)) }     tdm1 <- TermDocumentMatrix(corpus.data, control = list(tokenize = gram1Tokenizer))     tdm2 <- TermDocumentMatrix(corpus.data, control = list(tokenize = gram2Tokenizer))     stdm3 <- TermDocumentMatrix(corpus.data, control = list(tokenize = gram3Tokenizer))     gram1freq <- data.frame(word = tdm1$dimnames$Terms, freq = rowSums(sparseMatrix(i = tdm1$i, j = tdm1$j, x = tdm1$v)))     gram1freq <- arrange(gram1freq, desc(freq))     gram2freq <- data.frame(word = tdm2$dimnames$Terms, freq = rowSums(sparseMatrix(i = tdm2$i, j = tdm2$j, x = tdm2$v)))     gram2freq <- arrange(gram2freq, desc(freq))     gram3freq <- data.frame(word = tdm3$dimnames$Terms, freq = rowSums(sparseMatrix(i = tdm3$i, j = tdm3$j, x = tdm3$v)))     gram3freq <- arrange(gram3freq, desc(freq))
p_load("Matrix")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
install.packages("rJava", dependencies = TRUE)
install.packages("RWeka", dependencies = TRUE)
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
data.frame(gram1 = gram1freq$word[1:10], gram2 = gram2freq$word[1:10], gram3 = gram3freq$word[1:10])
data.graph <-    data.frame(gram1 = gram1freq$word[1:10], gram2 = gram2freq$word[1:10], gram3 = gram3freq$word[1:10])
par(mfrow=c(1,1))     wordcloud(data.graph$gram1, scale = c(9, 1), max.words = 40, random.order = FALSE, colors = brewer.pal(7, "Dark2"))
wordcloud(data.graph$gram2, scale = c(9, 1), max.words = 40, random.order = FALSE, colors = brewer.pal(7, "Dark2"))
wordcloud(data.graph$gram3, scale = c(9, 1), max.words = 40, random.order = FALSE, colors = brewer.pal(7, "Dark2"))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
g1 <- ggplot(data.graph[0], aes(x = reorder(data.graph[0]$word, data.graph[0]$freq), y = data.graph[0]$freq)) +     geom_bar(stat = "identity", fill = "red") +     ggtitle("1-gram") +     xlab("1-grams") + ylab("Frequency")
g1
g1 <- ggplot(data.graph[0], aes(x = word, y = freq)) +     geom_bar(stat = "identity", fill = "red") +     ggtitle("1-gram") +     xlab("1-grams") + ylab("Frequency") g1
data.graph[0] <- data.frame(word = unlist(word))
data.graph[0]
data.graph$gram1
gram1freq <= sqldf("select * from gram1freq where freq > 4000")
gram1freq <- sqldf("select * from gram1freq where freq > 4000")
g1 <- ggplot(gram1freq, aes(x = word, y = freq)) +     geom_bar(stat = "identity", fill = "red") +     ggtitle("1-gram") +     xlab("1-grams") + ylab("Frequency")
g1
gram2freq <- sqldf("select * from gram2freq where freq > 4000 order by freq desc")
g2 <- ggplot(gram2freq, aes(x = word, y = freq)) +     geom_bar(stat = "identity", fill = "green") +     ggtitle("2-gram") +     xlab("2-grams") + ylab("Frequency") g1
g2 <- ggplot(gram2freq, aes(x = word, y = freq)) +     geom_bar(stat = "identity", fill = "green") +     ggtitle("2-gram") +     xlab("2-grams") + ylab("Frequency") g2
g1 <- ggplot(gram1freq, aes(x = word, y = freq)) +     geom_bar(stat = "identity", fill = "red") +     ggtitle("1-gram") +     xlab("1-grams") + ylab("Frequency") g1
gram3freq <- sqldf("select * from gram3freq where freq > 4000 order by freq desc")
g3 <- ggplot(gram3freq, aes(x = word, y = freq)) +     geom_bar(stat = "identity", fill = "blue") +     ggtitle("3-gram") +     xlab("3-grams") + ylab("Frequency") g3
gram3freq <- data.frame(word = tdm3$dimnames$Terms, freq = rowSums(sparseMatrix(i = tdm3$i, j = tdm3$j, x = tdm3$v)))     gram3freq <- sqldf("select * from gram3freq where freq > 4000 order by freq desc")
g3 <- ggplot(gram3freq, aes(x = word, y = freq)) +     geom_bar(stat = "identity", fill = "blue") +     ggtitle("3-gram") +     xlab("3-grams") + ylab("Frequency") g3
wordcloud(names(gram1freq), gram1freq, min.freq = 25, max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
wordcloud(gram1freq@names, gram1freq, min.freq = 25, max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
wordcloud(gram1freq@word, gram1freq@freq, min.freq = 25, max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
wordcloud(gram1freq@word, gram1freq@freq,  max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
wordcloud(gram1freq@word, max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
wordcloud(gram1freq@word, gram1freq@freq,  max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
wordcloud(gram1freq$word, gram1freq$freq,  max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Dark2"))
wordcloud(gram2freq$word, gram2freq$freq, max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "yellow"))
wordcloud(gram1freq$word, gram1freq$freq,  max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "red"))
wordcloud(gram1freq$word, gram1freq$freq,  max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Accent"))
wordcloud(gram1freq$word, gram1freq$freq,  max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Set1"))
wordcloud(gram2freq$word, gram2freq$freq, max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Set2"))
wordcloud(gram3freq$word, gram3freq$freq, max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Seet3"))
wordcloud(gram3freq$word, gram3freq$freq, max.words = 200, random.order = FALSE, rot.per = 0.35, colors = brewer.pal(8, "Set3"))
suppressMessages(setwd("c:/dev/r-course/10-capstone/"))
covert_rmd_to_r <- function() {     library(knitr)     purl("milestone-report.Rmd") }
covert_rmd_to_r()
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
f1 <- findFreqTerms(tdm3)
f1 <- findFreqTerms(tdm3, 5)
freq.train <- dtm.train[, f1] freq.test <- dtm.test[, f1]
t1 <- (tdm3$nrow * .7)
dtm.train <- tdm3[1:t1,0]
count.train <- (tdm3$nrow * .7) # 70 % count.test <- tdm3$nrow-count.train
count.train <- (tdm3$nrow * .7) # 70 % count.test <- tdm3$nrow-count.train dtm.train <- tdm3[1:count.train, ] dtm.test <- tdm3[1:count.test, ]
freq.train <- dtm.train[, f1] freq.test <- dtm.test[, f1]
f1 <- findFreqTerms(tdm3, 5) count.train <- (tdm3$nrow * .7) # 70 % dtm.train <- tdm3[1:count.train, ] dtm.test <- tdm3[count.train:tdm3$nrow,]
f1 <- findFreqTerms(tdm3, 5) count.train <- (tdm3$nrow * .7) # 70 % dtm.train <- tdm3[1:count.train, ] dtm.test <- tdm3[count.train:tdm3$nrow,] dtm.train.labels <- tdm3[1:count.train,] dtm.test.labels <- tdm3[count.train:tdm3$nrow,] freq.train <- dtm.train[, f1] freq.test <- dtm.test[, f1]
dtm.test.labels <- tdm3[count.train:tdm3$nrow,]
dtm.train.labels <- tdm3[1:count.train,]
using("SparseM")
dtm_matrix = as.matrix.csr(as.matrix(dtm))
dtm_matrix = as.matrix.csr(as.matrix(tdm3))
svm_model <- svm(dtm_matrix, classvec, kernel = "linear")
using("enc2utf8")
dtm_matrix = as.matrix.csr(as.matrix(tdm3))
dtm.train.labels <- tdm3[1:count.train,]$dimnames$Terms
 # convert dtm to dtm_matrix using sparse storage dtm_matrix = as.matrix.csr(as.matrix(tdm3)) #specify the features, vector to be predicted, and kernel method in the svm model svm_model <- svm(dtm_matrix, dtm.train.labels, kernel = "linear") summary(svm_model) #inspect results pred <- predict(svm_model, dtm) table(pred, classvec)
# convert dtm to dtm_matrix using sparse storage dtm_matrix = as.matrix(dtm.train) #specify the features, vector to be predicted, and kernel method in the svm model svm_model <- svm(dtm_matrix, dtm.train.labels, kernel = "linear") summary(svm_model) #inspect results pred <- predict(svm_model, dtm) table(pred, classvec)
dtm_matrix = as.matrix(dtm.train)
svm_model <- svm(dtm_matrix, dtm.train.labels, kernel = "linear")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
update.packages("tm", checkBuilt = TRUE)
install.packages("filehash", lib="C:/Users/chris/Documents/R/win-library/3.4")
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
corpus.data <- PCorpus(VectorSource(group_texts), dbControl = list(dbName = "pcorpus.db", dbType = "DB1")))
corpus.data <- PCorpus(VectorSource(group_texts), dbControl = list(dbName = "pcorpus.db", dbType = "DB1"))
source("C:/dev/r-course/10-capstone/milestone-report.R", echo = TRUE, encoding = "Windows-1252")
using("caret")
using("caret") # Convert to a data.frame for training and assign a classification (factor) to each document. train <- as.matrix(tdm3) train <- cbind(train, c(0, 1)) colnames(train)[ncol(train)] <- 'y' train <- as.data.frame(train) train$y <- as.factor(train$y) # Train. fit <- train(y ~ ., data = train, method = 'bayesglm') # Check accuracy on training. predict(fit, newdata = train) # Test data. data2 <- c('Bats eat bugs.') corpus <- VCorpus(VectorSource(data2)) tdm <- DocumentTermMatrix(corpus, control = list(dictionary = Terms(tdm), removePunctuation = TRUE, stopwords = TRUE, stemming = TRUE, removeNumbers = TRUE)) test <- as.matrix(tdm) # Check accuracy on test. predict(fit, newdata = test)
 # Convert to a data.frame for training and assign a classification (factor) to each document. train <- as.matrix(tdm3) train <- cbind(train, c(0, 1)) colnames(train)[ncol(train)] <- 'y' train <- as.data.frame(train) train$y <- as.factor(train$y)
fit <- train(y ~ ., data = train, method = 'bayesglm')
predict(fit, newdata = train)
train <- as.matrix(dtm.train) train <- cbind(train, c(0, 1)) colnames(train)[ncol(train)] <- 'y' train <- as.data.frame(train) train$y <- as.factor(train$y)
using("ngram")
install.packages("ngram")
create_ngrams <- function(sentence_splits, ngram_size = 2) {     ngrams <- c()     for (sentence in sentence_splits) {         sentence <- Trim(sentence)         if ((nchar(sentence) > 0) && (sapply(gregexpr("\\W+", sentence), length) >= ngram_size)) {             ngs <- ngram(sentence, n = ngram_size)             ngrams <- c(ngrams, get.ngrams(ngs))         }     }     return(ngrams) }
n2 <- create_ngrams(corpus.data, ngram_size = 2) n3 <- create_ngrams(corpus.data, ngram_size = 3) n4 <- create_ngrams(corpus.data, ngram_size = 4) n5 <- create_ngrams(corpus.data, ngram_size = 5)
create_ngrams <- function(sentence_splits, ngram_size = 2) {     ngrams <- c()     for (sentence in sentence_splits) {         sentence <- trim(sentence)         if ((nchar(sentence) > 0) && (sapply(gregexpr("\\W+", sentence), length) >= ngram_size)) {             ngs <- ngram(sentence, n = ngram_size)             ngrams <- c(ngrams, get.ngrams(ngs))         }     }     return(ngrams) } n2 <- create_ngrams(corpus.data, ngram_size = 2) n3 <- create_ngrams(corpus.data, ngram_size = 3) n4 <- create_ngrams(corpus.data, ngram_size = 4) n5 <- create_ngrams(corpus.data, ngram_size = 5)
trim <- function(x) {     # http://stackoverflow.com/questions/2261079/how-to-trim-leading-and-trailing-whitespace-in-r     gsub("(^[[:space:]]+|[[:space:]]+$)", "", x) }
n2 <- create_ngrams(corpus.data, ngram_size = 2) n3 <- create_ngrams(corpus.data, ngram_size = 3) n4 <- create_ngrams(corpus.data, ngram_size = 4) n5 <- create_ngrams(corpus.data, ngram_size = 5)
create_ngrams <- function(sentence_splits, ngram_size = 2) {     ngrams <- c()     for (sentence in sentence_splits) {         sentence <- trim(sentence)         if ((nchar(sentence) > 0) && (sapply(gregexpr("\\W+", sentence), length) >= ngram_size)) {             ngs <- ngram(sentence, n = ngram_size)             ngrams <- c(ngrams, get.ngrams(ngs))         }     }     return(ngrams) } n2 <- create_ngrams(corpus.data, ngram_size = 2) n3 <- create_ngrams(corpus.data, ngram_size = 3) n4 <- create_ngrams(corpus.data, ngram_size = 4) n5 <- create_ngrams(corpus.data, ngram_size = 5)
using("foreach")
create_ngrams <- function(sentence_splits, ngram_size = 2) {     ngrams <- c()     for (sentence in sentence_splits) {         sentence <- trim(sentence)         if ((nchar(sentence) > 0) && (sapply(gregexpr("\\W+", sentence), length) >= ngram_size)) {             ngs <- ngram(sentence, n = ngram_size)             ngrams <- c(ngrams, get.ngrams(ngs))         }     }     return(ngrams) } n2 <- create_ngrams(corpus.data, ngram_size = 2) n3 <- create_ngrams(corpus.data, ngram_size = 3) n4 <- create_ngrams(corpus.data, ngram_size = 4) n5 <- create_ngrams(corpus.data, ngram_size = 5)
n2 <- create_ngrams(sample.all, ngram_size = 2) n3 <- create_ngrams(sample.all, ngram_size = 3) n4 <- create_ngrams(sample.all, ngram_size = 4) n5 <- create_ngrams(sample.all, ngram_size = 5)
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
using("ANLP")
install.packages("ANLP")
install.packages("ANLP")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
n2 <- buildNgramModel(2)
n2()
n2(data.blogs)
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
n2 <- buildNgramModel(2) tdm2 <- n2(data.all)
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
tdm <- generateTDM(data.all,3, N, isTrace = F)
tdm <- generateTDM(data.all,3,T)
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
tdm2 <- n2(sample.all)
class(tdm2)
n3 <- buildNgramModel(3)
tdm2 <- n3(sample.all)
tdm2 <- n2sample.all) tdm3 <- n3(sample.all)
tdm2 <- n2(sample.all) tdm3 <- n3(sample.all)
tdm <- generateTDM(sample.all)
tdm2 <- generateTDM(sample.all, 2, T)
test <- "If this isn't the cutest thing you've ever seen, then you must be"
lst <- c(tdm2,tdm3)
tdm2 <- generateTDM(sample.all, 2, T) tdm3 <- generateTDM(sample.all, 3, T) lst <- c(tdm2,tdm3) test <- "If this isn't the cutest thing you've ever seen, then you must be" predict_Backoff(test,lst,T)
lst <- as.vector(c(tdm2,tdm3))
predict_Backoff(test,lst,T)
test <- cleanTextData( "If this isn't the cutest thing you've ever seen, then you must be") predict_Backoff(test,lst,T)
lst <- as.list(c(tdm2,tdm3))
test <- cleanTextData( "If this isn't the cutest thing you've ever seen, then you must be") predict_Backoff(test,lst,T)
predict_Backoff(test,tdm3,T)
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
predict_Backoff(test,lst,T)
test <- cleanTextData( "If this isn't the cutest thing you've ever seen, then you must be") predict_Backoff(test,lst,T)
detach(package:neuralnet)
sessionInfo()
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
require(ngram)
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
# notice the trailing space at end to avoid picking last word word <- 'bacon ' matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(paste0('\\<', word), sentence)) {         print(sentence)         matches <- c(matches, sentence)     } } # find highest probability word precision_match <- c() for (a_match in matches) {     # how many spaces in from of search word     precision_match <- c(precision_match, nchar(strsplit(x = a_match, split = word)[[1]][[1]])) } # use highest number and a random of highest for multiples best_matched_sentence <- sample(matches[precision_match == max(precision_match)], size = 1) print(best_matched_sentence)
best_matched_sentence <- sample(matches[precision_match == max(precision_match)], size = 1)
# notice the trailing space at end to avoid picking last word word <- "If this isn' t the cutest thing you 've ever seen, then you must be " matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(paste0('\\<', word), sentence)) {         print(sentence)         matches <- c(matches, sentence)     } } # find highest probability word precision_match <- c() for (a_match in matches) {     # how many spaces in from of search word     precision_match <- c(precision_match, nchar(strsplit(x = a_match, split = word)[[1]][[1]])) } # use highest number and a random of highest for multiples best_matched_sentence <- sample(matches[precision_match == max(precision_match)], size = 1) print(best_matched_sentence)
# notice the trailing space at end to avoid picking last word word <- "then you must be " matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(paste0('\\<', word), sentence)) {         print(sentence)         matches <- c(matches, sentence)     } } # find highest probability word precision_match <- c() for (a_match in matches) {     # how many spaces in from of search word     precision_match <- c(precision_match, nchar(strsplit(x = a_match, split = word)[[1]][[1]])) } # use highest number and a random of highest for multiples best_matched_sentence <- sample(matches[precision_match == max(precision_match)], size = 1) print(best_matched_sentence)
matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(paste0('\\<', word), sentence)) {         print(sentence)         matches <- c(matches, sentence)     } }
word <- "must be " matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(paste0('\\<', word), sentence)) {         print(sentence)         matches <- c(matches, sentence)     } }
word <- "must be " matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(paste0('\\<', word), sentence)) {         print(sentence)         matches <- c(matches, sentence)     } }
# notice the trailing space at end to avoid picking last word word <- "must be " matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(word, sentence)) {         print(sentence)         matches <- c(matches, sentence)     } }
word <- "must be" matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(word, sentence)) {         print(sentence)         matches <- c(matches, sentence)     } }
word <- "dust them off and be on my"
matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(word, sentence)) {         print(sentence)         matches <- c(matches, sentence)     } }
# find highest probability word precision_match <- c() for (a_match in matches) {     # how many spaces in from of search word     precision_match <- c(precision_match, nchar(strsplit(x = a_match, split = word)[[1]][[1]])) } # use highest number and a random of highest for multiples best_matched_sentence <- sample(matches[precision_match == max(precision_match)], size = 1)
word <- "be on my " matches <- c() for (sentence in n_all) {     # find exact match with double backslash and escape     if (grepl(word, sentence)) {         print(sentence)         matches <- c(matches, sentence)     } } # find highest probability word precision_match <- c() for (a_match in matches) {     # how many spaces in from of search word     precision_match <- c(precision_match, nchar(strsplit(x = a_match, split = word)[[1]][[1]])) } # use highest number and a random of highest for multiples best_matched_sentence <- sample(matches[precision_match == max(precision_match)], size = 1)
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
library(stringr)
 predictNgrams <- function(input) {     ## clean input text     # remove numbers, punctuations     word <- gsub("[^a-zA-Z\n\']", " ", input)     # convert all words to lowercase     word <- tolower(word)     # remove extra spaces     trim <- function(x) return(gsub("^ *|(?<= ) | *$", "", x, perl = T))     word <- trim(word)     str <- unlist(str_split(word, " "))     len <- length(str)     predict <- c()     if (len >= 3) {         ##trigram          W1 <- str[len - 2];         W2 <- str[len - 1];         W3 <- str[len]         ngram <- n3[list(W1, W2, W3)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##bigram             ngram <- n3[list(W2, W3)]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }         if (length(predict) < 6) {             ##unigram             ngram <- n2[list(W3)]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 2) {         W1 <- str[len - 1];         W2 <- str[len]         ngram <- n3[list(W1, W2)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##unigram             ngram <- n2[list(W2)]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 1) {         W1 <- str[len]         ngram <- n2[list(W1)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)     }     predict <- predict[!is.na(predict)]     if (length(predict) < 5) {         predict <- c(predict, Unigrams_model$pred)     }     return(predict[1:5]) }
predictNgrams(word)
  ngram <- n3[[list(W2, W3)]]
predictNgrams <- function(input) {     ## clean input text     # remove numbers, punctuations     word <- gsub("[^a-zA-Z\n\']", " ", input)     # convert all words to lowercase     word <- tolower(word)     # remove extra spaces     trim <- function(x) return(gsub("^ *|(?<= ) | *$", "", x, perl = T))     word <- trim(word)     str <- unlist(str_split(word, " "))     len <- length(str)     predict <- c()     if (len >= 3) {         ##trigram          W1 <- str[len - 2];         W2 <- str[len - 1];         W3 <- str[len]         ngram <- n3[list(W1, W2, W3)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##bigram             ngram <- n3[[list(W2, W3)]]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }         if (length(predict) < 6) {             ##unigram             ngram <- n2[[list(W2, W3)]]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 2) {         W1 <- str[len - 1];         W2 <- str[len]         ngram <- n3[list(W1, W2)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##unigram             ngram <- n2[list(W2)]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 1) {         W1 <- str[len]         ngram <- n2[list(W1)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)     }     predict <- predict[!is.na(predict)]     if (length(predict) < 5) {         predict <- c(predict, Unigrams_model$pred)     }     return(predict[1:5]) }
predictNgrams(word)
n2()$predict
n2[]$predict
n2$predict
predictNgrams(word)
predictNgrams <- function(input) {     ## clean input text     # remove numbers, punctuations     word <- gsub("[^a-zA-Z\n\']", " ", input)     # convert all words to lowercase     word <- tolower(word)     # remove extra spaces     trim <- function(x) return(gsub("^ *|(?<= ) | *$", "", x, perl = T))     word <- trim(word)     str <- unlist(str_split(word, " "))     len <- length(str)     predict <- c()     if (len >= 3) {         ##trigram          W1 <- str[len - 2];         W2 <- str[len - 1];         W3 <- str[len]         ngram <- n3[W1, W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##bigram             ngram <- n3[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }         if (length(predict) < 6) {             ##unigram             ngram <- n2[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 2) {         W1 <- str[len - 1];         W2 <- str[len]         ngram <- n2[W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##unigram             ngram <- n2[list(W2)]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 1) {         W1 <- str[len]         ngram <- n2[list(W1)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)     }     predict <- predict[!is.na(predict)]     if (length(predict) < 5) {         predict <- c(predict, Unigrams_model$pred)     }     return(predict[1:5]) }
predictNgrams(word)
 predictNgrams <- function(input) {     ## clean input text     # remove numbers, punctuations     word <- gsub("[^a-zA-Z\n\']", " ", input)     # convert all words to lowercase     word <- tolower(word)     # remove extra spaces     trim <- function(x) return(gsub("^ *|(?<= ) | *$", "", x, perl = T))     word <- trim(word)     str <- unlist(str_split(word, " "))     len <- length(str)     predict <- c()     if (len > 3) {         ##trigram          W1 <- str[len - 2];         W2 <- str[len - 1];         W3 <- str[len]         ngram <- n3[W1, W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##bigram             ngram <- n3[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }         if (length(predict) < 6) {             ##unigram             ngram <- n2[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 2) {         W1 <- str[len - 1];         W2 <- str[len]         ngram <- n2[W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##unigram             ngram <- n2[list(W2)]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 1) {         W1 <- str[len]         ngram <- n2[list(W1)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)     }     predict <- predict[!is.na(predict)]     if (length(predict) < 5) {         predict <- c(predict, Unigrams_model$pred)     }     return(predict[1:5]) } predictNgrams(word)
n1 <- buildNgramModel(1) 
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
doit()
n1()
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
predictNgrams <- function(input) {     ## clean input text     # remove numbers, punctuations     word <- gsub("[^a-zA-Z\n\']", " ", input)     # convert all words to lowercase     word <- tolower(word)     # remove extra spaces     trim <- function(x) return(gsub("^ *|(?<= ) | *$", "", x, perl = T))     word <- trim(word)     str <- unlist(str_split(word, " "))     len <- length(str)     predict <- c()     if (len > 3) {         ##trigram          W1 <- str[len - 2];         W2 <- str[len - 1];         W3 <- str[len]         ngram <- n3[W1, W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##bigram             ngram <- n3[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }         if (length(predict) < 6) {             ##unigram             ngram <- n1[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 2) {         W1 <- str[len - 1];         W2 <- str[len]         ngram <- n2[W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##unigram             ngram <- n1[list(W2)]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 1) {         W1 <- str[len]         ngram <- n2[list(W1)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)     }     predict <- predict[!is.na(predict)]     if (length(predict) < 5) {         predict <- c(predict, n1$pred)     }     return(predict[1:5]) } predictNgrams(word)
  predict <- predict[!is.na(predict)]
    if (length(predict) < 5) {         predict <- c(predict, n1$pred)     }
predictNgrams <- function(input) {     ## clean input text     # remove numbers, punctuations     word <- gsub("[^a-zA-Z\n\']", " ", input)     # convert all words to lowercase     word <- tolower(word)     # remove extra spaces     trim <- function(x) return(gsub("^ *|(?<= ) | *$", "", x, perl = T))     word <- trim(word)     str <- unlist(str_split(word, " "))     len <- length(str)     predict <- c()     if (len > 3) {         ##trigram          W1 <- str[len - 2];         W2 <- str[len - 1];         W3 <- str[len]         ngram <- n3[W1, W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##bigram             ngram <- n3[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }         if (length(predict) < 6) {             ##unigram             ngram <- n1[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 2) {         W1 <- str[len - 1];         W2 <- str[len]         ngram <- n2[W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##unigram             ngram <- n1[list(W2)]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 1) {         W1 <- str[len]         ngram <- n2[list(W1)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)     }   #  predict <- predict[!is.na(predict)]     if (length(predict) < 5) {         predict <- c(predict, n1$pred)     }     return(predict[1:5]) } predictNgrams(word)
predictNgrams <- function(input) {     ## clean input text     # remove numbers, punctuations     word <- gsub("[^a-zA-Z\n\']", " ", input)     # convert all words to lowercase     word <- tolower(word)     # remove extra spaces     trim <- function(x) return(gsub("^ *|(?<= ) | *$", "", x, perl = T))     word <- trim(word)     str <- unlist(str_split(word, " "))     len <- length(str)     predict <- c()     if (len > 3) {         ##trigram          W1 <- str[len - 2];         W2 <- str[len - 1];         W3 <- str[len]         ngram <- n3[W1, W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##bigram             ngram <- n3[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }         if (length(predict) < 6) {             ##unigram             ngram <- n1[W2, W3]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 2) {         W1 <- str[len - 1];         W2 <- str[len]         ngram <- n2[W2, W3]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)         if (length(predict) < 6) {             ##unigram             ngram <- n1[list(W2)]             predict <- c(predict, head(ngram[order(ngram$freq, decreasing = T),]$pred))         }     } else if (len == 1) {         W1 <- str[len]         ngram <- n2[list(W1)]         predict <- head(ngram[order(ngram$freq, decreasing = T),]$pred)     }   #  predict <- predict[!is.na(predict)]     if (length(predict) < 5) {         predict <- c(predict, n1)     }     return(predict[1:5]) } predictNgrams(word)
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/quiz2.R", echo = TRUE, encoding = "Windows-1252")
class(n2)
tdm2 <- generateTDM(sample.all, 2, T) tdm3 <- generateTDM(sample.all, 3, T)
tdm2 <- generateTDM(n_all, 2, T) tdm3 <- generateTDM(n_all, 3, T)
tdm2 <- generateTDM(sample.all, 2, T)
tokenize.ngram <- function(x, min.gram, max.gram) {     NGramTokenizer(x, Weka_control(min = min.gram, max = min.gram)) } tdm1 <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenize.ngram(1))) tdm2 <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenize.ngram(2))) tdm3 <- TermDocumentMatrix(corpus.data, control = list(tokenize = tokenize.ngram(3)))
source("C:/dev/r-course/10-capstone/project_1__setup.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
group_texts <- list(sample.blogs, sample.news, sample.twitter) group_texts <- tolower(group_texts) group_texts <- removeNumbers(group_texts) group_texts <- removePunctuation(group_texts, preserve_intra_word_dashes = TRUE) group_texts <- gsub("http[[:alnum:]]*", "", group_texts) group_texts <- stripWhitespace(group_texts) group_texts <- gsub("\u0092", "'", group_texts) group_texts <- gsub("\u0093|\u0094", "", group_texts)
group_texts <- tm_map(group_texts, qdap::clean) group_texts <- tm_map(group_texts, qdap::scrubber) group_texts <- tm_map(group_texts, qdap::replace_symbol)
group_texts <- sent_detect_nlp(group_texts)
corpus.data <- PCorpus(VectorSource(group_texts), dbControl = list(dbName = "pcorpus.db", dbType = "DB1")) toEmpty <- content_transformer(function(x, pattern) gsub(pattern, "", x, fixed = TRUE)) toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x, fixed = TRUE)) corpus.data <- tm_map(corpus.data, toEmpty, "#\\w+") corpus.data <- tm_map(corpus.data, toEmpty, "(\\b\\S+\\@\\S+\\..{1,3}(\\s)?\\b)") corpus.data <- tm_map(corpus.data, toEmpty, "@\\w+") corpus.data <- tm_map(corpus.data, toEmpty, "http[^[:space:]]*") corpus.data <- tm_map(corpus.data, toSpace, "/|@|\\|")
save_file("https://goo.gl/To9w5B", "bad_word_list.txt") bad_words <- readLines("./bad_word_list.txt") corpus.data <- tm_map(corpus.data, removeWords, bad_words) corpus.data <- tm_map(corpus.data, stemDocument)
source('c:/dev/r-course/10-capstone/project_4__ngram_generation.r')
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_cleanup.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_cleanup.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_cleanup.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_cleanup.r", echo = TRUE, encoding = "Windows-1252")
library(tm)
using("filehashSQLite")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
   db <- dbInit("pcorpus")
 db <- dbInit(database_name)
group_texts <- removeNumbers(group_texts)     group_texts <- removePunctuation(group_texts, preserve_intra_word_dashes = TRUE)     group_texts <- gsub("http[[:alnum:]]*", "", group_texts)     group_texts <- stripWhitespace(group_texts)     group_texts <- gsub("\u0092", "'", group_texts)     group_texts <- gsub("\u0093|\u0094", "", group_texts)     corpus.data <- PCorpus(VectorSource(group_texts), dbControl = list(dbName = database_name, dbType = "SQLite"))     toEmpty <- content_transformer(function(x, pattern) gsub(pattern, "", x, fixed = TRUE))     toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x, fixed = TRUE))     corpus.data <- tm_map(corpus.data, toEmpty, "#\\w+")     corpus.data <- tm_map(corpus.data, toEmpty, "(\\b\\S+\\@\\S+\\..{1,3}(\\s)?\\b)")     corpus.data <- tm_map(corpus.data, toEmpty, "@\\w+")     corpus.data <- tm_map(corpus.data, toEmpty, "http[^[:space:]]*")     corpus.data <- tm_map(corpus.data, toSpace, "/|@|\\|")     save_file("https://goo.gl/To9w5B", "bad_word_list.txt")     bad_words <- readLines("./bad_word_list.txt")     corpus.data <- tm_map(corpus.data, removeWords, bad_words)     corpus.data <- tm_map(corpus.data, stemDocument)     writeCorpus(corpus.data,path = ".", filenames = database_name)
 group_texts <- list(sample.blogs, sample.news, sample.twitter)     group_texts <- sent_detect_nlp(group_texts)
    dbCreate(database_name, "SQLite")     db <- dbInit(database_name, "SQLite")
database_name <- "pcorpus.db"
    dbCreate(database_name, "SQLite")     db <- dbInit(database_name, "SQLite")
    corpus.data <- PCorpus(VectorSource(group_texts), dbControl = list(dbName = database_name, dbType = "SQLite"))
    corpus.data <- PCorpus(VectorSource(group_texts), dbControl = list(dbName = database_name, dbType = "SQLite"))
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
 corpus.data <- PCorpus(VectorSource(group_texts), dbControl = list(dbName = database_name, dbType = "DB1"))
db <- dbInit(database_name)     corpus.data <- dbLoad(db)
    db <- dbInit(database_name)     corpus.data <- dbLoad(db)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
 db <- dbInit(db = database_name)
  db <- dbInit(db = database_file_path)
database_name <- "sqldb_pcorpus_mydata"
db <- dbInit(db = database_file_path)
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
    corpus.data <- PCorpus(VectorSource(group_texts), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))     dbCreate(database_name, "SQLite")     db <- dbInit(database_name, "SQLite")
  db <- dbInit(database_name, "SQLite")
 db.ListTables()
corpus.data.ListTables()
class(db)
dbGetDBIVersion()
db.dbGetDBIVersion()
db.GetDBIVersion()
dbListTables(db)
show(db)
dbFetch(db, "900")
 corpus.data <- dbInit(database_name, "SQLite")
    corpus.data <- dbInit(db = database_name)     corpus.data <- dbLoad(db)     corpus.data <- dbInit(database_name, "SQLite")     class(corpus.data)
>>>>>>> 5db9279125fa37c48a4b1df208d6738e100916ff
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
 cat("database--> creation started...")     group_texts <- list(sample.blogs, sample.news, sample.twitter)     group_texts <- sent_detect_nlp(group_texts)     group_texts <- tolower(group_texts)     group_texts <- removeNumbers(group_texts)     group_texts <- removePunctuation(group_texts, preserve_intra_word_dashes = TRUE)     group_texts <- gsub("http[[:alnum:]]*", "", group_texts)     group_texts <- stripWhitespace(group_texts)     group_texts <- gsub("\u0092", "'", group_texts)     group_texts <- gsub("\u0093|\u0094", "", group_texts)
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
group_texts <- data.all
    group_texts <- sent_detect_nlp(group_texts)
object.size(data.all)
source("C:/dev/r-course/10-capstone/project_2__data_retrieval.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_2__data_retrieval.r", echo = TRUE, encoding = "Windows-1252")
is.na(data.all)
length(data.all) == 0
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
dbDisconnect()
dbDisconnect(db)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
file.remove("C:\dev\r-course\10-capstone\sqldb_pcorpus_mydata")
file.remove("sqldb_pcorpus_mydata")
suppressMessages(setwd("c:/dev/r-course/10-capstone/"))
file.remove("sqldb_pcorpus_mydata")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
db <- dbInit(database_name, "SQLite")
dbListTables(db)
dbInsert(db, "test", "hi there")
dbFetch(db, "900")
dbFetch(db)
dbFetch(db, "5")
dbFetch(db, "-1")
dbGetInfo(db)
  corpus.data <- dbLoad(db)
using("MicrosoftML")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
corpus.data <- PCorpus(DataframeSource(dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
corpus.data[[1]]
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
dbDisconnect(db)
db <- dbInit(database_name, "SQLite")
dbDisconnect(db)
corpus.data <- PCorpus(VectorSource(dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
db <- dbInit(database_name, "SQLite")
    corpus.data <- PCorpus(VectorSource(dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
corpus.data <- PCorpus(dbLoad(db), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
rm(`1037`, envir = as.environment(".GlobalEnv"))
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
corpus.data <- PCorpus(DataframeSource(data.all), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
corpus.data <- PCorpus(VectorSource(data.all), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
corpus.data <- PCorpus(DataframeSource(dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
db <- dbInit(database_name, "SQLite")
    corpus.data <- PCorpus(DataframeSource(dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
corpus.data <- PCorpus(VectorSource(dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
corpus.data <- PCorpus(VectorSource(dbLoad(db)), readerControl = list(language = "en"))
data.all <- as.data.frame(data.all)
source("C:/dev/r-course/10-capstone/project_3__data_load_and_clean.r", echo = TRUE, encoding = "Windows-1252")
db <- dbInit(database_name, "SQLite")
dbDisconnect(db)
dbDisconnect(db)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
db <- dbInit(database_name, "SQLite")
corpus.data <- PCorpus(DataframeSource(dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite",))
corpus.data <- dbLoad(db)
dbDisconnect(db)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
corpus.data <- PCorpus(VectorSource(data.all), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
 corpus.data <- PCorpus(VectorSource(data.all), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
dbDisconnect(db)
db <- dbInit(database_name, "SQLite")
dbDisconnect(db)
corpus.data <- PCorpus(VectorSource(data.all), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
warnings()
corpus.data2 <- dbLoad(db)
db <- dbInit(database_name, "SQLite")
corpus.data2 <- dbLoad(db)
 dbDisconnect(db)
identical(corpus.data, corpus.data2)
p_load("openNLP")
gc()
using("textreg")
using("textreg")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
data.all <- sent_detect_nlp(data.all)
 data.all <- tolower(data.all)     data.all <- removeNumbers(data.all)     data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)     data.all <- gsub("http[[:alnum:]]*", "", data.all)     data.all <- stripWhitespace(data.all)     data.all <- gsub("\u0092", "'", data.all)     data.all <- gsub("\u0093|\u0094", "", data.all)
data.all <- tolower(data.all)
    data.all <- removeNumbers(data.all)
    data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)
    data.all <- gsub("http[[:alnum:]]*", "", data.all)
    data.all <- stripWhitespace(data.all)
    data.all <- gsub("\u0092", "'", data.all)
    data.all <- gsub("\u0093|\u0094", "", data.all)
    data.all <- gsub("\u0093|\u0094", "", data.all)
corpus.data <- build.corpus(corpus.data, labeling, banned = bad_words, verbosity = 1, token.type = "word")
corpus.data <- build.corpus(data.all, banned = bad_words, verbosity = 1, token.type = "word")
corpus.data <- build.corpus(data.all,labeling = c(TRUE), banned = bad_words, verbosity = 1, token.type = "word")
corpus.data <- clean.text(corpus.data)
corpus.data
  corpus.data <- clean.text(corpus.data)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
corpus.original <- PCorpus(VectorSource(data.all), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite"))
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
save.corpus.to.files(corpus.original)
corpus.stemmed <- stem.corpus(convert.tm.to.character(corpus.original))
corpus.original <- build.corpus(data.all, labeling = c(TRUE), banned = bad_words, verbosity = 1, token.type = "word")
phrases(corpus.original)
corpus.original
is.textreg.corpus(corpus.original)
ccc <- textreg(corpus.stemmed,TRUE)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
    corpus.textreg <- textreg(data.all, labeling = c(TRUE), banned = bad_words, verbosity = 1, token.type = "word")
    corpus.stemmed <- stem.corpus(convert.tm.to.character(corpus.original))
corpus.data <- tm_map(corpus.data, removeWords, bad_words)     corpus.original <- clean.text(corpus.original)
warnings()
calc.loss(corpus.textreg)
ccc <- textreg(corpus.stemmed, c("news", "blogs", "tweets"))
corpus.result <- textreg(data.all, labeling = c(TRUE), banned = bad_words, verbosity = 1, token.type = "word")
print.textreg.corpus()
predict.textreg.result()
is.textreg.result(corpus.result)
make.appearance.matrix
(corpus.result)
make.appearance.matrix(corpus.result)
make.count.table(c("hello"),c(TRUE,FALSE),corpus.stemmed)
make.phrase.correlation.chart(corpus.result)
make.phrase.correlation.chart(corpus.result,FALSE,5,FALSE)
make.similarity.matrix(corpus.result)
phrase.count("I want a", corpus.original)
phrases(corpus.result)
make.phrase.correlation.chart(corpus.result, count = FALSE, num.groups = 5, use.corrplot = FALSE)
corpus.original <- PCorpus((dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite",))
db <- dbInit(database_name, "SQLite")
corpus.original <- PCorpus((dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite",))
corpus.original <- PCorpus(VectorSource(dbLoad(db)), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite",))
using("glue")
install.packages("glue")
corpus.original <- PCorpus(VectorSource(c(dbLoad(db))), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite",))
using("tau")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
 save_file("https://goo.gl/To9w5B", "bad_word_list.txt")     bad_words <- readLines("./bad_word_list.txt")     log("database--> creation started...")     data.all <- as.data.frame(data.all)     data.all <- sent_detect(data.all) #Detect and split sentences on endmark boundaries.     data.all <- tolower(data.all)     data.all <- removeNumbers(data.all)     data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)     data.all <- gsub("http[[:alnum:]]*", "", data.all)     data.all <- stripWhitespace(data.all)     data.all <- gsub("\u0092", "'", data.all)     data.all <- gsub("\u0093|\u0094", "", data.all)
corpus.original <- PCorpus(VectorSource(data.all), readerControl = list(language = "english"), dbControl = list(dbName = database_name, dbType = "SQLite"))
corpus.original <- suppressWarnings(PCorpus(VectorSource(c(dbLoad(db))), readerControl = list(language = "english"), dbControl = list(dbName = database_name, dbType = "SQLite")))
db <- dbInit(database_name, "SQLite")
    corpus.original <- suppressWarnings(PCorpus(VectorSource(c(dbLoad(db))), readerControl = list(language = "english"), dbControl = list(dbName = database_name, dbType = "SQLite")))
    rm(corpus.original)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
save_file("https://goo.gl/To9w5B", "bad_word_list.txt")     bad_words <- readLines("./bad_word_list.txt")     log("database--> creation started...")     data.all <- as.data.frame(data.all)     data.all <- sent_detect(data.all) #Detect and split sentences on endmark boundaries.
using("tm")
update.packages("tm")
save_file("https://goo.gl/To9w5B", "bad_word_list.txt")     bad_words <- readLines("./bad_word_list.txt")
    log("database--> creation started...")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
p_load("log4r")
using("futile.logger")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
db <- dbInit(database_name, "SQLite")
save_file("https://goo.gl/To9w5B", "bad_word_list.txt")     bad_words <- readLines("./bad_word_list.txt")     flog.info("database--> creation started...")     data.all <- as.data.frame(data.all)     data.all <- sent_detect(data.all) #Detect and split sentences on endmark boundaries.
   data.all <- tolower(data.all)     data.all <- removeNumbers(data.all)     data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)     data.all <- gsub("http[[:alnum:]]*", "", data.all)     data.all <- stripWhitespace(data.all)     data.all <- gsub("\u0092", "'", data.all)     data.all <- gsub("\u0093|\u0094", "", data.all)
    data.all <- tolower(data.all)
    data.all <- removeNumbers(data.all)
    data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)
    data.all <- gsub("http[[:alnum:]]*", "", data.all)
    data.all <- stripWhitespace(data.all)
    data.all <- gsub("\u0092", "'", data.all)
    data.all <- gsub("\u0093|\u0094", "", data.all)
    suppressWarnings(corpus.original <- PCorpus(VectorSource(data.all), readerControl = list(language = "en"), dbControl = list(dbName = database_name, dbType = "SQLite")))
    toEmpty <- content_transformer(function(x, pattern) gsub(pattern, "", x, fixed = TRUE))
    toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x, fixed = TRUE))
    corpus.original <- tm_map(corpus.original, toEmpty, "#\\w+")
    corpus.original <- tm_map(corpus.original, toEmpty, "(\\b\\S+\\@\\S+\\..{1,3}(\\s)?\\b)")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "Windows-1252")
if (!file.exists("data/bad_words.RDS")) {     save_file("https://goo.gl/To9w5B", "bad_word_list.txt")     bad_words <- readLines("./bad_word_list.txt")     saveRDS("data/bad_words.RDS") } load("data/bad_words.RDS")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "Windows-1252")
    saveRDS(bad_words,"data/bad_words.RDS")
saveRDS(bad_words,"./data/bad_words.RDS")
saveRDS(bad_words, get_data_file_path("bad_words.RDS"))
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
load(get_data_file_path("bad_words.RDS"))
  bad_words <- readLines("./bad_word_list.txt")     save(bad_words, get_data_file_path("bad_words.RData"))
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
load(get_data_file_path("bad_words.RData"))
save(bad_words, get_data_file_path("bad_words.RData"))
save(bad_words,file =  get_data_file_path("bad_words.RData"))
load(get_data_file_path("bad_words.RData"))
if (!file.exists(get_data_file_path("data_cleaned.RData"))) {     flog.info("data cleaning...")     data.all <- as.data.frame(data.all)     data.all <- sent_detect(data.all) #Detect and split sentences on endmark boundaries.     data.all <- tolower(data.all)     data.all <- removeNumbers(data.all)     data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)     data.all <- stripWhitespace(data.all)     data.all <- gsub("\u0092|\u0093|\u0094", "", data.all)     data.all <- gsub("(ftp|http)(s?)://.*\\b", "", data.all) # urls     data.all <- gsub("\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\b", "", data.all) # emails      data.all <- gsub("[@][a - zA - Z0 - 9_]{1,15}", "", data.all) # twitter usernames      data.all <- gsub("RT |via", "", data.all) # twitter tags      save(data.all, file = get_data_file_path("data_cleaned.RData")) }
load(get_data_file_path("data_cleaned.RData"))
load(get_data_file_path("data_cleaned.RData"))
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "Windows-1252")
p_load("tm")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "Windows-1252")
require(ngram)
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
n1 <- ngram::ngram_asweka(data.all, min = 1, max = 1)
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
ds <- sapply(data.all, as.character) # a character vector
n1 <- ngram::ngram_asweka(ds, min = 1, max = 1)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
flog.warn(w)
 ds <- sapply(data.all, as.character) # a character vector
n1 <- ngram::ngram_asweka(ds, min = 1, max = 1)
ds <- data.all[]
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
ds <- as.character(data.all[,])
ds <- as.character(data.all[])
require(tidyr)
data.stringified <- paste(data.all, collapse = '')
length(data.stringified)
length(data.stringified[0])
length(data.stringified[1])
class(data.stringified)
n1 <- ngram::ngram_asweka(data.stringified, min = 1, max = 1)
n2 <- ngram::ngram_asweka(data.stringified, min = 2, max = 2)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
 data.all <- gsub("[\d-]", "", data.all) # remove numbers
  flog.info("data cleaning...")     #data.all <- as.data.frame(data.all)     data.all <- sent_detect(data.all) #Detect and split sentences on endmark boundaries.     data.all <- gsub("\u0092|\u0093|\u0094", "", data.all)     data.all <- gsub("(ftp|http)(s?)://.*\\b", "", data.all) # urls     data.all <- gsub("\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\b", "", data.all) # emails      data.all <- gsub("[@][a - zA - Z0 - 9_]{1,15}", "", data.all) # twitter usernames      data.all <- gsub("RT |via", "", data.all) # twitter tags      data.all <- gsub("[\d-]", "", data.all) # remove numbers     data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)     data.all <- stripWhitespace(data.all)     data.all <- tolower(data.all)     save(data.all, file = get_data_file_path("data.all.RData"))
 flog.info("data cleaning...")     #Create clean functions     remove_symbols <- function(corpus) gsub(perl = TRUE,                                         pattern = '[\\]\\[\\(\\)-/+;:#%$^\\*=^~\\{\\}/"<>«»_\\\\“\\”⁰•‘’–]',                                         replacement = "", corpus)     convert_to_period <- function(corpus) gsub(pattern = "[\\!\\?…]",                                            replacement = ".", corpus)     reduce_periods <- function(corpus) gsub(pattern = "[\\.]{2,}",                                         replacement = ".", corpus)     convert_to_and <- function(corpus) gsub(pattern = "&", replacement = " and ", corpus)     replace_numbers <- function(corpus) gsub(pattern = "[0-9]+",                                          replacement = "", corpus)     #data.all <- as.data.frame(data.all)     data.all <- sent_detect(data.all) #Detect and split sentences on endmark boundaries.     data.all <- convert_to_and(data.all)     data.all <- convert_to_period(data.all)     data.all <- remove_symbols(data.all)     data.all <- reduce_periods(data.all)     data.all <- replace_numbers(data.all)     data.all <- gsub("(ftp|http)(s?)://.*\\b", "", data.all) # urls     data.all <- gsub("\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\b", "", data.all) # emails      data.all <- gsub("[@][a - zA - Z0 - 9_]{1,15}", "", data.all) # twitter usernames      data.all <- gsub("RT |via", "", data.all) # twitter tags      data.all <- gsub("[\d-]", "", data.all) # remove numbers     data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)     data.all <- stripWhitespace(data.all)     data.all <- tolower(data.all)     save(data.all, file = get_data_file_path("data.all.RData"))
 flog.info("data cleaning...")     #Create clean functions     remove_symbols <- function(corpus) gsub(perl = TRUE,                                         pattern = '[\\]\\[\\(\\)-/+;:#%$^\\*=^~\\{\\}/"<>«»_\\\\“\\”⁰•‘’–]',                                         replacement = "", corpus)     convert_to_period <- function(corpus) gsub(pattern = "[\\!\\?…]",                                            replacement = ".", corpus)     reduce_periods <- function(corpus) gsub(pattern = "[\\.]{2,}",                                         replacement = ".", corpus)     convert_to_and <- function(corpus) gsub(pattern = "&", replacement = " and ", corpus)     replace_numbers <- function(corpus) gsub(pattern = "[0-9]+",                                          replacement = "", corpus)     #data.all <- as.data.frame(data.all)     data.all <- sent_detect(data.all) #Detect and split sentences on endmark boundaries.     data.all <- convert_to_and(data.all)     data.all <- convert_to_period(data.all)     data.all <- remove_symbols(data.all)     data.all <- reduce_periods(data.all)     data.all <- replace_numbers(data.all)     data.all <- gsub("(ftp|http)(s?)://.*\\b", "", data.all) # urls     data.all <- gsub("\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\b", "", data.all) # emails      data.all <- gsub("[@][a - zA - Z0 - 9_]{1,15}", "", data.all) # twitter usernames      data.all <- gsub("RT |via", "", data.all) # twitter tags      data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)     data.all <- stripWhitespace(data.all)     data.all <- tolower(data.all)     save(data.all, file = get_data_file_path("data.all.RData"))
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
babble(ng = ng, genlen = 12)
babble(ng = n1, genlen = 12)
get.phrasetable(n2)
class(n2)
ng1 <- get.ngrams(n1)
require(ngram)
ng1 <- get.ngrams(n1)
get.ngrams(ng)
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
class(n2)
nx <- ngram(n2)
nx
class(nx)
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
n2 <- ngram(ngram::ngram_asweka(data.stringified, min = 2, max = 2, sep = " "))
n2 <- ngram(ngram::ngram_asweka(data.stringified, min = 2, max = 2, sep = " "))     n3 <- ngram(ngram::ngram_asweka(data.stringified, min = 3, max = 3, sep = " "))     n4 <- ngram(ngram::ngram_asweka(data.stringified, min = 4, max = 4, sep = " "))     n5 <- ngram(ngram::ngram_asweka(data.stringified, min = 5, max = 5, sep = " "))
get.phrasetable(n2)
babble(n2, genlen = 5, seed = 2017)
babble(n2, genlen = 5, seed = 2017)
babble(n2, genlen = 3, seed = 2017)
get.ngrams()
get.ngrams(n2)
get.Pearsall Primary School
gc()
    rm(remove_symbols)     rm(reduce_periods)     rm(convert_to_and)     rm(convert_to_period)     rm(replace_numbers)
get.nextwords(n2)
wordcount(n2)
wordcount(data.stringified)
textcnt()
wordcount(n2, sep = " ", count.function = sum)
wordcount(data.stringified, sep = " ", count.function = sum)
n5$n
require(tau)
n2 <- textcnt(data.stringified,n=2,method = "ngram")
n2
n2 <- textcnt(data.stringified, n = 2, method = "string")
p_load("data.table")
n2 <- create_ngram(data.stringified, 2)
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
n2 <- sqldf("select * from n2 where freq > 100")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
n2 <- sqldf("select * from n2 where fpredictor <- function() {     sample1 <- function() {            cat("sample1")     }     sample2 <- function() {         cat("sample2")     } }req > 100")
sample1Fn <- function() {     cat("sample1") } predictor <- function() {     sample1 <- sample1Fn() }
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "utf-8")
predictor().sample1Fn()if (!file.exists(get_data_file_path("bad_words.RData"))) {     save_file("https://goo.gl/To9w5B", "bad_word_list.txt")     bad_words <- readLines("./bad_word_list.txt")     save(bad_words, file = get_data_file_path("bad_words.RData")) }
overwrite <- FALSE ## bad words if (!file.exists(get_data_file_path("bad_words.RData"))) {     save_file("https://goo.gl/To9w5B", "bad_word_list.txt")     bad_words <- readLines("./bad_word_list.txt")     save(bad_words, file = get_data_file_path("bad_words.RData")) }
load(get_data_file_path("bad_words.RData"))
  flog.info("data cleaning...")     #Create clean functions     remove_symbols <- function(corpus) gsub(perl = TRUE,                                         pattern = '[\\]\\[\\(\\)-/+;:#%$^\\*=^~\\{\\}/"<>«»_\\\\“\\”⁰•‘’–]',                                         replacement = "", corpus)     convert_to_period <- function(corpus) gsub(pattern = "[\\!\\?…]",                                            replacement = ".", corpus)     reduce_periods <- function(corpus) gsub(pattern = "[\\.]{2,}",                                         replacement = ".", corpus)     convert_to_and <- function(corpus) gsub(pattern = "&", replacement = " and ", corpus)     replace_numbers <- function(corpus) gsub(pattern = "[0-9]+",                                          replacement = "", corpus)     #data.all <- as.data.frame(data.all)     data.all <- sent_detect(data.all) #Detect and split sentences on endmark boundaries.     data.all <- convert_to_and(data.all)     data.all <- convert_to_period(data.all)     data.all <- remove_symbols(data.all)     data.all <- reduce_periods(data.all)     data.all <- replace_numbers(data.all)     data.all <- gsub("(ftp|http)(s?)://.*\\b", "", data.all) # urls     data.all <- gsub("\\b[A-Z a-z 0-9._ - ]*[@](.*?)[.]{1,3} \\b", "", data.all) # emails      data.all <- gsub("[@][a - zA - Z0 - 9_]{1,15}", "", data.all) # twitter usernames      data.all <- gsub("RT |via", "", data.all) # twitter tags      data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)     data.all <- stripWhitespace(data.all)     data.all <- tolower(data.all)     save(data.all, file = get_data_file_path("data.all.RData"))     rm(remove_symbols)     rm(reduce_periods)     rm(convert_to_and)     rm(convert_to_period)     rm(replace_numbers)
load(get_data_file_path("data.all.RData"))
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
if (is.na(word_string) || length(word_string == 0)) ERROR("word_string NA or empty")
ERROR("word_string NA or empty")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function(sentence_string) {     if (is.na(sentence_string) || length(sentence_string == 0)) throw("sentence_string NA or empty")     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         result <- sqldf("select * from n2 where w1w2 like '%sentence_string' order by freq desc")         result     } } predictor("a")
 warning("sentence_string NA or empty")
throw()
throw
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function(sentence_string) {     if (is.na(sentence_string) || length(sentence_string == 0)) {         warning("sentence_string NA or empty")         stop()     }     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         result <- sqldf("select * from n2 where w1w2 like '%sentence_string' order by freq desc")         result     } } predictor("a")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function(sentence_string) {     flog.debug(paste("predictor input", sentence_string))     if (is.na(sentence_string) || length(sentence_string == 0)) {         warning("sentence_string NA or empty")         stop()     }     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         result <- sqldf("select * from n2 where w1w2 like '%sentence_string' order by freq desc")         result     } } predictor("a")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function(sentence_string) {     flog.debug(paste("predictor input", sentence_string))     sentence_string     if (is.na(sentence_string) || length(sentence_string == 0)) {         warning("sentence_string NA or empty")         stop()     }     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         result <- sqldf("select * from n2 where w1w2 like '%sentence_string' order by freq desc")         result     } } predictor("a")
predictor("dfgsdfga")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function(sentence_string) {     flog.debug(paste("predictor input", sentence_string))     #sentence_string     #if (is.na(sentence_string) || length(sentence_string == 0)) {         #warning("sentence_string NA or empty")         #stop()     #}     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         result <- sqldf("select * from n2 where w1w2 like '%sentence_string' order by freq desc")         result     } } predictor("dfgsdfga")
sql <- paste("select * from n2 where word like '%", sentence_string, "%'")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function(sentence_string) {     flog.debug(paste("predictor input", sentence_string))     #sentence_string     #if (is.na(sentence_string) || length(sentence_string == 0)) {         #warning("sentence_string NA or empty")         #stop()     #}     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         sql <- paste("select * from n2 where word like '%", sentence_string, "%'")         result <- sqldf(sql)         result     } } predictor("dfgsdfga")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function(sentence_string) {     flog.debug(paste("predictor input", sentence_string))     #sentence_string     #if (is.na(sentence_string) || length(sentence_string == 0)) {         #warning("sentence_string NA or empty")         #stop()     #}     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         sql <- paste("select * from n2 where w1w2 like '%", sentence_string, "%'")         result <- sqldf(sql)         result     } } predictor("dfgsdfga")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function(sentence_string) {     flog.debug(paste("predictor input", sentence_string))     sentence_string     #if (is.na(sentence_string) || length(sentence_string == 0)) {         #warning("sentence_string NA or empty")         #stop()     #}     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         sql <- paste("select * from n2 where w1w2 like '%", sentence_string, "%'")         result <- sqldf(sql)         result     } } predictor("dfgsdfga")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function(sentence_string) {     flog.info(paste("predictor input", sentence_string))    $ sentence_string     #if (is.na(sentence_string) || length(sentence_string == 0)) {         #warning("sentence_string NA or empty")         #stop()     #}     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         sql <- paste("select * from n2 where w1w2 like '%", sentence_string, "%'")         result <- sqldf(sql)         result     } } predictor("dfgsdfga")
sentence_string <- "a" #' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function() {     flog.info(paste("predictor input", sentence_string))    $ sentence_string     #if (is.na(sentence_string) || length(sentence_string == 0)) {         #warning("sentence_string NA or empty")         #stop()     #}     words = strsplit(sentence_string, " ")     if (length(words) == 1) {         sql <- paste("select * from n2 where w1w2 like '%", sentence_string, " %'")         result <- sqldf(sql)         result     } } predictor("dfgsdfga")
sentence_string <- "a" #' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function() {     flog.info(paste("predictor input", sentence_string))    $ sentence_string     #if (is.na(sentence_string) || length(sentence_string == 0)) {         #warning("sentence_string NA or empty")         #stop()     #}     words = strsplit(sentence_string, " ")     if (length(words) == 1)     {         sql <- paste("select * from n2 where w1w2 like '%", sentence_string, " %'")         result <- sqldf(sql)         result     } } predictor("dfgsdfga")
sentence_string <- "a" #' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function() {     flog.info(paste("predictor input", sentence_string))     if (is.na(sentence_string) || length(sentence_string == 0)) {         warning("sentence_string NA or empty")         #stop()     }     words = strsplit(sentence_string, " ")     if (length(words) == 1)     {         sql <- paste("select * from n2 where w1w2 like '%", sentence_string, " %'")         result <- sqldf(sql)         result     } } predictor("dfgsdfga")
sentence_string <- "a" #' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function() {     flog.info(paste("predictor input", sentence_string))     if (is.na(sentence_string) || length(sentence_string == 0)) {         warning("sentence_string NA or empty")         #stop()     }     words = strsplit(sentence_string, " ")     if (length(words) == 1)     {         sql <- paste("select * from n2 where w1w2 like '%", sentence_string, " %'")         result <- sqldf(sql)         result     } } predictor()
sss <- "a" #' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function() {     flog.info(paste("predictor input", sss))     if (is.na(sss) || length(sss == 0)) {         warning("sentence_string NA or empty")         #stop()     }     words = strsplit(sss, " ")     if (length(words) == 1)     {         sql <- paste("select * from n2 where w1w2 like '%", words[1], " %'")         result <- sqldf(sql)         result     } } predictor()
sss <- "a" #' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence_string #' #' @return #' @export #' #' @examples predictor <- function() {     flog.info(paste("predictor input", sss))     if (is.na(sss)) {         warning("sentence_string NA or empty")         #stop()     }     words = strsplit(sss, " ")     if (length(words) == 1)     {         sql <- paste("select * from n2 where w1w2 like '%", words[1], " %'")         result <- sqldf(sql)         result     } } predictor()
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence  #' #' @return #' @export #' #' @examples predictor <- function(sentence) {     flog.info(paste("predictor input", sentence))     if (is.na(sss)) {         warning("sentence_string NA or empty")         #stop()     }     words = strsplit(sentence, " ")     if (length(words) == 1)     {         sql <- paste("select * from n2 where w1w2 like '%", words[1], " %'")         result <- sqldf(sql)         result     } } predictor("beer")
source("C:/dev/r-course/10-capstone/project_1__setup.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence  #' #' @return #' @export #' #' @examples predictor <- function(sentence) {     flog.info(paste("predictor input", sentence))     if (is.na(sss)) {         warning("sentence_string NA or empty")         #stop()     }     words = strsplit(sentence, " ")     if (length(words) == 1)     {         sql <- paste("select * from n2 where w1w2 like '%", words[1], "%'")         flog.info(paste("predictor sql", sql))         result <- sqldf(sql)         result     } } predictor("beer")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence  #' #' @return #' @export #' #' @examples predictor <- function(sentence) {     flog.info(paste("predictor input", sentence))     if (is.na(sss)) {         warning("sentence_string NA or empty")         #stop()     }     words = strsplit(sentence, " ")     if (length(words) == 1)     {         sql <- paste0("select * from n2 where w1w2 like '%", words[1], "%'")         flog.info(paste("predictor sql", sql))         result <- sqldf(sql)         result     } } predictor("beer")
#' Given a sentence predict the next word #' 1 word = quick lookup in bigram (n2) for most frequent terms #' #' relies on n2,n3,n4,n5 #' @param sentence  #' #' @return #' @export #' #' @examples predictor <- function(sentence) {     flog.info(paste("predictor input", sentence))     if (is.na(sss)) {         warning("sentence_string NA or empty")         #stop()     }     words = strsplit(sentence, " ")     if (length(words) == 1)     {         sql <- paste0("select * from n2 where w1w2 like '%", words[1], "%'")         flog.info(paste("predictor sql", sql)) 
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "utf-8")
 flog.info("data cleaning...")     #Create clean functions     remove_symbols <- function(corpus) gsub(perl = TRUE,                                         pattern = '[\\]\\[\\(\\)-/+;:#%$^\\*=^~\\{\\}/"<>«»_\\\\“\\”⁰•‘’–]',                                         replacement = "", corpus)     convert_to_period <- function(corpus) gsub(pattern = "[\\!\\?…]",                                            replacement = ".", corpus)     reduce_periods <- function(corpus) gsub(pattern = "[\\.]{2,}",                                         replacement = ".", corpus)     convert_to_and <- function(corpus) gsub(pattern = "&", replacement = " and ", corpus)     replace_numbers <- function(corpus) gsub(pattern = "[0-9]+",                                          replacement = "", corpus)
    data.all <- as.data.frame(data.all)
data.all <- parallelizeTask(sent_detect_nlp, data.all) #Detect and split sentences on endmark boundaries.
data.all <- parallelize_task(sent_detect_nlp, data.all) #Detect and split sentences on endmark boundaries.
source("C:\\dev\\r-course\\10-capstone\\project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
    flog.info("data cleaning...")     #Create clean functions     remove_symbols <- function(corpus) gsub(perl = TRUE,                                         pattern = '[\\]\\[\\(\\)-/+;:#%$^\\*=^~\\{\\}/"<>«»_\\\\“\\”⁰•‘’–]',                                         replacement = "", corpus)     convert_to_period <- function(corpus) gsub(pattern = "[\\!\\?…]",                                            replacement = ".", corpus)     reduce_periods <- function(corpus) gsub(pattern = "[\\.]{2,}",                                         replacement = ".", corpus)     convert_to_and <- function(corpus) gsub(pattern = "&", replacement = " and ", corpus)     replace_numbers <- function(corpus) gsub(pattern = "[0-9]+",                                          replacement = "", corpus)
data.all <- convert_to_and(data.all)
data.all <- convert_to_period(data.all)
    data.all <- remove_symbols(data.all)
    data.all <- reduce_periods(data.all)
    data.all <- replace_numbers(data.all)
    data.all <- gsub("(ftp|http)(s?)://.*\\b", "", data.all) # urls
    data.all <- gsub("\\S+@\\S+", "", data.all) # emails 
    data.all <- gsub("[@][a - zA - Z0 - 9_]{1,15}", "", data.all) # twitter usernames 
    data.all <- gsub("RT |via", "", data.all) # twitter tags 
    data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)
    data.all <- stripWhitespace(data.all)
    flog.info("data cleaning complete...")
    save(data.all, file = get_data_file_path("data.all.RData"))
    flog.info("data cleaning file saved...")
    rm(remove_symbols)
    rm(reduce_periods)
    rm(convert_to_and)
    rm(convert_to_period)
    rm(replace_numbers)
    gc()
data.all <- tolower(data.all)
    data.all <- stripWhitespace(data.all)
save(data.all, file = get_data_file_path("data.all.RData"))
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
 flog.info("data cleaning...")     #Create clean functions     remove_symbols <- function(corpus) gsub(perl = TRUE,                                         pattern = '[\\]\\[\\(\\)-/+;:#%$^\\*=^~\\{\\}/"<>«»_\\\\“\\”⁰•‘’–]',                                         replacement = "", corpus)     convert_to_period <- function(corpus) gsub(pattern = "[\\!\\?…]",                                            replacement = ".", corpus)     reduce_periods <- function(corpus) gsub(pattern = "[\\.]{2,}",                                         replacement = ".", corpus)     convert_to_and <- function(corpus) gsub(pattern = "&", replacement = " and ", corpus)     replace_numbers <- function(corpus) gsub(pattern = "[0-9]+",                                          replacement = "", corpus)
data.all <- as.data.table(data.all, stringsAsFactors = FALSE)
    data.all <- parallelize_task(convert_to_and, data.all)
load(get_data_file_path("data.all.RData"))
data.all <- as.data.table(data.all, stringsAsFactors = FALSE)
data.all <- parallelize_task(sent_detect_nlp, data.all) #Detect and split sentences on endmark boundaries.
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
 flog.info("data cleaning...")     #Create clean functions     remove_symbols <- function(corpus) gsub(perl = TRUE,                                         pattern = '[\\]\\[\\(\\)-/+;:#%$^\\*=^~\\{\\}/"<>«»_\\\\“\\”⁰•‘’–]',                                         replacement = "", corpus)     convert_to_period <- function(corpus) gsub(pattern = "[\\!\\?…]",                                            replacement = ".", corpus)     reduce_periods <- function(corpus) gsub(pattern = "[\\.]{2,}",                                         replacement = ".", corpus)     convert_to_and <- function(corpus) gsub(pattern = "&", replacement = " and ", corpus)     replace_numbers <- function(corpus) gsub(pattern = "[0-9]+",                                          replacement = "", corpus)
data.all <- as.data.table(data.all, stringsAsFactors = FALSE) # stringsAsFactors = FALSE important for speed
    data.all <- parallelize_task_chunked(sent_detect, data.all) #Detect and split sentences on endmark boundaries.
df.merged <- foreach(df.split = isplitRows(df.big, chunkSize = chunk_size)) %dopar% {         df.chunked <- task()         merge(df.split, df.chunked, by = 'num', all.x = T)     }
data.all <- parallelize_task_chunked(sent_detect, data.all) #Detect and split sentences on endmark boundaries.
parallelize_task_chunked <- function(task, df.big, chunk_size = 1000) {     p_load("itertools")     # Calculate the number of cores     ncores <- detectCores() - 1     # Initiate cluster     cl <- makeCluster(ncores)     registerDoParallel(cl)     flog.debug("Task starting")     df.merged <- foreach(df.split = isplitRows(df.big, chunkSize = chunk_size)) %dopar% {         df.chunked <- task(df.split)     }     flog.debug("Task done")     stopCluster(cl)     df.merged }
data.all <- parallelize_task_chunked(sent_detect, data.all) #Detect and split sentences on endmark boundaries.
load(get_data_file_path("data.all.RData"))
data.all <- as.data.table(data.all, stringsAsFactors = FALSE) # stringsAsFactors = FALSE important for speed
    data.all <- parallelize_task_chunked(sent_detect, data.all) #Detect and split sentences on endmark boundaries.
load(get_data_file_path("data.all.RData"))
data.all <- as.data.table(data.all, stringsAsFactors = FALSE) # stringsAsFactors = FALSE important for speed
    data.all <- parallelize_task_chunked(sent_detect, data.all) #Detect and split sentences on endmark boundaries.
load(get_data_file_path("data.all.RData"))
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "Windows-1252")
data.all <- as.data.table(data.all, stringsAsFactors = FALSE) # stringsAsFactors = FALSE important for speed
    data.all <- parallelize_task_chunked(sent_detect, data.all) #Detect and split sentences on endmark boundaries.
load(get_data_file_path("data.all.RData"))
data.all <- as.data.table(data.all, stringsAsFactors = FALSE) # stringsAsFactors = FALSE important for speed
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "Windows-1252")
parallelize_task_chunked <- function(task, df.big, chunk_size = 1000) {     p_load("itertools")     # Calculate the number of cores     ncores <- detectCores() - 1     # Initiate cluster     cl <- makeCluster(ncores)     registerDoParallel(cl)     flog.debug("Task starting")     df.merged <- foreach(df.split = isplitRows(df.big, chunkSize = chunk_size), .combine = cbind, .inorder = TRUE) %dopar% {         rbind(task(df.split))     }     flog.debug("Task done")     stopCluster(cl)     df.merged }
data.all <- parallelize_task_chunked(sent_detect, data.all) #Detect and split sentences on endmark boundaries.
data.all <- parallelize_task_chunked(convert_to_and, data.all)
    data.all <- parallelize_task_chunked(convert_to_period, data.all)
    data.all <- parallelize_task_chunked(remove_symbols, data.all)
    data.all <- parallelize_task_chunked(reduce_periods, data.all)
    data.all <- parallelize_task_chunked(replace_numbers, data.all)
    data.all <- parallelize_task(removePunctuation, data.all, preserve_intra_word_dashes = TRUE)
    data.all <- parallelize_task(gsub, data.all, "(ftp|http)(s?)://.*\\b", "") # urls
    data.all <- parallelize_task(gsub, data.all, "\\S+@\\S+", "") # emails 
    data.all <- parallelize_task(gsub, data.all, "\\S+@\\S+", "") # emails 
    data.all <- parallelize_task(gsub, data.all, "\\S+@\\S+", "") # emails 
    data.all <- parallelize_task(gsub, data.all, "(ftp|http)(s?)://.*\\b", "") # urls
 data.all <- parallelize_task(removePunctuation, data.all, preserve_intra_word_dashes = TRUE)     data.all <- parallelize_task(gsub, data.all, "(ftp|http)(s?)://.*\\b", "") # urls     data.all <- parallelize_task(gsub, data.all, "\\S+@\\S+", "") # emails      data.all <- parallelize_task(gsub, data.all, "[@][a - zA - Z0 - 9_]{1,15}", "") # twitter usernames      data.all <- parallelize_task(gsub, data.all, "RT |via", "") # twitter tags 
    data.all <- parallelize_task_chunked(stripWhitespace, data.all)     data.all <- parallelize_task_chunked(tolower, data.all)
load(get_data_file_path("data.all.RData"))
data.all <- as.data.table(data.all, stringsAsFactors = FALSE) # stringsAsFactors = FALSE important for speed
    data.all <- parallelize_task_chunked(sent_detect, data.all) #Detect and split sentences on endmark boundaries.
    data.all <- parallelize_task_chunked(convert_to_and, data.all)
    data.all <- parallelize_task_chunked(convert_to_period, data.all)
    data.all <- parallelize_task_chunked(remove_symbols, data.all)
    data.all <- parallelize_task_chunked(reduce_periods, data.all)
    data.all <- parallelize_task_chunked(replace_numbers, data.all)
    data.all <- parallelize_task(removePunctuation, data.all, preserve_intra_word_dashes = TRUE)
    data.all <- parallelize_task(gsub, data.all, "(ftp|http)(s?)://.*\\b", "") # urls
load(get_data_file_path("data.all.RData"))
 data.all <- as.data.table(data.all, stringsAsFactors = FALSE) # stringsAsFactors = FALSE important for speed     #chunked     data.all <- parallelize_task_chunked(sent_detect, data.all) #Detect and split sentences on endmark boundaries.     data.all <- parallelize_task_chunked(convert_to_and, data.all)     data.all <- parallelize_task_chunked(convert_to_period, data.all)     data.all <- parallelize_task_chunked(remove_symbols, data.all)     data.all <- parallelize_task_chunked(reduce_periods, data.all)     data.all <- parallelize_task_chunked(replace_numbers, data.all)
data.all <- parallelize_task(removePunctuation, data.all, preserve_intra_word_dashes = TRUE)
    data.all <- parallelize_task(gsub, data.all, "\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)", "") # urls
load(get_data_file_path("data.all.RData"))
  data.all <- as.data.table(data.all, stringsAsFactors = FALSE) # stringsAsFactors = FALSE important for speed     #chunked     data.all <- parallelize_task_chunked(sent_detect, data.all) #Detect and split sentences on endmark boundaries.     data.all <- parallelize_task_chunked(convert_to_and, data.all)     data.all <- parallelize_task_chunked(convert_to_period, data.all)     data.all <- parallelize_task_chunked(remove_symbols, data.all)     data.all <- parallelize_task_chunked(reduce_periods, data.all)     data.all <- parallelize_task_chunked(replace_numbers, data.all)
data.all <- parallelize_task(removePunctuation, data.all, preserve_intra_word_dashes = TRUE)
data.all <- parallelize_task(gsub, data.all, "\\S+@\\S+", "") # emails 
load(get_data_file_path("data.all.RData"))
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
 flog.info("data cleaning...")     #Create clean functions     remove_symbols <- function(corpus) gsub(perl = TRUE,                                         pattern = '[\\]\\[\\(\\)-/+;:#%$^\\*=^~\\{\\}/"<>«»_\\\\“\\”⁰•‘’–]',                                         replacement = "", corpus)     convert_to_period <- function(corpus) gsub(pattern = "[\\!\\?…]",                                            replacement = ".", corpus)     reduce_periods <- function(corpus) gsub(pattern = "[\\.]{2,}",                                         replacement = ".", corpus)     convert_to_and <- function(corpus) gsub(pattern = "&", replacement = " and ", corpus)     replace_numbers <- function(corpus) gsub(pattern = "[0-9]+",                                          replacement = "", corpus)
 data.all <- as.data.table(data.all, stringsAsFactors = FALSE) # stringsAsFactors = FALSE important for speed
    data.all <- parallelize_task_chunked(sent_detect, data.all) #Detect and split sentences on endmark boundaries.     data.all <- parallelize_task_chunked(convert_to_and, data.all)     data.all <- parallelize_task_chunked(convert_to_period, data.all)     data.all <- parallelize_task_chunked(remove_symbols, data.all)     data.all <- parallelize_task_chunked(reduce_periods, data.all)     data.all <- parallelize_task_chunked(replace_numbers, data.all)
parallelize_task_chunked <- function(task, df.big, chunk_size = 1000) {     p_load("itertools")     # Calculate the number of cores     ncores <- detectCores() - 1     # Initiate cluster     cl <- makeCluster(ncores)     registerDoParallel(cl)     flog.debug(paste("Task starting", match.call()[2]))     df.merged <- foreach(df.split = isplitRows(df.big, chunkSize = chunk_size), .combine = cbind, .inorder = TRUE) %dopar% {         rbind(task(df.split))     }     flog.debug(paste("Task complete", match.call()[2]))     stopCluster(cl)     df.merged }
data.all <- parallelize_task_chunked(convert_to_and, data.all)
data.all <- parallelize_task(removePunctuation, data.all, preserve_intra_word_dashes = TRUE)
    data.all <- gsub(data.all, "https?:\\/\\/(.*?|\\/)(?=\\s|$)\\s?", "") # urls      data.all <- gsub(data.all, "\\S+@\\S+", "") # emails      data.all <- gsub(data.all, "[@][a - zA - Z0 - 9_]{1,15}", "") # twitter usernames      data.all <- gsub(data.all, "RT |via", "") # twitter tags 
load(get_data_file_path("data.all.RData"))
 flog.info("data cleaning...")     #Create clean functions     remove_symbols <- function(corpus) gsub(perl = TRUE,                                         pattern = '[\\]\\[\\(\\)-/+;:#%$^\\*=^~\\{\\}/"<>«»_\\\\“\\”⁰•‘’–]',                                         replacement = "", corpus)     convert_to_period <- function(corpus) gsub(pattern = "[\\!\\?…]",                                            replacement = ".", corpus)     reduce_periods <- function(corpus) gsub(pattern = "[\\.]{2,}",                                         replacement = ".", corpus)     convert_to_and <- function(corpus) gsub(pattern = "&", replacement = " and ", corpus)     replace_numbers <- function(corpus) gsub(pattern = "[0-9]+",                                          replacement = "", corpus)
data.all <- as.data.table(data.all, stringsAsFactors = FALSE) # stringsAsFactors = FALSE important for speed     #chunked     data.all <- parallelize_task_chunked(sent_detect, data.all) #Detect and split sentences on endmark boundaries.     data.all <- parallelize_task_chunked(convert_to_and, data.all)     data.all <- parallelize_task_chunked(convert_to_period, data.all)     data.all <- parallelize_task_chunked(remove_symbols, data.all)     data.all <- parallelize_task_chunked(reduce_periods, data.all)     data.all <- parallelize_task_chunked(replace_numbers, data.all)
data.all <- parallelize_task(removePunctuation, data.all, preserve_intra_word_dashes = TRUE)
data.all <- gsub(data.all, "https?:\\/\\/(.*?|\\/)(?=\\s|$)\\s?", "") # urls      data.all <- gsub(data.all, "\\S+@\\S+", "") # emails      data.all <- gsub(data.all, "(^|[^@\\w])@(\\w{1,15})\\b", "") # twitter usernames 
load(get_data_file_path("data.all.RData"))
load(get_data_file_path("data.all.RData"))
 data.all <- as.data.table(data.all, stringsAsFactors = FALSE) # stringsAsFactors = FALSE important for speed     #chunked     data.all <- parallelize_task_chunked(sent_detect, data.all) #Detect and split sentences on endmark boundaries.     data.all <- parallelize_task_chunked(convert_to_and, data.all)     data.all <- parallelize_task_chunked(convert_to_period, data.all)     data.all <- parallelize_task_chunked(remove_symbols, data.all)     data.all <- parallelize_task_chunked(reduce_periods, data.all)     data.all <- parallelize_task_chunked(replace_numbers, data.all)
    data.all <- parallelize_task(removePunctuation, data.all, preserve_intra_word_dashes = TRUE)
pacman::p_load_gh("trinker/qdapRegex") # https://github.com/trinker/qdapRegex
    data.all <- parallelize_task_chunked(ex_hash, data.all) # Twitter Hash Tags     data.all <- parallelize_task_chunked(ex_tag, data.all) #Name Tags     data.all <- parallelize_task_chunked(ex_url, data.all) # URLs
load(get_data_file_path("data.all.RData"))
 data.all <- as.data.table(data.all, stringsAsFactors = FALSE) # stringsAsFactors = FALSE important for speed     #chunked     data.all <- parallelize_task_chunked(sent_detect, data.all) #Detect and split sentences on endmark boundaries.     data.all <- parallelize_task_chunked(convert_to_and, data.all)     data.all <- parallelize_task_chunked(convert_to_period, data.all)     data.all <- parallelize_task_chunked(remove_symbols, data.all)     data.all <- parallelize_task_chunked(reduce_periods, data.all)     data.all <- parallelize_task_chunked(replace_numbers, data.all)
data.all <- parallelize_task(removePunctuation, data.all, preserve_intra_word_dashes = TRUE)
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "Windows-1252")
    data.all <- parallelize_task_chunked(sent_detect, data.all) #Detect and split sentences on endmark boundaries.
    data.all <- parallelize_task_chunked(convert_to_and, data.all)
    data.all <- parallelize_task_chunked(convert_to_period, data.all)
    data.all <- parallelize_task_chunked(remove_symbols, data.all)
    data.all <- parallelize_task_chunked(reduce_periods, data.all)
    data.all <- parallelize_task_chunked(replace_numbers, data.all)
    data.all <- parallelize_task(removePunctuation, data.all, preserve_intra_word_dashes = TRUE)
    data.all <- parallelize_task_chunked(stripWhitespace, data.all)
    data.all <- parallelize_task_chunked(tolower, data.all)
    save(data.all, file = get_data_file_path("data.all.RData"))
 rm(remove_symbols)     rm(reduce_periods)     rm(convert_to_and)     rm(convert_to_period)     rm(replace_numbers)     gc()
class(data.all)
 create_ngram <- function(text, ngram_size) {         ngram <- data.table()         ng <- textcnt(text, method = "string", n = ngram_size,lower = 10L) # freq must be 10 or greater         if (ngram_size == 1) {             ngram <- data.table(word = names(ng), freq = unclass(ng), length = nchar(names(ng)))         }         else {             ngram <- data.table(word = names(ng), freq = unclass(ng), length = nchar(names(ng)))         }         return(ngram)     }
data.stringified <- paste(data.all, collapse = '')
    n2 <- parallelize_task(create_ngram,data.stringified, 2)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
n3 <- parallelize_task(create_ngram, data.stringified, 3)
n4 <- create_ngram(data.stringified, 4)
n5 <- create_ngram(data.stringified, 5)
#' Search ngrams with identical first two words.  #' Calculate probabilities using modified Kneser - Ney smoothing.  #' Use the word with the highest probability as prediction and the words with the highest probabilities as possible choices. #' relies on n2,n3,n4,n5 #' @param sentence  #' #' @return #' @export #' #' @examples #' https://en.wikipedia.org/wiki/Katz%27s_back-off_model predictor <- function(sentence) {     flog.info(paste("predictor input", sentence))     if (is.na(sentence)) {         warning("sentence NA or empty")         #stop()     }     words = strsplit(sentence, " ")     # 1 word given --> lookup to see the next in the list ordered by most frequesnt     if (length(words) == 1) sqldf(create_sql("n2", words))     # 2 words given      if (length(words) == 2) sqldf(create_sql("n3", words))     # 3 words given      if (length(words) == 3) sqldf(create_sql("n4", words))     # 4 words given      if (length(words) >= 4) sqldf(create_sql("n5", words)) } #' Dynamic SQL string to search a ngram source for N terms provided #' #' @param ngram_name #' @param words #' #' @return #' @export #' #' @examples create_sql <- function(ngram_name,words) {     arg <- paste(words, collapse = '')     flog.debug(paste("predictor sql", sql))     sql <- paste0("select * from ",ngram," where word like '", word, "%' order by freq desc limit 10")     sql } result <- predictor("The guy in front of me just bought a pound of bacon, a bouquet, and a case of") result
result <- predictor("The guy in front of me just bought a pound of bacon, a bouquet, and a case of")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_3__data_clean.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
words = as.data.frame(strsplit(sentence, " "))
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
words = unlist(strsplit(sentence, " "))
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"
    words = unlist(strsplit(sentence, " "))
sentence <- clean.text(words)
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
words = unlist(strsplit(sentence, " "))     words <- clean.text(words)
sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     words = unlist(strsplit(sentence, " "))     words <- clean.text(as.data.frame(words))
words = (strsplit(sentence, " "))
words = str_split(sentence, " ")
words = unlist(str_split(sentence, " "))
words <- clean.text(as.data.frame(words))
words = str_split(sentence, " ")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
flog.debug(paste("predictor --> sentence =", paste(words, collapse = ''), "length=", length(words)))
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
convert.to.string <- function(df) {     x <- sapply(df, as.character)     x }
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
words[length(words) - 4:length(words)]
words[length(words) - 4:length(words)]()
xxx <- words[length(words) - 4:length(words)]
xxx
xxx()
unlist(xxx)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
df.words <- words
ngram <- 5
    df.words <- as.data.table(df.words)
    df.words$index <- seq.int(nrow(df.words))
df.words <- sqldf(paste("select * from df.words where index > ", length(df.words) - ngram))
df.words$index <- as.numeric(seq.int(nrow(df.words)))
df.words <- sqldf(paste("select * from df.words where index > ", length(df.words) - ngram))
df.words <- sqldf(paste("select * from [df.words] where index > ", length(df.words) - ngram))
 ngram <- 5     df.words <- words     df.words <- as.data.table(df.words)     df.words$id <- as.numeric(seq.int(nrow(df.words)))     df.words <- sqldf(paste("select * from [df.words] where id > ", length(df.words) - ngram))     arg <- convert.to.string(df.words)     flog.debug(paste("predictor sql", sql))     sql <- paste0("select * from n",ngram," where word like '", arg, "%' order by freq desc limit 10")     sql
  ngram <- 5
    df.words <- words
    df.words <- as.data.table(df.words)
    df.words$id <- as.numeric(seq.int(nrow(df.words)))
    df.words <- sqldf(paste("select * from [df.words] where id > ", length(df.words) - ngram))
 arg <- convert.to.string(df.words)
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
df.words <- sapply(df.words[1], clean.text)
df.words <- as.data.table(df.words)
    df.words$id <- as.numeric(seq.int(nrow(df.words)))
df.words <- sqldf(paste("select * from [df.words] where id > ", length(df.words) - ngram))
length(df.words)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
sqldf(create_sql(4, words))
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     words = unlist(str_split(sentence, " "))     words <- clean.text(as.data.frame(words, stringsAsFactors = FALSE))     sentence <- convert.to.string(words)        flog.debug(paste("predictor --> sentence-cleaned =", sentence))     # 1 word given --> lookup to see the next in the list ordered by most frequesnt     flog.debug("predictor --> trying n2")     if (length(words) == 1) sqldf(create_sql(2, words))     flog.debug("predictor --> trying n3")     # 2 words given      if (length(words) == 2) sqldf(create_sql(3, words))     flog.debug("predictor --> trying n4")     # 3 words given      if (length(words) == 3) sqldf(create_sql(4, words))     flog.debug("predictor --> trying n5+")     # 4 words given      if (length(words) >= 4) sqldf(create_sql(5, words))
 sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     words = unlist(str_split(sentence, " "))     words <- clean.text(as.data.frame(words, stringsAsFactors = FALSE))     sentence <- convert.to.string(words)        flog.debug(paste("predictor --> sentence-cleaned =", sentence))     # 1 word given --> lookup to see the next in the list ordered by most frequesnt     flog.debug("predictor --> trying n2")     if (length(words) == 1) sqldf(create_sql(2, words))     flog.debug("predictor --> trying n3")     # 2 words given      if (length(words) == 2) sqldf(create_sql(3, words))     flog.debug("predictor --> trying n4")     # 3 words given      if (length(words) == 3) sqldf(create_sql(4, words))     flog.debug("predictor --> trying n5+")     # 4 words given      if (length(words) >= 4) sqldf(create_sql(5, words))
flog.info(paste("predictor input", sentence))     if (is.na(sentence)) {         warning("sentence NA or empty")         stop()     }     sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     words = unlist(str_split(sentence, " "))     words <- clean.text(as.data.frame(words, stringsAsFactors = FALSE))     sentence <- convert.to.string(words)        flog.debug(paste("predictor --> sentence-cleaned =", sentence))     # 1 word given --> lookup to see the next in the list ordered by most frequesnt     flog.debug("predictor --> trying n2")     if (length(words) == 1) sqldf(create_sql(2, words))     flog.debug("predictor --> trying n3")     # 2 words given      if (length(words) == 2) sqldf(create_sql(3, words))     flog.debug("predictor --> trying n4")     # 3 words given      if (length(words) == 3) sqldf(create_sql(4, words))     flog.debug("predictor --> trying n5+")     # 4 words given      if (length(words) >= 4) sqldf(create_sql(5, words))
 if (length(words) == 1) sqldf(create_sql(2, words))
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
  sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     words = unlist(str_split(sentence, " "))     words <- clean.text(as.data.frame(words, stringsAsFactors = FALSE))     sentence <- convert.to.string(words)     flog.debug(paste("predictor --> sentence-cleaned =", sentence))     if (is.data.frame(df.words)) {         df.words <- sapply(df.words[1], clean.text)     }     df.words <- as.data.table(df.words)     df.words$id <- as.numeric(seq.int(nrow(df.words)))    # stop()     df.words <- sqldf(paste("select * from [df.words] where id > ", length(df.words) - ngram))     arg <- convert.to.string(df.words)     flog.debug(paste("predictor sql", sql))     sql <- paste0("select * from n",ngram," where word like '", arg, "%' order by freq desc limit 10")     sql
 ngram < 5     sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     words = unlist(str_split(sentence, " "))     words <- clean.text(as.data.table(words, stringsAsFactors = FALSE))     sentence <- convert.to.string(words)     flog.debug(paste("predictor --> sentence-cleaned =", sentence))     if (is.data.frame(df.words)) {         df.words <- sapply(df.words[1], clean.text)     }     df.words <- as.data.table(df.words)     df.words$id <- as.numeric(seq.int(nrow(df.words)))    # stop()     df.words <- sqldf(paste("select * from [df.words] where id > ", length(df.words) - ngram))     arg <- convert.to.string(df.words)     flog.debug(paste("predictor sql", sql))     sql <- paste0("select * from n",ngram," where word like '", arg, "%' order by freq desc limit 10")     sql
ngram < 5     sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     words = unlist(str_split(sentence, " "))     words <- clean.text(as.data.table(words, stringsAsFactors = FALSE))     sentence <- convert.to.string(words)     flog.debug(paste("predictor --> sentence-cleaned =", sentence))
ngram < 5     sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     words = unlist(str_split(sentence, " "))     words <- clean.text(as.data.table(words, stringsAsFactors = FALSE))     sentence <- convert.to.string(words)     flog.debug(paste("predictor --> sentence-cleaned =", sentence))
 if (is.data.frame(df.words)) {         df.words <- sapply(df.words[1], clean.text)     }
 ngram < 5     sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     words = unlist(str_split(sentence, " "))     words <- clean.text(as.data.table(words, stringsAsFactors = FALSE))     sentence <- convert.to.string(words)     flog.debug(paste("predictor --> sentence-cleaned =", sentence))     df.words <- words
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
 ngram < 5     sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     words = unlist(str_split(sentence, " "))     words <- clean.text(as.data.table(words, stringsAsFactors = FALSE))     sentence <- convert.to.string(words)     flog.debug(paste("predictor --> sentence-cleaned =", sentence))     df.words <- words
ngram <- 5
    sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     words = unlist(str_split(sentence, " "))     words <- clean.text(as.data.table(words, stringsAsFactors = FALSE))     sentence <- convert.to.string(words)     flog.debug(paste("predictor --> sentence-cleaned =", sentence))     df.words <- words
    if (is.data.frame(df.words)) {         df.words <- sapply(df.words[1], clean.text)     }
    df.words <- as.data.table(df.words)     df.words$id <- as.numeric(seq.int(nrow(df.words)))
df.words <- sqldf(paste("select * from [df.words] where id > ", length(df.words) - ngram))
result <- predictor("The guy in front of me just bought a pound of bacon, a bouquet, and a case of") result
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
require(tm)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
sentence <- clean.text(sentence)
    sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"
    ngram <- 5
    sentence <- clean.text(sentence)
words = str_split(sentence, " ")
  if (is.data.frame(df.words)) {         df.words <- sapply(df.words[1], clean.text)     }
    df.words <- words
    if (is.data.frame(df.words)) {         df.words <- sapply(df.words[1], clean.text)     }
    df.words <- as.data.table(df.words)
    df.words$id <- as.numeric(seq.int(nrow(df.words)))
    df.words <- sqldf(paste("select * from [df.words] where id > ", length(df.words) - ngram))
length(df.words)
nrow(df.words)
    df.words <- sqldf(paste("select * from [df.words] where id > ", nrow(df.words) - ngram))
  arg <- convert.to.string(df.words)
arg <- convert.to.string(df.words[1])
arg <- paste(df.words, collapse = '')
arg
arg <- as.character(paste(df.words, collapse = ''))
arg
class(arg)
arg()
arg[1]
unlist(arg)
arg <- toString(paste(df.words, collapse = ''))
arg
flog.debug(paste("predictor sql", sql))
sql <- paste0("select * from n", ngram, " where word like '", arg, "%' order by freq desc limit 10")
    flog.debug(paste("predictor sql", sql))
arg <- apply(df.words, 1, paste, collapse = "")
arg <- apply(df.words, 1, paste, collapse = "")
arg <- df.words %>% unite(V1,,sep = "")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
arg <- convert.to.string(df.words)
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
 arg <- convert.to.string(df.words)
arg <- convert.to.string(df.words$V1)
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
    arg <- convert.to.string(df.words$V1)
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
arg <- convert.to.string(df.words$V1)
convert.to.string <- function(df, row_count =nrow(df)) {     str <- paste(df[1, 1], df[1, 2], df[1, 3]) }
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
arg <- convert.to.string(df.words)
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
 arg <- convert.to.string(df.words)
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
arg <- convert.to.string(df.words)
arg <- str_trim(convert.to.string(df.words))
 ngram <- 5     sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     sentence <- clean.text(sentence)     words = str_split(sentence, " ")     words <- (as.data.table(words, stringsAsFactors = FALSE))     sentence <- convert.to.string(words)     flog.debug(paste("predictor --> sentence-cleaned =", sentence))     df.words <- words     if (is.data.frame(df.words)) {         df.words <- sapply(df.words[1], clean.text)     }     df.words <- as.data.table(df.words)     df.words$id <- as.numeric(seq.int(nrow(df.words)))    # stop()     df.words <- sqldf(paste("select * from [df.words] where id > ", nrow(df.words) - ngram))     arg <- str_trim(convert.to.string(df.words))     sql <- paste0("select * from n", ngram, " where word like '", arg, "%' order by freq desc limit 10")     flog.debug(paste("predictor sql", sql))     sql
 sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     sentence <- clean.text(sentence)
 words = str_split(sentence, " ")     words <- (as.data.table(words, stringsAsFactors = FALSE))     sentence <- convert.to.string(words)     flog.debug(paste("predictor --> sentence-cleaned =", sentence))
    df.words <- words
    df.words <- as.data.table(df.words)
    df.words$id <- as.numeric(seq.int(nrow(df.words)))
    df.words <- sqldf(paste("select * from [df.words] where id > ", nrow(df.words) - ngram))
    arg <- str_trim(convert.to.string(df.words))
sql <- paste0("select * from n", ngram, " where word like '", arg, "%' order by freq desc limit 10")
    flog.debug(paste("predictor sql", sql))
ngram <- 5     sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     sentence <- clean.text(sentence)     words = str_split(sentence, " ")     words <- (as.data.table(words, stringsAsFactors = FALSE))     sentence <- convert.to.string(words)     flog.debug(paste("predictor --> sentence-cleaned =", sentence))     df.words <- words     if (is.data.frame(df.words)) {         df.words <- sapply(df.words[1], clean.text)     }     df.words <- as.data.table(df.words)     df.words$id <- as.numeric(seq.int(nrow(df.words)))    # stop()     df.words <- sqldf(paste("select * from [df.words] where id > ", nrow(df.words) - ngram))     arg <- str_trim(convert.to.string(df.words))     sql <- paste0("select * from n", ngram, " where word like '%", arg, "%' order by freq desc limit 10")     flog.debug(paste("predictor --> sql =", sql))     sql
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
    ngram <- 5
    sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"
    flog.debug(paste("predictor --> sentence =", sentence))
    sentence <- clean.text(sentence)
    words = str_split(sentence, " ")
    words <- (as.data.table(words, stringsAsFactors = FALSE))
    sentence <- convert.to.string(words)
    flog.debug(paste("predictor --> sentence-cleaned =", sentence))
    df.words <- words
    if (is.data.frame(df.words)) {         df.words <- sapply(df.words[1], clean.text)     }
    df.words <- as.data.table(df.words)
    df.words$id <- as.numeric(seq.int(nrow(df.words)))
   # stop()
    df.words <- sqldf(paste("select * from [df.words] where id > ", nrow(df.words) - ngram))
    arg <- str_trim(convert.to.string(df.words))
df.words <- sqldf(paste("select * from [df.words] limit ", nrow(df.words) - ngram))
paste("select * from [df.words] limit ", nrow(df.words) - ngram)
sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     sentence <- clean.text(sentence)     words = str_split(sentence, " ")     words <- (as.data.table(words, stringsAsFactors = FALSE))     sentence <- convert.to.string(words)     flog.debug(paste("predictor --> sentence-cleaned =", sentence))     df.words <- words
    if (is.data.frame(df.words)) {         df.words <- sapply(df.words[1], clean.text)     }     df.words <- as.data.table(df.words)     df.words$id <- as.numeric(seq.int(nrow(df.words)))
    df.words <- sqldf(paste("select * from [df.words] limit ", nrow(df.words) - ngram))
sql <- paste("select * from [df.words] limit ", nrow(df.words) - ngram)
sql
  sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"
    flog.debug(paste("predictor --> sentence =", sentence))
    sentence <- clean.text(sentence)
words <- (as.data.table(words, stringsAsFactors = FALSE))
sentence <- convert.to.string(words)
    df.words <- words
    if (is.data.frame(df.words)) {         df.words <- sapply(df.words[1], clean.text)     }
sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     sentence <- clean.text(sentence)     words = str_split(sentence, " ")     words <- (as.data.table(words, stringsAsFactors = FALSE))     sentence <- convert.to.string(words)     flog.debug(paste("predictor --> sentence-cleaned =", sentence))     df.words <- words     sql <- paste("select * from [df.words] limit ", nrow(df.words) - ngram)     sql     df.words <- sqldf(sql)     arg <- str_trim(convert.to.string(df.words))     sql <- paste0("select * from n", ngram, " where word like '%", arg, "%' order by freq desc limit 10")     flog.debug(paste("predictor --> sql =", sql))     sql
   sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     sentence <- clean.text(sentence)     words = str_split(sentence, " ")     words <- (as.data.table(words, stringsAsFactors = FALSE))     sentence <- convert.to.string(words)     flog.debug(paste("predictor --> sentence-cleaned =", sentence))     df.words <- words     sql <- paste("select * from [df.words] limit ", nrow(df.words) - ngram)     sql     df.words <- sqldf(sql)     arg <- str_trim(convert.to.string(df.words))     sql <- paste0("select * from n", ngram, " where word like '%", arg, "%' order by freq desc limit 10")     flog.debug(paste("predictor --> sql =", sql))     sql
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
load(get_data_file_path(ngram_file_name))
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
  # 1 word given --> lookup to see the next in the list ordered by most frequesnt     flog.debug("predictor --> trying n2")     if (length(words) == 1) sqldf(create_sql(2, words))     flog.debug("predictor --> trying n3")     # 2 words given      if (length(words) == 2) sqldf(create_sql(3, words))     flog.debug("predictor --> trying n4")     # 3 words given      if (length(words) == 3) sqldf(create_sql(4, words))     flog.debug("predictor --> trying n5+")     # 4 words given      if (length(words) >= 4) sqldf(create_sql(5, words))
result <- predictor("The guy in front of me just bought a pound of bacon, a bouquet, and a case of") result
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"
    flog.debug(paste("predictor --> sentence =", sentence))
    sentence <- clean.text(sentence)
flog.debug(paste("predictor --> sentence.cleaned =", sentence))
    words = str_split(sentence, " ")
words <- as.data.table(words, stringsAsFactors = FALSE)
    if (length(words) == 1) sqldf(create_sql(2, words))
length(words) == 1
if (nrow(words) == 1) sqldf(create_sql(2, words))
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     sentence <- clean.text(sentence)     flog.debug(paste("predictor --> sentence.cleaned =", sentence))     words = str_split(sentence, " ")     words <- as.data.table(words, stringsAsFactors = FALSE)
if (nrow(words) >= 4) sqldf(create_sql(5, words))
ngram <- 5
    df.words <- words
    sql <- paste("select * from [df.words] limit ", nrow(df.words) - ngram)
    flog.debug(paste("predictor --> sql words =", sql))
    df.words <- sqldf(sql)
sql <- paste("select * from [df.words] limit ", ((nrow(df.words)) - nrow(df.words) - ngram))
    flog.debug(paste("predictor --> sql words =", sql))
sql <- paste("select * from [df.words] limit ", ngram)
    flog.debug(paste("predictor --> sql words =", sql))
    df.words <- sqldf(sql)
    arg <- str_trim(convert.to.string(df.words))
convert.to.string <- function(df, row_count =nrow(df)) {     str <- ""     for (i in 1:row_count) {         str <- paste(str, df[row_count,1])     }     str }
convert.to.string <- function(df, row_count =nrow(df)) {     str <- ""     for (i in 1:row_count) {         str <- paste(str, df[i,1])     }     str }
ngram <- 5     df.words <- words     sql <- paste("select * from [df.words] limit ", ngram)     flog.debug(paste("predictor --> sql words =", sql))     df.words <- sqldf(sql)     arg <- str_trim(convert.to.string(df.words))     sql <- paste0("select * from n", ngram, " where word like '%", arg, "%' order by freq desc limit 10")     flog.debug(paste("predictor --> sql =", sql))
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
if (nrow(df.any)>=0) df.any
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_1__setup.r", echo = TRUE, encoding = "Windows-1252")
p_load("tidyverse")
source("C:/dev/r-course/10-capstone/project_1__setup.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_1__setup.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_1__setup.r", echo = TRUE, encoding = "Windows-1252")
install.packages("tidyverse")
source("C:/dev/r-course/10-capstone/project_1__setup.r", echo = TRUE, encoding = "Windows-1252")
install.packages("colorspace", lib="C:/Users/chris/OneDrive/Documents/R/win-library/3.3")
source("C:/dev/r-course/10-capstone/project_1__setup.r", echo = TRUE, encoding = "Windows-1252")
update.packages()
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__ngram_generation.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
p_load("stringr")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
stopifnot(nrow(df.words) > 0)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
   ngram <- 5        df.words <- words     df.any <- create_sql(ngram, df.words)     df.any <- as.data.table(df.any)     if (nrow(df.any)) df.any     flog.debug(paste("try_next_ngram --> no results found trying next ngram:",ngram-1))     if (ngram > 1) {         # no results found - try next ngram with 1 less word         sql <- paste("select * from [df.words] limit ", ngram - 1)         flog.debug(paste("try_next_ngram --> sql words =", sql))         df.words <- sqldf(sql)         try_next_ngram(ngram - 1, df.words)     }     df.any
    ngram <- 5        df.words <- words
    df.any <- create_sql(ngram, df.words)
ngram <- 5     df.words <- words     stopifnot(nrow(df.words) > 0)     df.any <- create_sql(ngram, df.words)     df.any <- as.data.table(df.any)     if (nrow(df.any)) df.any     flog.debug(paste("try_next_ngram --> no results found trying next ngram:",ngram-1))     if (ngram > 1) {         # no results found - try next ngram with 1 less word         sql <- paste("select * from [df.words] limit ", ngram - 1)         flog.debug(paste("try_next_ngram --> sql words =", sql))         df.words <- sqldf(sql)         try_next_ngram(ngram - 1, df.words)     }     df.any
   df.any <- create_sql(ngram, df.words)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
 words = str_split(sentence, " ")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
  sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"
    words = str_split(sentence, " ")
 sentence <- clean.text(sentence)
    words = str_split(sentence, " ")
    words <- as.data.table(words, stringsAsFactors = FALSE)
word_count <- nrow(words)
to <- nrow(words)     from <- to - ngram
www <- words[, from:to]
 sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     sentence <- clean.text(sentence)     flog.debug(paste("predictor --> sentence.cleaned =", sentence))     words = str_split(sentence, " ")     words <- as.data.table(words, stringsAsFactors = FALSE)
to <- nrow(words)
from <- to - ngram
    www <- words[, from:to]
www
sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     sentence <- clean.text(sentence)     flog.debug(paste("predictor --> sentence.cleaned =", sentence))
    words = str_split(sentence, " ")
    words <- as.data.table(words, stringsAsFactors = FALSE)
    to <- nrow(words)
    from <- to - ngram
    www <- words[from:to]
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
 sentence <- "The guy in front of me just bought a pound of bacon, a bouquet, and a case of"     flog.debug(paste("predictor --> sentence =", sentence))     sentence <- clean.text(sentence)     flog.debug(paste("predictor --> sentence.cleaned =", sentence))     words = str_split(sentence, " ")     words <- as.data.table(words, stringsAsFactors = FALSE)
 sqldf(try_next_ngram(5, words))
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
   ngram <- 5        df.words <- words     arg <- str_trim(convert.to.string(take_words(df.words, ngram - 1)))     sql <- paste0("select * from n", ngram, " where word like '%", arg, "%' order by freq desc limit 10")     flog.debug(paste("predictor --> create_sql --> sql ngram =", sql))
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
www <- take_words(df.words, ngram - 1)
wwwwww take_words <- function(df.words, word_count_to_take) {     to <- nrow(df.words)     from <- to - word_count_to_take     chopped <- df.words[from:to]     flog.debug(chopped)     chopped }
    ngram <- 5        df.words <- words     www <- take_words(df.words, ngram - 1)     arg <- str_trim(convert.to.string(take_words(df.words, ngram - 1)))     sql <- paste0("select * from n", ngram, " where word like '%", arg, "%' order by freq desc limit 10")     flog.debug(paste("predictor --> create_sql --> sql ngram =", sql))     sql
www <- take_words(df.words, ngram - 1)     www
 arg <- str_trim(convert.to.string(take_words(df.words, ngram - 1)))
arg
arg <- str_trim(convert.to.string(take_words(df.words, ngram - 1)[1]))
    arg
www
chopped <- take_words(df.words, ngram - 1)$V1
 arg <- str_trim(convert.to.string()))
arg <- str_trim(convert.to.string())
arg <- str_trim(convert.to.string(chopped))
chopped <- take_words(df.words, ngram - 1)
arg <- str_trim(convert.to.string(chopped))
chopped <- unlist(take_words(df.words, ngram - 1))
 ngram <- 5        df.words <- words
chopped <- take_words(df.words, ngram - 1)
chopped[2]
chopped$V1
arg <- str_trim(convert.to.string(chopped$V1))
chopped <- take_words(df.words, ngram - 1)$V1    
chopped <- as.character(take_words(df.words, ngram - 1)$V1    )
arg <- str_trim(convert.to.string(chopped$V1))
arg <- str_trim(convert.to.string(chopped))
arg <- str_trim(chopped)
sql <- paste0("select * from n", ngram, " where word like '%", arg, "%' order by freq desc limit 10")
    flog.debug(paste("predictor --> create_sql --> sql ngram =", sql))
    chopped <- as.character(take_words(df.words, ngram - 1)$V1    )     for (i in 1:nrow(chopped)) {         arg <- paste(arg, chopped[i, 1])     }
nrow(chopped)
chopped
chopped <- as.character(take_words(df.words, ngram - 1))
    for (i in 1:nrow(chopped)) {         arg <- paste(arg, chopped[i, 1])     }
chopped <- take_words(df.words, ngram - 1)
 for (i in 1:nrow(chopped)) {         arg <- paste(arg, chopped[i, 1])     }
 for (i in 1:nrow(chopped)) {         arg <- paste(arg, chopped$V1[i])     }
str <- ""     arg <- sapply(chopped, FUN = function(x) { paste(str,x)})
  chopped <- take_words(df.words, ngram - 1)     str <- ""     arg <- sapply(chopped, FUN = function(x) { paste(str,x)})
    chopped <- take_words(df.words, ngram - 1)
    str <- ""
    arg <- sapply(chopped, FUN = function(x) { paste(str,x)})
    df.chopped <- take_words(df.words, ngram - 1)     arg <- paste(df.chopped, sep = ""))
arg <- paste(df.chopped, sep = "")
arg
arg <- as.character(paste(df.chopped, sep = ""))
arg
class(arg)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
    df.chopped <- take_words(df.words, ngram - 1)     arg <- ""     for (i in 1:nrow(df.chopped)) {        arg <- paste(arg, df.chopped$V1[i])     }
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
  data.all <- clean.text.parrallel(df.text = data.all)
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
 data.all <- parallelize_task_chunked(sent_detect, data.all) #Detect and split sentences on endmark boundaries.
data.all <- parallelize_task(sent_detect, data.all) #Detect and split sentences on endmark boundaries.
data.all <- clean.text.parrallel(df.text = data.all)
data.all <- clean.text(df.text = data.all)
data.all <- clean.text(data.all)
 save(data.all, file = get_data_file_path("data.all.RData"))
   data.all <- parallelize_task_chunked(rm_non_words, data.all) # Remove Non-Words & N Character Words
 data.all <- parallelize_task(sent_detect, data.all) #Detect and split sentences on endmark boundaries.     data.all <- clean.convert_to_and(data.all)     data.all <- clean.convert_to_period(data.all)     data.all <- clean.remove_symbols(data.all)     data.all <- clean.reduce_periods(data.all)     data.all <- clean.replace_numbers(data.all)     data.all <- removePunctuation(data.all, preserve_intra_word_dashes = TRUE)     data.all <- rm_non_words(data.all) # Remove Non-Words & N Character Words     data.all <- stripWhitespace(data.all)     data.all <- tolower(data.all)     flog.info("data cleaning complete...")
 save(data.all, file = get_data_file_path("data.all.RData"))
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
ngram <- 5     df.words <- words
   df.chopped <- take_words(df.words, ngram-1)     arg <- ""
    ngram <- 5
    df.words <- words
    df.chopped <- take_words(df.words, ngram-1)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
 if (ngram <= 1) NULL     ngram <- 5     df.words <- words     df.chopped <- take_words(df.words, ngram-1)     arg <- ""     for (i in 1:nrow(df.chopped)) {        arg <- paste(arg, df.chopped$V1[i])     }     arg <- str_trim(arg)     sql <- paste0("select * from n", ngram, " where word like '%", arg, "%' order by freq desc limit 10")     flog.debug(paste("predictor --> create_sql --> sql ngram =", sql))     sql
ngram <- 5     df.words <- words
df.chopped <- take_words(df.words, ngram-1)
 flog.info(paste("predictor input", sentence))     if (is.na(sentence)) {         warning("sentence NA or empty")         stop()     }     sentence <- "a couple of weeks ago"     flog.debug(paste("predictor --> sentence =", sentence))     sentence <- clean.text(sentence)     flog.debug(paste("predictor --> sentence.cleaned =", sentence))     words = str_split(sentence, " ")     words <- as.data.table(words, stringsAsFactors = FALSE)
 ngram <- 5     df.words <- words
    df.chopped <- take_words(df.words, ngram-1)     arg <- ""
    for (i in 1:nrow(df.chopped)) {        arg <- paste(arg, df.chopped$V1[i])     }
    arg <- str_trim(arg)
sql <- paste0("select * from n", ngram, " where word like '%", arg, "%' order by freq desc limit 10")
 df.any <- create_sql(ngram, df.words)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
search_ngram <- function(ngram, df.words) {     #ngram <- 5     #df.words <- words     stopifnot(nrow(df.words) > 0)     df.any <- create_sql(ngram, df.words)     flog.debug(paste("predictor --> search_ngram --> no results found trying next ngram:",ngram-1))     if (ngram > 1) {         # no results found - try next ngram with 1 less word         search_ngram(ngram - 1, df.words)     }     df.any }
take_words <- function(df.words, word_count_to_take) {     to <- nrow(df.words)     from <- to - word_count_to_take     chopped <- df.words[from:to]     flog.debug(chopped)     chopped }
sentence <- "a couple of weeks ago"
    sentence <- clean.text(sentence)
    words = str_split(sentence, " ")
    words <- as.data.table(words, stringsAsFactors = FALSE)
        sqldf(search_ngram(5, words))
        sqldf(search_ngram(4, words))
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
    sentence <- "a couple of weeks"
    flog.debug(paste("predictor --> sentence =", sentence))
    sentence <- clean.text(sentence)
    flog.debug(paste("predictor --> sentence.cleaned =", sentence))
    words = str_split(sentence, " ")
    words <- as.data.table(words, stringsAsFactors = FALSE)
sqldf(search_ngram(5, words))
sqldf("select * from n5 where word like '%a couple of weeks%' order by freq desc limit 10")
sqldf("select * from n4 where word like '%a couple of weeks%' order by freq desc limit 10")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
  sentence <- "a couple of weeks"     flog.debug(paste("predictor --> sentence =", sentence))     sentence <- clean.text(sentence)     flog.debug(paste("predictor --> sentence.cleaned =", sentence))     words = str_split(sentence, " ")     words <- as.data.table(words, stringsAsFactors = FALSE)     if (nrow(words) >= 5) {            words <- take_words(words=words, 5)     }
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
arg <- ""     for (i in 1:nrow(df.chopped)) {         arg <- paste(arg, df.chopped$V1[i])     }
   arg <- str_trim(arg)     sql <- paste0("select * from n", ngram, " where word like '%", arg, "%' order by freq desc limit 10")     flog.debug(paste("predictor --> create_sql --> sql ngram =", sql))
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
sqldf(search_ngram(words))
ngram <- row_count - 1     df.chopped <- take_words(df.words, ngram - 1)
    df.any <- create_sql(ngram, df.words)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
 sqldf(search_ngram(words))
df.words <- words
row_count <- nrow(df.words)
    stopifnot(row_count > 0)
    df.words <- take_words(df.words, ngram)
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
to <- nrow(df.words)
    if (word_count_to_take >= to) {         df.words     }
    from <- to - word_count_to_take
from <- to - ngram
to
from
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
row_count <- nrow(df.words)     stopifnot(row_count > 0)     df.words <- take_words(df.words, ngram)     df.any <- create_sql(ngram, df.words)     flog.debug(paste("predictor --> search_ngram --> no results found trying next ngram:",ngram-1))     if (ngram > 1) {         # no results found - try next ngram with 1 less word         df.words <- take_words(df.words, ngram-1)         search_ngram(df.words)     }
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
 row_count <- nrow(df.words)     stopifnot(row_count > 0)     df.words <- take_words(df.words, ngram)     df.any <- create_sql(ngram, df.words)     flog.debug(paste("predictor --> search_ngram --> no results found trying next ngram:",ngram-1))     if (ngram > 1) {         # no results found - try next ngram with 1 less word         df.words <- take_words(df.words, ngram-1)         search_ngram(df.words)     }     df.any
df.words <- take_words(df.words, ngram)
    df.words <- take_words(df.words, ngram)     df.words <- create_sql(ngram, df.words)
  df.result <- sqldf(create_sql(ngram, df.words))
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
df.searchterms <- take_words(words, row_count)
    arg <- ""     for (i in ngram - 1:nrow(df.searchterms)) {         arg <- paste(arg, df.searchterms$V1[i])     }
 arg <- str_trim(arg)     sql <- paste0("select * from n", ngram, " where word like '%", arg, "%' order by freq desc limit 10")     flog.debug(paste("predictor --> create_search_ngram --> sql ngram =", sql))
    df.result <- sqldf(sql)
 if (nrow(df.result) == 0 && row_count > 1) {         # no results found - try next ngram with 1 less word         flog.debug(paste("predictor --> search_ngram --> no results found trying next ngram:", row_count-1))         df.searchterms <- take_words(df.searchterms, row_count - 1)         #search_ngram(df.searchterms)     }
 df.searchterms <- take_words(words, row_count)     arg <- ""     for (i in ngram - 1:nrow(df.searchterms)) {         arg <- paste(arg, df.searchterms$V1[i])     }     arg <- str_trim(arg)     sql <- paste0("select * from n", ngram, " where word like '%", arg, "%' order by freq desc limit 10")     flog.debug(paste("predictor --> create_search_ngram --> sql ngram =", sql))     df.result <- sqldf(sql)     if (nrow(df.result) == 0 && row_count > 1) {         flog.debug(paste("predictor --> search_ngram --> no results found trying next ngram:", row_count-1))         df.searchterms <- take_words(df.searchterms, row_count - 1)         search_ngram(df.searchterms)     }     df.result
take = 5
    df.searchterms <- take_words(words, take)
    arg <- ""     for (i in ngram - 1:nrow(df.searchterms)) {         arg <- paste(arg, df.searchterms$V1[i])     }
    arg <- str_trim(arg)
    sql <- paste0("select * from n", ngram, " where word like '%", arg, "%' order by freq desc limit 10")
    flog.debug(paste("predictor --> create_search_ngram --> sql ngram =", sql))
    df.result <- sqldf(sql)
    if (nrow(df.result) == 0 && take > 1) {         flog.debug(paste("predictor --> search_ngram --> no results found trying next ngram:", take - 1))         df.searchterms <- take_words(df.searchterms, take - 1)         search_ngram(df.searchterms, (take-1))     }
take_next <- take-1
        take_next <- take-1
        flog.debug(paste("predictor --> search_ngram --> no results found trying next ngram:", take_next))
        df.searchterms <- take_words(df.searchterms, take_next)
        search_ngram(df.searchterms, take_next)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
df.searchterms <- take_words(words, take)
arg <- str_trim(arg)
sql <- paste0("select * from n", take, " where word like '%", arg, "%' order by freq desc limit 10")
    flog.debug(paste("predictor --> create_search_ngram --> sql ngram =", sql))
df.result <- sqldf(sql)
search_ngram(df.searchterms, take_next)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
gc()
reqiup_load("foreach")
p_load("foreach")
    arg <- ""     foreach(i = 1:take) %do%         arg <- paste(df.searchterms[i])
foreach(i = 1:take) %do%  arg <- paste(df.searchterms[i])
require(foreach)
foreach(i = 1:take) %do%  arg <- paste(df.searchterms[i])
foreach(i = 1:take) %do% arg <- paste(df.searchterms[i])
foreach(i = 1:take) %do%         arg <- paste(df.searchterms[i])
using("foreach")
 arg <- ""     foreach(i = 1:take) %do%         arg <- paste(df.searchterms[i])
 df.searchterms <- take_words(words, take)
    arg <- ""     foreach(i = 1:take) %do%         arg <- paste(df.searchterms[i])
arg <- ""     foreach(i = 1:take) %do%         arg <- paste(df.searchterms[take])
 arg <- ""     foreach(i=1:take) %do%         arg <- paste(df.searchterms[take])
    if (row_count == 2) arg <- paste(df.searchterms[1], df.searchterms[2])     if (row_count == 3) arg <- paste(df.searchterms[1], df.searchterms[2], df.searchterms[3])     if (row_count == 4) arg <- paste(df.searchterms[1], df.searchterms[2], df.searchterms[3], df.searchterms[4])     if (row_count == 5) arg <- paste(df.searchterms[1], df.searchterms[2], df.searchterms[3], df.searchterms[4], df.searchterms[5])
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
  df.searchterms <- take_words(words, take)     if (row_count == 2) arg <- paste(df.searchterms[1], df.searchterms[2])     if (row_count == 3) arg <- paste(df.searchterms[1], df.searchterms[2], df.searchterms[3])     if (row_count == 4) arg <- paste(df.searchterms[1], df.searchterms[2], df.searchterms[3], df.searchterms[4])     if (row_count == 5) arg <- paste(df.searchterms[1], df.searchterms[2], df.searchterms[3], df.searchterms[4], df.searchterms[5])
    arg <- str_trim(arg)
    sql <- paste0("select * from n", take, " where word like '%", arg, "%' order by freq desc limit 10")
    flog.debug(paste("predictor --> search_ngram --> sql ngram =", sql))
    df.result <- sqldf(sql)
    if (nrow(df.result) == 0 && take > 1) {         take_next <- take-1         df.searchterms <- take_words(df.searchterms, take_next)         search_ngram(df.searchterms, take_next)     }
  arg <- str_trim(arg)     sql <- paste0("select * from n", take, " where word like '%", arg, "%' order by freq desc limit 10")     flog.debug(paste("predictor --> search_ngram --> sql ngram =", sql))     df.result <- sqldf(sql)     if (nrow(df.result) == 0 && take > 1) {         take_next <- take-1         df.searchterms <- take_words(df.searchterms, take_next)         search_ngram(df.searchterms, take_next)     }     df.result
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
 flog.debug(paste("take",take))     df.searchterms <- take_words(words, take)
 arg <- ""     counter <- 1     for (item in df.searchterms) {         if (counter > take) {             arg <- paste(arg, item$V1[counter])             counter <- counter+1         }     }
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
 df.searchterms <- take_words(words, take)
    arg <- ""
    counter <- 1
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
 flog.debug(paste("take",take))     df.searchterms <- take_words(words, take)
    arg <- ""
    counter <- 1
    for (item in df.searchterms) {         arg <- paste(arg, item$V1[1])     }
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
  df.searchterms <- take_words(words, take)
    df.searchterms <- take_words(words, take)
    arg <- ""
    counter <- 1
    for (item in df.searchterms) {         if (counter > take) {             arg <- paste(arg, item$V1[counter])         }         counter <- counter + 1         next     }
arg <- paste(df.searchterms$V1[1], df.searchterms$V1[2], df.searchterms$V1[3], df.searchterms$V1[4], df.searchterms$V1[5])
  arg <- sapply(df.searchterms$V1, function(x)         {             paste(x)         })
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
arg <- paste(df.searchterms$V1, sep = " ", collapse = NULL)
arg <- paste(df.searchterms$V1, sep = " ",collapse = "")
arg <- paste(df.searchterms$V1, sep = " ",collapse = " ")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
sqldf("select * from n4 where word like '%a couple of weeks%'")
df.result <- words[0,]
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
df.result <- as.data.table(ngram = character(0), word = character(0),freq = integer(0), length=integer(0))
df.result <- as.data.table(ngram = character(0), word = character(0),freq = integer(0), length=integer(0)) )
df.result <- as.data.table(ngram = character(0), word = character(0),freq = integer(0), length=integer(0)) 
df.result <- as.data.frame(ngram = character(0), word = character(0), freq = integer(0), length = integer(0))
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
  take <- nrow(words)
 while (take > 1) {         df.searchterms <- take_words(df.searchterms, take)         df.searchterms         take <- take-1     }
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
df.result <- data.table()
dt.result <- data.table()
dt.result <- c("ngram", "word", "freq")
dt.result[nrow(dt.result) + 1,] = list(take, result[1], result[2])
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
dt.result <- data.table(key = "ngram", stringsAsFactors = FALSE)
dt.result <- data.table(key = "ngram", stringsAsFactors = FALSE)     dt.result <- c("ngram", "word", "freq")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
print(result)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
dt.result <- data.table(stringsAsFactors = FALSE)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
rm(dt.result)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
 dt.result <- data.table()
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
 dt.result <- c("ngram", "word", "freq", "length")
    sentence <- "a couple of weeks"
    flog.debug(paste("predictor --> sentence =", sentence))
    sentence <- clean.text(sentence)
    flog.debug(paste("predictor --> sentence.cleaned =", sentence))
    dt.words = as.data.table(str_split(sentence, " "),stringsAsFactors = FALSE)
    take <- nrow(dt.words)
    dt.result <- c("ngram", "word", "freq", "length")
    while (take > 1) {         result <- search_ngram(words, take)         print(result)         if (nrow(result) > 0) {             flog.debug(paste("predictor --> ngram ", take, "found", nrow(result)))             df.row <- sqldf(paste("select word, freq, length from result"))             print(df.row)             dt.result <- rbind(dt.result, data.frame(ngram = take, word = df.row[1], freq = df.row[2], length = df.row[3]))         }         take <- take-1     }
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
<<<<<<< HEAD
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
sentence <- "a couple of weeks"
    sentence <- clean.text(sentence)
dt.search.terms = as.data.table(str_split(sentence, " "), stringsAsFactors = FALSE)
take <- nrow(dt.search.terms)
    dt.search.result <- c("ngram", "word", "freq", "length")     while (take > 1) {         result <- search_ngram(take)         #print(result)         if (nrow(result) > 0) {             flog.debug(paste("predictor --> ngram ", take, "found", nrow(result)))             df.row <- sqldf(paste("select word, freq, length from result"))             #print(df.row)             dt.search.result <- rbind(dt.search.result, data.frame(ngram = take, word = df.row[1], freq = df.row[2], length = df.row[3]))         }         take <- take-1     }
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
 sentence <- "a couple of weeks"     flog.debug(paste("predictor --> sentence =", sentence))     sentence <- clean.text(sentence)     flog.debug(paste("predictor --> sentence.cleaned =", sentence))     dt.search.terms = as.data.table(str_split(sentence, " "), stringsAsFactors = FALSE)     term_count <- nrow(dt.search.terms)
term_count <- 3
    flog.debug(paste("term_count",term_count))
    arg <- paste(dt.search.terms$V1, sep = " ",collapse = " ")
    arg <- str_trim(arg)
    if (arg=="") stop("arg empty")
    print(arg)
    sql <- paste0("select * from n", term_count, " where word like '%", arg, "%' order by freq desc limit 10")
    flog.debug(paste("predictor --> search_ngram --> sql ngram =", sql))
    df.result <- sqldf(sql)
    df.result
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
print(search_terms)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
dt.search.terms = as.data.table(str_split(sentence, " "), stringsAsFactors = FALSE)
predictor("a basket")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
predictor("a basket")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
predictor("know if")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
print(term_count)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
search <- "is one of the"
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
f <- predict.word(search,result)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
#' takes a dt.search.result data table and outputs the top 5 words predict.word <- function(txt,dt.pred) {     words <-sapply(dt.pred$word,gsub(txt, ""))     words } search <- "is one of the" result <- predictor(search) f <- predict.word(search,result)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
words <- lapply(dt.pred, function(y) gsub(txt, "", y[2])   f <- predict.word(search,result) f <- predict.word(search,result)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
`%not in%` <- function(x, table) is.na(match(x, table, nomatch = NA_integer_))
'%not in%' <- function(x, table) is.na(match(x, table, nomatch = NA_integer_))
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
   dt.search.result <- c("ngram", "word", "freq", "length","predicted")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
replace_search_terms <- function(x) {     for (word in strsplit(search, " ")) {         x <- str_replace(x, word, " ")     }     x }
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
dt.search.result$predicted_word <- dt.search.result$word
dt.search.result$predicted_word <- dt.search.result$word
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
search <- "is one of the" search <- clean.text(search) result <- predictor(search) result
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
search <- "is one of the" search <- clean.text(search) result <- predictor(search) result #f <- predict.word(search,result)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
search <- "is one of the" search <- clean.text(search) result <- predictor(search) result #f <- predict.word(search,result)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
search <- "is one of the" search <- clean.text(search) result <- predictor(search) result #f <- predict.word(search,result)
for (a in 1:length(dt.search.result$predicted_word)) {         for (b in 1:length(dt.search.terms)) {
            print(dt.search.terms[b])
            dt.search.result$predicted_word[a] = gsub(dt.search.terms$V1[b],"",(dt.search.result$predicted_word[a]))
        }     }
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
st <- as.data.table(str_split(search, " "), stringsAsFactors = FALSE)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
st <- as.data.table(str_split(search, " "), stringsAsFactors = FALSE) for (a in 1:length(st)) {     for (b in 1:length(st)) {         print(st[b])         result[a, 4] = gsub(st$V1[b], "", (result[a, 4]))     } }
result
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
search <- "is one of the" search <- clean.text(search) result <- predictor(search)
st <- as.data.table(str_split(search, " "), stringsAsFactors = FALSE) for (a in 1:length(st)) {     for (b in 1:length(st)) {         print(st[b])         result[a, 4] = gsub(st$V1[b], "", (result[a, 4]))     } }
result
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
search <- "is one of the" search <- clean.text(search) result <- predictor(search) result
result
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
st <- as.data.table(str_split(search, " "), stringsAsFactors = FALSE) for (a in 1:length(result$predicted)) {     for (b in 1:length(st)) {         print(st[b])         result[a, 4] = gsub(st$V1[b], "", (result[a, 4]))     } }
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
search <- "is one of the" search <- clean.text(search) result <- predictor(search)
st <- as.data.table(str_split(search, " "), stringsAsFactors = FALSE)
for (a in 1:length(result$predicted)) {     for (b in 1:length(st)) {         result$predicted[a] = str_replace_all(result$predicted[a], st$V1[b],"")     } }
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
search <- "is one of the" search <- clean.text(search) result <- predictor(search)
st <- as.data.table(str_split(search, " "), stringsAsFactors = FALSE) for (a in 1:length(result$predicted)) {     for (b in 1:length(st)) {         result$predicted[a] = str_replace_all(result$predicted[a], st$V1[b],"")     } }
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
search <- "is one of the" search <- clean.text(search) result <- predictor(search)
dt.search.terms = as.data.table(str_split(sentence, " "), stringsAsFactors = FALSE)
dt.search.terms = as.data.table(str_split(search, " "), stringsAsFactors = FALSE)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
search <- "is one of the" search <- clean.text(search) dt.search.terms = as.data.table(str_split(search, " "), stringsAsFactors = FALSE) result <- predictor(search)
result$predicted <- lapply(result$predicted, function(x) {     x <- lapply(dt.search.terms, function(y) str_replace_all(x, y, '-')) })
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
search <- "is one of the" search <- clean.text(search) dt.search.terms = as.data.table(str_split(search, " "), stringsAsFactors = FALSE) result <- predictor(search)
result$predicted <- lapply(result$predicted, function(x) {    lapply(dt.search.terms, function(y) l(x, y, ' ')) })
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
foreach(i = 1:nrow(result$predicted), .combine = rbind) %dopar%
    result$predicted[i,] <- "hi"
foreach(i = 1:nrow(result), .combine = rbind) %dopar%
    result$predicted[i,] <- "hi"
foreach(i = 1:nrow(result), .combine = rbind) %dopar%
    print(1)
result$predicted <- lapply(result$predicted, function(x) {     # lapply(na.omit(dt.search.terms), function(y) str_replace_all(x, y, ' '))     print(x) })
result$predicted <- lapply(result$predicted, function(x) {     # lapply(na.omit(dt.search.terms), function(y) str_replace_all(x, y, ' '))     for (i in 1:nrow(dt.search.terms)) {         x <- str_replace_all(x, dt.search.terms[i], "x")        next     } })
result$predicted <- lapply(result$predicted, function(x) {     # lapply(na.omit(dt.search.terms), function(y) str_replace_all(x, y, ' '))     for (i in 1:nrow(dt.search.terms)) {         x <- str_replace_all(x, dt.search.terms[i], "x")         x     } })
search <- "is one of the" search <- clean.text(search) dt.search.terms = as.data.table(str_split(search, " "), stringsAsFactors = FALSE) result <- predictor(search)
result$predicted <- lapply(result$predicted, function(x) {     # lapply(na.omit(dt.search.terms), function(y) str_replace_all(x, y, ' '))     for (i in 1:nrow(dt.search.terms)) {         x <- str_replace_all(x, dt.search.terms[i], "x")         x     } })
search <- "is one of the" search <- clean.text(search) dt.search.terms = as.data.table(str_split(search, " "), stringsAsFactors = FALSE) result <- predictor(search)
result$predicted <- lapply(result$predicted, function(x) {     # lapply(na.omit(dt.search.terms), function(y) str_replace_all(x, y, ' '))     for (i in 1:nrow(dt.search.terms)) {         x <- gsub(dt.search.terms[i], "",x, perl = TRUE)         x            } })
search <- "is one of the" search <- clean.text(search) dt.search.terms = as.data.table(str_split(search, " "), stringsAsFactors = FALSE) result <- predictor(search)
result$predicted <- lapply(result$predicted, function(x) {     # lapply(na.omit(dt.search.terms), function(y) str_replace_all(x, y, ' '))     for (i in 1:nrow(dt.search.terms)) {         x <- gsub(dt.search.terms[i], "",x, perl = TRUE)         print(x)     } })
for (r in 1:nrow(result$predicted)) {     for (i in 1:nrow(dt.search.terms)) {         x <- gsub(dt.search.terms[i], "", x, perl = TRUE)         result$predicted[r] <- x     } }
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
search <- "is one of the" search <- clean.text(search) dt.search.terms = as.data.table(str_split(search, " "), stringsAsFactors = FALSE) result <- predictor(search)
for (r in 1:nrow(result$predicted)) {     for (i in 1:nrow(dt.search.terms)) {         x <- gsub(dt.search.terms[i], "", x, perl = TRUE)         result$predicted[r] <- x     } }
search <- "is one of the" search <- clean.text(search) dt.search.terms = as.data.table(str_split(search, " "), stringsAsFactors = FALSE) result <- predictor(search)
for (r in 1:nrow(result$predicted)) {     for (i in 1:nrow(dt.search.terms)) {         x <- gsub(dt.search.terms[i], "", x, perl = TRUE)         print(x)         result$predicted[r] <- x     } }
search <- "is one of the" search <- clean.text(search) dt.search.terms = as.data.table(str_split(search, " "), stringsAsFactors = FALSE) result <- predictor(search)
for (r in 1:length(result$predicted)) {     for (i in 1:nrow(dt.search.terms)) {         x <- gsub(dt.search.terms[i], "", x, perl = TRUE)         print(x)         result$predicted[r] <- x     } }
search <- "is one of the" search <- clean.text(search) dt.search.terms = as.data.table(str_split(search, " "), stringsAsFactors = FALSE) result <- predictor(search)
for (r in 1:length(result$predicted)) {     for (i in 1:nrow(dt.search.terms)) {         x <- gsub(dt.search.terms[i], "", result$predicted[r], perl = TRUE)         print(x)         result$predicted[r] <- x     } }
result$predicted <- substr(result$predicted, (nchar(result$predicted) + 1) - n, nchar(result$predicted))
for (i in 1:length(result$predicted)) {     tmp <- as.data.table(str_split(result$predicted, " "), stringsAsFactors = FALSE)     last <- tmp[length(tmp)-1]     result$predicted[i] <- stringi::stri_sub(result$predicted[i],,) }
str_get_last_word <- function(str) {     tmp <- as.data.table(str_split(str, " "), stringsAsFactors = FALSE)     last <- tmp[length(tmp) - 1]     str <- stringi::stri_sub(str,,) }
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
str_get_last_word("asd dadaee dadkjlj-asdf dd")
ff <- str_get_last_word("asd dadaee dadkjlj-asdf dd")
ff
str_get_last_word <- function(str) {     tmp <- as.data.table(str_split(str, " "), stringsAsFactors = FALSE)     last <- tmp[length(tmp) - 1]     last }
ff <- str_get_last_word("asd dadaee dadkjlj-asdf dd") ff
ff <- str_get_last_word("asd dadaee dadkjlj-asdf dd")
ff
str_get_last_word <- function(str) {     tmp <- as.data.table(str_split(str, " "), stringsAsFactors = FALSE)     last <- tmp[length(tmp) - 1]     tail(last, n = 1) }
ff <- str_get_last_word("asd dadaee dadkjlj-asdf dd")
ff
print(last)
str_get_last_word <- function(str) {     tmp <- as.data.table(str_split(str, " "), stringsAsFactors = FALSE)     last <- tmp[length(tmp) - 1]     print(last)     tail(last, n = 1) }
ff <- str_get_last_word("asd dadaee dadkjlj-asdf dd")
ff
str_get_last_word <- function(str) {     tmp <- str_split(str, " ")     rev(tmp)[1] }
ff <- str_get_last_word("asd dadaee dadkjlj-asdf dd")
ff
str_get_last_word <- function(str) {     tmp <- str_split(str, " ")     tmp[length(tmp)] }
ff <- str_get_last_word("asd dadaee dadkjlj-asdf dd") ff
str_get_last_word <- function(str) {     tmp <- str_split(str, " ")     tmp[length(tmp)][1] }
ff <- str_get_last_word("asd dadaee dadkjlj-asdf dd")
ff
str_get_last_word <- function(str) {     tmp <- str_split(str, " ")     tmp[length(tmp)][[1]] }
ff <- str_get_last_word("asd dadaee dadkjlj-asdf dd") ff
str_get_last_word <- function(str) {     tmp <- str_split(str, " ")     dplyr::last(tmp) }
ff <- str_get_last_word("asd dadaee dadkjlj-asdf dd")
ff
str_get_last_word <- function(str) {     tmp <- str_split(str, " ")     print(tmp) }
ff <- str_get_last_word("asd dadaee dadkjlj-asdf dd")
ff
print(last(tmp))
rm(tmp)
str_get_last_word <- function(str) {     tmp <- str_split(str, " ")     print(last(tmp)) }
ff <- str_get_last_word("asd dadaee dadkjlj-asdf dd")
ff
tail(tmp,n=1)
str_get_last_word <- function(str) {     tmp <- str_split(str, " ")     tail(tmp,n=1) }
ff <- str_get_last_word("asd dadaee dadkjlj-asdf dd")
ff
str <- ("asd dadaee dadkjlj-asdf dd") tmp <- str_split(str, " ") tail(tmp, n = 1) ff
str <- ("asd dadaee dadkjlj-asdf dd") tmp <- str_split(str, " ") tmp[1]
as.character(tmp[1])
str <- ("asd dadaee dadkjlj-asdf dd") tail(strsplit(str, split = " ")[[1]], 1)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
for (i in 1:length(result$predicted)) {     result$predicted[i] <- str_get_last_word(result$predicted[i]) }
result$predicted[1] <- str_get_last_word(result$predicted[1])
result$predicted[2] <- str_get_last_word(result$predicted[2])
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
search <- "is one of the" search <- clean.text(search) dt.search.terms = as.data.table(str_split(search, " "), stringsAsFactors = FALSE) result <- predictor(search)
for (i in 1:length(result$predicted)) {     result$predicted[2] <- str_get_last_word(result$predicted[2]) }
for (i in 1:length(result$predicted)) {     result$predicted[i] <- str_get_last_word(result$predicted[i]) }
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
search <- "is one of the" search <- clean.text(search) dt.search.terms = as.data.table(str_split(search, " "), stringsAsFactors = FALSE) result <- predictor(search)
search <- "When you breathe, I want to be the air for you. I'll be there for you, I'd live and I'd" search <- clean.text(search) dt.search.terms = as.data.table(str_split(search, " "), stringsAsFactors = FALSE) result <- predictor(search)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
    for (i in 1:length(dt.search.result$predicted)) {         dt.search.result$predicted[i] <- str_get_last_word(dt.search.result$predicted[i])     }
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
predict.word <- function(dt.result) {     for (i in 1:length(dt.result[5])) {         dt.result[5][i] <- str_get_last_word(as.character(dt.result[5][i]))     }     dt.result }
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
search <- "a few" search <- clean.text(search) dt.search.terms = as.data.table(str_split(search, " "), stringsAsFactors = FALSE) result <- predictor(search) #result <- predict.word(result)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
sapply(dt.search.result$prediction, function(x) str_get_last_word(x))
sapply(dt.search.result, function(x) str_get_last_word(x$prediction))
sapply(dt.search.result, function(x) str_get_last_word(x[5]))
dt.search.result <- sapply(dt.search.result, function(x) str_get_last_word(x[5]))
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
for (i in 1:length(dt.search.result[5,])) {         dt.search.result[5,i] <- str_get_last_word(as.character(dt.search.result[5,i]))         #print(dt.search.result$predicted[i])     }
for (i in 1:length(dt.search.result)) {         dt.search.result[5,i] <- str_get_last_word(as.character(dt.search.result[5,i]))         #print(dt.search.result$predicted[i])     }
for (i in 1:nrow(dt.search.result)) {         dt.search.result[5,i] <- str_get_last_word(as.character(dt.search.result[5,i]))         #print(dt.search.result$predicted[i])     }
for (i in 1:length(dt.search.result$prediction)) {         dt.search.result[5,i] <- str_get_last_word(as.character(dt.search.result[5,i]))         #print(dt.search.result$predicted[i])     }
for (i in 1:length(dt.search.result)) {         dt.search.result[5,i] <- str_get_last_word(as.character(dt.search.result[5,i]))         #print(dt.search.result$predicted[i])     }
    for (i in 1:length(dt.search.result)) {         dt.search.result[[5,i]] <- str_get_last_word(as.character(dt.search.result[[5,i]]))         #print(dt.search.result$predicted[i])     }
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
    for (i in 1:length(dt.search.result)) {         dt.search.result[[5]][i]] <- str_get_last_word(as.character(dt.search.result[[5]][i]]))         #print(dt.search.result$predicted[i])     }          for (i in 1:length(dt.search.result)) {         dt.search.result[[5]][i]] <- str_get_last_word(as.character(dt.search.result[[5]][i]]))         #print(dt.search.result$predicted[i])     } search <- "a few" search <- clean.text(search) dt.search.terms = as.data.table(str_split(search, " "), stringsAsFactors = FALSE) result <- predictor(search)
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
=======
install.packages("futile.logger")
install.packages("devtools")
devtools::install_github("bmschmidt/wordVectors")
library(devtools)
install_github("mukul13/rword2vec")
library(rword2vec)
ls("package:rword2vec")
source("C:/dev/r-course/10-capstone/project_4__word2vec.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__word2vec.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/include.r", echo = TRUE, encoding = "utf-8")
source("C:/dev/r-course/10-capstone/project_4__word2vec.r", echo = TRUE, encoding = "Windows-1252")
suppressMessages(setwd("c:/dev/r-course/10-capstone/"))
source("C:\\dev\\r-course\\10-capstone\\project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_1__setup.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
library(devtools) install_github("mukul13/rword2vec")
library(rword2vec) ls("package:rword2vec")
source("C:/dev/r-course/10-capstone/project_1__setup.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
ana = word_analogy(file_name = "vec.bin", search_words = q1, num = 20)
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
pacman::p_load_current_gh("mlampros/fastTextR") library(fastTextR)
pacman::p_load_current_gh("mlampros/fastTextR") library(fastTextR)
source("C:/dev/r-course/10-capstone/project_4__word2vec.r", echo = TRUE, encoding = "Windows-1252")
res = skipgram_cbow(input_path = "data/final/en_US/en_US.blogs.txt",                     output_path = "/data/fasttext.model",                     method = "skipgram", lr = 0.1,                     lrUpdateRate = 100, dim = 100,                     ws = 5, epoch = 5, minCount = 1,                     neg = 5, wordNgrams = 1, loss = "ns",                     bucket = 2000000, minn = 0,                     maxn = 0, thread = 6, t = 0.0001,                     verbose = 2)
res = predict_unknown_words(skipgram_cbow_model_output = "/data/fasttext.model",                             output_path = "/data/fasttext/NEW_VEC",                             verbose = TRUE)
source("C:/dev/r-course/10-capstone/project_4__word2vec.r", echo = TRUE, encoding = "Windows-1252")
res = skipgram_cbow(input_path = "data/final/en_US/en_US.blogs.txt",                     output_path = "/data/fasttext.model",                     method = "skipgram", lr = 0.1,                     lrUpdateRate = 100, dim = 100,                     ws = 5, epoch = 5, minCount = 1,                     neg = 5, wordNgrams = 1, loss = "ns",                     bucket = 2000000, minn = 0,                     maxn = 0, thread = 6, t = 0.0001,                     verbose = 2)
res = skipgram_cbow(input_path = "data/final/en_US/en_US.blogs.txt",                     output_path = "/data/fasttext.model",                     method = "skipgram", lr = 0.1,                     lrUpdateRate = 100, dim = 100,                     ws = 5, epoch = 5, minCount = 1,                     neg = 5, wordNgrams = 1, loss = "ns",                     bucket = 2000000, minn = 0,                     maxn = 0, thread = 6, t = 0.0001,                     verbose = 2)
devtools::install_github('mlampros/fastTextR')
library(fastTextR)
res = skipgram_cbow(input_path = "data/final/en_US/en_US.blogs.txt",                     output_path = "/data/fasttext.model",                     method = "skipgram", lr = 0.1,                     lrUpdateRate = 100, dim = 100,                     ws = 5, epoch = 5, minCount = 1,                     neg = 5, wordNgrams = 1, loss = "ns",                     bucket = 2000000, minn = 0,                     maxn = 0, thread = 6, t = 0.0001,                     verbose = 2)
res = predict_unknown_words(skipgram_cbow_model_output = "/data/fasttext.model",                             output_path = "/data/fasttext/NEW_VEC",                             verbose = TRUE)
library(fastTextR)
res = skipgram_cbow(input_path = "data/final/en_US/en_US.blogs.txt",                     output_path = "/data/fasttext.model",                     method = "skipgram", lr = 0.1,                     lrUpdateRate = 100, dim = 100,                     ws = 5, epoch = 5, minCount = 1,                     neg = 5, wordNgrams = 1, loss = "ns",                     bucket = 2000000, minn = 0,                     maxn = 0, thread = 6, t = 0.0001,                     verbose = 2) res = predict_unknown_words(skipgram_cbow_model_output = "/data/fasttext.model",                             output_path = "/data/fasttext/NEW_VEC",                             verbose = TRUE)
source("C:/dev/r-course/10-capstone/project_4__word2vec.r", echo = TRUE, encoding = "Windows-1252")
devtools::install_github('mlampros/fastTextR') library(fastTextR) res = skipgram_cbow(input_path = "data/final/en_US/en_US.blogs.txt",                     output_path = "/data/fasttext.model",                     method = "skipgram", lr = 0.1,                     lrUpdateRate = 100, dim = 100,                     ws = 5, epoch = 5, minCount = 1,                     neg = 5, wordNgrams = 1, loss = "ns",                     bucket = 2000000, minn = 0,                     maxn = 0, thread = 6, t = 0.0001,                     verbose = 2) res = predict_unknown_words(skipgram_cbow_model_output = "/data/fasttext.model",                             output_path = "/data/fasttext/NEW_VEC",                             verbose = TRUE)
source("C:/dev/r-course/10-capstone/project_4__word2vec.r", echo = TRUE, encoding = "Windows-1252")
word2vec_file_name <- "word2vec.RData" word2vec_file_path <- get_data_file_path(word2vec_file_name) #if (!file.exists(word2vec_file_path)) {     #model = word2vec(train_file = "data/final/en_US/en_US.blogs.txt", output_file = "vec.bin", binary = 1) #} #load(get_data_file_path(word2vec_file_name)) #-------------------------- # skipgram or cbow methods #-------------------------- devtools::install_github('mlampros/fastTextR') library(fastTextR) res = skipgram_cbow(input_path = "data/final/en_US/en_US.blogs.txt",                     output_path = "/data/fasttext.model",                     method = "skipgram", lr = 0.1,                     lrUpdateRate = 100, dim = 100,                     ws = 5, epoch = 5, minCount = 1,                     neg = 5, wordNgrams = 1, loss = "ns",                     bucket = 2000000, minn = 0,                     maxn = 0, thread = 6, t = 0.0001,                     verbose = 2) res = predict_unknown_words(skipgram_cbow_model_output = "/data/fasttext.model",                             output_path = "/data/fasttext/NEW_VEC",                             verbose = TRUE)
source("C:/dev/r-course/10-capstone/project_4__word2vec.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
res = skipgram_cbow(input_path = "data/final/en_US/en_US.blogs.txt",                     output_path = "/data/fasttext.model",                     method = "skipgram", lr = 0.1,                     lrUpdateRate = 100, dim = 100,                     ws = 5, epoch = 5, minCount = 1,                     neg = 5, wordNgrams = 1, loss = "ns",                     bucket = 2000000, minn = 0,                     maxn = 0, thread = 6, t = 0.0001,                     verbose = 2) res = predict_unknown_words(skipgram_cbow_model_output = "/data/fasttext.model",                             output_path = "/data/fasttext/NEW_VEC",                             verbose = TRUE)
res
source("C:/dev/r-course/10-capstone/project_4__word2vec.r", echo = TRUE, encoding = "Windows-1252")
#word2vec_file_name <- "word2vec.RData" #word2vec_file_path <- get_data_file_path(word2vec_file_name) #if (!file.exists(word2vec_file_path)) {     #model = word2vec(train_file = "data/final/en_US/en_US.blogs.txt", output_file = "vec.bin", binary = 1) #} #load(get_data_file_path(word2vec_file_name)) #-------------------------- # skipgram or cbow methods #-------------------------- suppressMessages(setwd("c:/dev/r-course/10-capstone/")) devtools::install_github('mlampros/fastTextR') library(fastTextR) res = skipgram_cbow(input_path = "data/final/en_US/en_US.blogs.txt",                     output_path = "/data/fasttext.model",                     method = "skipgram", lr = 0.1,                     lrUpdateRate = 100, dim = 100,                     ws = 5, epoch = 5, minCount = 1,                     neg = 5, wordNgrams = 1, loss = "ns",                     bucket = 2000000, minn = 0,                     maxn = 0, thread = 6, t = 0.0001,                     verbose = 2) res = predict_unknown_words(skipgram_cbow_model_output = "/data/fasttext.model",                             output_path = "/data/fasttext/NEW_VEC",                             verbose = TRUE)
suppressMessages(setwd("c:/dev/r-course/10-capstone/"))
p_load("textTinyR")
library(textTinyR)
#word2vec_file_name <- "word2vec.RData" #word2vec_file_path <- get_data_file_path(word2vec_file_name) #if (!file.exists(word2vec_file_path)) {     #model = word2vec(train_file = "data/final/en_US/en_US.blogs.txt", output_file = "vec.bin", binary = 1) #} #load(get_data_file_path(word2vec_file_name)) #-------------------------- # skipgram or cbow methods #-------------------------- suppressMessages(setwd("c:/dev/r-course/10-capstone/")) p_load("textTinyR") library(textTinyR) subset = read_rows(input_file = PATH, read_delimiter = "\n",                    rows = 100,                    write_2file = "data/final/en_US/en_US.blogs.txt")
subset = read_rows(input_file = PATH, read_delimiter = "\n", rows = 100,  write_2file = "data/final/en_US/en_US.blogs.txt")
subset#word2vec_file_name <- "word2vec.RData" #word2vec_file_path <- get_data_file_path(word2vec_file_name) #if (!file.exists(word2vec_file_path)) {     #model = word2vec(train_file = "data/final/en_US/en_US.blogs.txt", output_file = "vec.bin", binary = 1) #} #load(get_data_file_path(word2vec_file_name)) #-------------------------- # skipgram or cbow methods #-------------------------- suppressMessages(setwd("c:/dev/r-course/10-capstone/")) p_load("textTinyR") library(textTinyR) sss = read_rows(input_file = PATH, read_delimiter = "\n", rows = 100,  write_2file = "data/final/en_US/en_US.blogs.txt")
sss
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
require(pacman)
p_load("textTinyR")
library(textTinyR)
sss = read_rows(input_file = PATH, read_delimiter = "\n", rows = 100,  write_2file = "data/final/en_US/en_US.blogs.txt")
sss = read_rows(input_file = "data/final/en_US/en_US.blogs.txt", read_delimiter = "\n", rows = 100, write_2file = "data")
#word2vec_file_name <- "word2vec.RData" #word2vec_file_path <- get_data_file_path(word2vec_file_name) #if (!file.exists(word2vec_file_path)) {     #model = word2vec(train_file = "data/final/en_US/en_US.blogs.txt", output_file = "vec.bin", binary = 1) #} #load(get_data_file_path(word2vec_file_name)) #-------------------------- # skipgram or cbow methods #-------------------------- suppressMessages(setwd("c:/dev/r-course/10-capstone/")) require(pacman) p_load("textTinyR") library(textTinyR) sss = read_rows(input_file = "data/final/en_US/en_US.blogs.txt", read_delimiter = "\n", rows = 100, write_2file = "data/output.bin")
suppressMessages(setwd("c:/dev/r-course/10-capstone/")) require(pacman) p_load("textTinyR") library(textTinyR)
sss = read_rows(input_file = "data/final/en_US/en_US.blogs.txt", read_delimiter = "\n", rows = 100, write_2file = "data/output.bin")
#word2vec_file_name <- "word2vec.RData" #word2vec_file_path <- get_data_file_path(word2vec_file_name) #if (!file.exists(word2vec_file_path)) {     #model = word2vec(train_file = "data/final/en_US/en_US.blogs.txt", output_file = "vec.bin", binary = 1) #} #load(get_data_file_path(word2vec_file_name)) #-------------------------- # skipgram or cbow methods #-------------------------- suppressMessages(setwd("c:/dev/r-course/10-capstone/")) require(pacman) p_load("textTinyR") library(textTinyR) sss = read_rows(input_file = "data/final/en_US/en_US.blogs.txt", read_delimiter = "\n", rows = 100, write_2file = "data/output.bin")
sss = read_rows(input_file = "data/final/en_US/en_US.blogs.txt", write_2file = "data/output.bin")
sss = read_rows(input_file = "c:/dev/r-course/10-capstone/data/final/en_US/en_US.blogs.txt", write_2file = "data/output.bin")
source("C:/dev/r-course/10-capstone/project_4__word2vec.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__word2vec.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_1__setup.r", echo = TRUE, encoding = "Windows-1252")
pacman::p_load_current_gh("mlampros/fastTextR")
source("C:/dev/r-course/10-capstone/project_1__setup.r", echo = TRUE, encoding = "Windows-1252")
using("fastrtext")
pacman::p_load_current_gh("pommedeterresautee/fastrtext")
install.packages("pacman")
pacman::p_load_current_gh("pommedeterresautee/fastrtext")
p_load_current_gh("pommedeterresautee/fastrtext")
install_github("pommedeterresautee/fastrtext")
install_github("pommedeterresautee/fastrtext") library(fastTextR)
library(fastrtext)
devtools::install_github("pommedeterresautee/fastrtext")
require(fastrtext)
devtools::install_github("pommedeterresautee/fastrtext")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
p_load("text2vec")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
  data.stringified <- paste(data.all, collapse = '')
it <- itoken(data.stringified, preprocess_function = tolower,              tokenizer = word_tokenizer, chunks_number = 10, progessbar = F)
source("C:/dev/r-course/10-capstone/project_4__tokenization.r", echo = TRUE, encoding = "Windows-1252")
vocab <- vocab %>%     prune_vocabulary(term_count_min = 10, doc_proportion_max = 0.5, doc_proportion_min = 0.001)
source("C:/dev/r-course/10-capstone/project_4__tokenization.r", echo = TRUE, encoding = "Windows-1252")
t1 <- Sys.time() vocab <- vocabulary(src = it, ngram = c(1L, 3L))
source("C:/dev/r-course/10-capstone/project_4__tokenization.r", echo = TRUE, encoding = "Windows-1252")
it <- itoken(data.stringified, preprocess_function = tolower,              tokenizer = word_tokenizer, chunks_number = 10, progessbar = F)
source("C:/dev/r-course/10-capstone/project_4__tokenization.r", echo = TRUE, encoding = "Windows-1252")
 it <- itoken(data.stringified, preprocess_function = tolower, tokenizer = word_tokenizer, chunks_number = 10, progessbar = F)
it
source("C:/dev/r-course/10-capstone/project_4__tokenization.r", echo = TRUE, encoding = "Windows-1252")
df.tokens <- itoken(data.stringified, preprocess_function = tolower, tokenizer = word_tokenizer, chunks_number = 10, progessbar = F)
vocab <- vocabulary(src = df.tokens, ngram = c(1L, 3L))
source("C:/dev/r-course/10-capstone/project_4__tokenization.r", echo = TRUE, encoding = "Windows-1252")
p_load("text2vec")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__tokenization.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__tokenization.r", echo = TRUE, encoding = "Windows-1252")
it <- itoken(data.stringified, preprocess_function = tolower, tokenizer = word_tokenizer, chunks_number = 10, progessbar = F)
data.stringified <- paste(data.all, collapse = '') vocab <- vocabulary(src = it, ngram = c(1L, 3L))
vocab <- vocabulary(src = it, ngram = c(1L, 3L))
vocab <- vocabulary(it, ngram = c(1L, 3L))
fh <- feature_hasher(hash_size = 2 ** 18, ngram = c(1L, 3L))
vocab <- create_vocabulary(it, ngram = c(1L, 3L))
source("C:/dev/r-course/10-capstone/project_4__tokenization.r", echo = TRUE, encoding = "Windows-1252")
>>>>>>> 6cb9cea540a62888912aec733dcedef23da732d7
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__tokenization.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__tokenization.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__tokenization.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__tokenization.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__tokenization.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_4__tokenization.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_2__data_retrieval.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_2__data_retrieval.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/project_0_run.r", echo = TRUE, encoding = "Windows-1252")
source("C:/dev/r-course/10-capstone/predictor.R", echo = TRUE, encoding = "Windows-1252")
result
predictor(q1)
source("C:/dev/r-course/10-capstone/prediction.tests.R", echo = TRUE, encoding = "Windows-1252")
predictor(q1)
predictor(q2)
source("C:/dev/r-course/10-capstone/prediction.tests.R", echo = TRUE, encoding = "Windows-1252")
