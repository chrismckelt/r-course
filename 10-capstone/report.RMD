---
title: "<b>Final Report for DataScience Capstone</b>"
subtitle: Word prediction with Natural Language Processing
author: "Chris McKelt"
date: "December 2017"
output:
  revealjs::revealjs_presentation:
    theme: solarized
    highlight: pygments
    center: true

---

```{r setup, include=FALSE}
suppressMessages(rm(list = ls()))
Sys.setenv(JAVA_HOME = 'C:\\Program Files\\Java\\jre1.8.0_151\\')
 
suppressMessages(setwd("c:/dev/r-course/10-capstone/"))
options(mc.cores = 1)
options(encoding = "UTF-8")
options(stringsAsFactors = FALSE)
```


## Introduction
This report outlines the methodology for building a word prediction application using Natural Language Processing techniques as part of the the <a href="https://www.coursera.org/specializations/jhu-data-science">Coursera Data Science Specialization</a>.

The essence of the Capstone project is to create an application that uses NLP techniques and predictive analytics, and like SwiftKey's applications, takes in a word phrase and returns next-predicted word.

The <a href="http://chrismckelt.github.io/r-course/10-capstone/milestone/milestone-report.html">milestone report</a> outlines the initial approach to building a series of <a href="https://en.wikipedia.org/wiki/N-gram">ngram models</a> from a range of text documents.


## Overview of the application
The application was developed in R using a number of packages and the Shiny web framework.
Below outlines the methodology used to build,predict and evaluate the application.

#### NGram model 
    1. Sample text taken from SwiftKey corpus data (15% of original)
    2. The text was cleaned from non-ASCII characters, derogatory language, punctuation, non words and   extra whitespace
    3. The corpus was processed to produce 5 ngram models which were morphed into a data table together with the totalled frequence of the ngram (bag of words).

## Prediction 
A prediction function then takes a sentence as input and execute the below steps
    1. Validates and clean the input sentence (using the same 'clean_text' function to build the ngram models)
    2. For each ngram gram model take the N last words for the input text where N is the size-1 of the ngram model. For example:
![image](https://user-images.githubusercontent.com/662868/33523738-e134d69a-d848-11e7-9868-ed3d2992e81b.png)

####  Description of the algorithm used to make the prediction
With a data table containing the ngram model, sentence, frequency and predicted word, the top 3 most probable words are predicted using a <a href="http://www.aclweb.org/anthology/D07-1090.pdf">Stupid Backoff</a> smoothing strategy.
 
A pseudo code description to calculate the 'score' for each word follows:

    if the rows ngram model was 5

      score = matched 5 gram Count / input 4 gram Count

    else if the rows ngram model was 4

      score = 0.4 * matched 4 gram Count / input 3 gram Count

    else if the rows ngram model was 3

      score = 0.4 * 0.4 * matched 3 gram Count / input 2 gram Count

    else if the rows ngram model was 2

      score = 0.4 * 0.4 * 0.4 * matched 2 gram Count / input 1 gram Count


Finally we group and sum similar words 

For example if the predicted word 'you' was found in ngram 4 (and thus ngram 3 & 2) it may look like

|   ngram | predicted   | score   |    
|---|---|---|---|
|  4 | you  |  0.2 |
|  3 | you  |  0.1 |
|  2 | you  |  0.05 |

The total score for the predicted word 'you' is (0.2 + 0.1 + 0.05) = 0.35

The final scoring is aggregated and summed for each word.  The top 3 words are selected according to the highest score.

When no results are found the 3 most common words from the English language ('the', 'be', 'to') are returned as a response.

## Evaluation
The prediction model was evaluated using the Benchmark.R tool (see references for source).

Initial predicts were quite high but also quite slow.  The decision to only using 1-3 ngram models to speed up the search cut the time in half and only dropped the accuracy by 10%. 

![image](https://user-images.githubusercontent.com/662868/33319651-68536484-d47a-11e7-9fdb-7e5f029235c6.png)

## Instructions 
To use the application navigate to the following URL
 
 <a href="https://chrismckelt.shinyapps.io/datascience-capstone/">https://chrismckelt.shinyapps.io/datascience-capstone/ </a>

Start typing in text 

![image](https://user-images.githubusercontent.com/662868/33524020-99dec59c-d84f-11e7-9efa-eba99d83fadf.png)

For access to the code please contact the author using one of the contact links on the site.

## References
[Speech and Language Processing, by D. Jurafsky &amp; al, Chapter 4, Draft of January 9, 2015](https://web.stanford.edu/~jurafsky/slp3/)

[JHU DS Capstone Swiftkey Dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)

[Large language models in machine translation by T. Brants et al, in EMNLP/CoNLL 2007](http://www.aclweb.org/anthology/D07-1090.pdf)

[Next word prediction benchmark](https://github.com/hfoffani/dsci-benchmark)