---
title: "<b>Milestone Report</b>"
subtitle: "Analysis of the data for the Coursea DataScience Capstone project"
author: "Chris McKelt"
date: "November 2017"
html_document:
    theme: united
    highlight: tango
---

```{r setup, include=FALSE}
suppressMessages(rm(list = ls()))
Sys.setenv(JAVA_HOME = 'C:\\Program Files\\Java\\jre1.8.0_151\\')
source('c:/dev/r-course/10-capstone/include.r')
library(pacman)
install_standard_packages()
using("tm")
using("rJava")
using("RWeka")
using("RWeka")
using("stringi")
using("NLP")
using("SnowballC")
using("RColorBrewer")
using("wordcloud")
using("igraph")
using("fpc")
using("biclust")
using("cluster")
using("quanteda")
using("Rgraphviz")
using("Matrix")
using("dplyr")
library(Matrix)
suppressMessages(setwd("c:/dev/r-course/10-capstone/"))
options(mc.cores = 1)
options(encoding = "UTF-8")
options(stringsAsFactors = FALSE)
```

# Overview

This is a milestone report for the Coursera Data Science unit to build predictive model for text input.

The goal of the Capstone project is to create an application that uses NLP techniques and predictive analytics, and like SwiftKey's applications, takes in a word phrase and returns next-word choices. 

The project is developed in partnership with SwiftKey as the company well known for their predictive text analytics. As of March 1, 2016, SwiftKey became part of the Microsoft family of products. SwiftKey applications are used on Android and iOS anticipating and providing next-word choices while keyboard typing through Natural Language Processing (NLP) techniques. Microsoft's Word Flow Technology is another example of NLP in action.

The milestone reports purpose is to identify the initial steps taken to produce the overall capstone project, a Natural Language Processing word prediction alogrithm application. 
 
# Introduction

The data for analysis to be used is from

https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

This analysis is based on three text files containing internet blogs, news feeds, and Twitter tweets from HCCorpora.   

This report will outline the process for:
1. Retrieving the data
2. Cleaning up the data
3. The tokenizing functions used to find the most frequently used word (unigrams), word expressions (bigrams,trigrams,n-grams)
4. Exploratory analyse of the data
5. Future plans for a prediction application

 

## Retrieve the data

```{r, echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE}

download_zip_files <- function() {
    url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
    if (!file.exists("data.zip")) {
        save_file(url, "data.zip")
        unzip(zipfile = "c:/dev/r-course/10-capstone/data.zip", exdir = "c:/dev/r-course/10-capstone/final/")
    }
}

smaller <- function(x) {
    x <- sample(x, length(x) * 0.0009999)
}

read_file <- function(path) {
    con <- file(path, open = "rb")
    data <- readLines(con, skipNul = TRUE, encoding = "UTF-8")
    close(con)
    data
}

download_zip_files()

```

Having downloaded and extracted the zip file, we then read the files and adjust for encoding.  
A sample of each text is taken to optimise performance of the analyse.

```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}

data.blogs <- read_file("final/en_US/en_US.blogs.txt")
data.news <- read_file("final/en_US/en_US.news.txt")
data.twitter <- read_file("final/en_US/en_US.twitter.txt")
data.blogs = iconv(data.blogs, "latin1", "ASCII", sub = "")
data.news = iconv(data.news, "latin1", "ASCII", sub = "")
data.twitter = iconv(data.twitter, "latin1", "ASCII", sub = "")

sample.blogs <- smaller(data.blogs)
sample.news <- smaller(data.news)
sample.twitter <- smaller(data.twitter)
sample.all <- c(sample.blogs, sample.news, sample.twitter)

```

# Overview of the original data

#### The texts inside the zip file we will analyse are under the 'en_US' folder

![image](https://user-images.githubusercontent.com/662868/32357634-a02e21c4-c079-11e7-9136-8277964e9f8b.png)

```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
    data_summary <- data.frame(
    Blogs=c((summary(nchar(data.blogs)))),News =c((summary(nchar(data.news)))),
    Twitter =c((summary(nchar(data.twitter)))))
    kable(data_summary)
```

## Clean the text to create a corpus we can use to build a model

#### Remove numbers, urls, whitespace, punctuation, and non-useful text (eg lines '-----')

```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}

	group_texts <- list(sample.blogs, sample.news, sample.twitter)
	group_texts <- tolower(group_texts)
	group_texts <- removeNumbers(group_texts)
	group_texts <- removePunctuation(group_texts, preserve_intra_word_dashes = TRUE)
	group_texts <- gsub("http[[:alnum:]]*", "", group_texts)
	group_texts <- stripWhitespace(group_texts)
	group_texts <- gsub("\u0092", "'", group_texts)
	group_texts <- gsub("\u0093|\u0094", "", group_texts)

	corpus.data <- VCorpus(VectorSource(group_texts))
	toEmpty <- content_transformer(function(x, pattern) gsub(pattern, "", x, fixed = TRUE))
	toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x, fixed = TRUE))
	corpus.data <- tm_map(corpus.data, toEmpty, "#\\w+")
	corpus.data <- tm_map(corpus.data, toEmpty, "(\\b\\S+\\@\\S+\\..{1,3}(\\s)?\\b)")
	corpus.data <- tm_map(corpus.data, toEmpty, "@\\w+")
	corpus.data <- tm_map(corpus.data, toEmpty, "http[^[:space:]]*")
	corpus.data <- tm_map(corpus.data, toSpace, "/|@|\\|")
 
``` 
#### Remove derogatory words from a known bad word list

```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}

	save_file("https://goo.gl/To9w5B", "bad_word_list.txt")
	bad_words <- readLines("./bad_word_list.txt")
	corpus.data <- tm_map(corpus.data, removeWords, bad_words)
 
``` 
#### Stem words in a text document using Porter's stemming algorithm
##### So the word 'mining' was stemmed to 'mine'

```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}


	corpus.data <- tm_map(corpus.data, stemDocument)
 
``` 



## Explore the data
 
Tokenize the sample to get with Uni and Bigrams to perform an N-grams analysis.  
This will allow us to see the most used words and expressions. 
 
```{r, echo=FALSE,eval=TRUE, warning=FALSE, message=FALSE}
   
    gram1Tokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 1, max = 1)) }
    gram2Tokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2)) }
    gram3Tokenizer <- function(x) { RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3))}

    tdm1 <- TermDocumentMatrix(corpus.data, control = list(tokenize = gram1Tokenizer))
    tdm2 <- TermDocumentMatrix(corpus.data, control = list(tokenize = gram2Tokenizer))
    tdm3 <- TermDocumentMatrix(corpus.data, control = list(tokenize = gram3Tokenizer))

    gram1freq <- data.frame(word = tdm1$dimnames$Terms, freq = rowSums(sparseMatrix(i = tdm1$i, j = tdm1$j, x = tdm1$v)))
    gram1freq <- as.data.frame(sqldf("select * from gram1freq order by freq desc limit 10"))

    gram2freq <- data.frame(word = tdm2$dimnames$Terms, freq = rowSums(sparseMatrix(i = tdm2$i, j = tdm2$j, x = tdm2$v)))
    gram2freq <- sqldf("select * from gram2freq order by freq desc limit 10")

    gram3freq <- data.frame(word = tdm3$dimnames$Terms, freq = rowSums(sparseMatrix(i = tdm3$i, j = tdm3$j, x = tdm3$v)))
    gram3freq <- sqldf("select * from gram3freq order by freq desc limit 10")

``` 
 
 Uni-grams - frequent single words
```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
	
	g1 <- ggplot(gram1freq, aes(x = word, y = freq)) +
		geom_bar(stat = "identity", fill = "red") +
		ggtitle("1-gram") +
		xlab("1-grams") + ylab("Frequency")
	g1

``` 
Bi-gram - two words phrases
```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
	g2 <- ggplot(gram2freq, aes(x = word, y = freq)) +
		geom_bar(stat = "identity", fill = "yellow") +
		ggtitle("2-gram") +
		xlab("2-grams") + ylab("Frequency")
	g2
``` 
Tri-gram - three words phrases
```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
	g3 <- ggplot(gram3freq, aes(x = word, y = freq)) +
		geom_bar(stat = "identity", fill = "blue") +
		ggtitle("3-gram") +
		xlab("3-grams") + ylab("Frequency")
	g3
``` 
 

## Wordcloud

Following is a wordcloud of unigrams and distributions of 2- and 3-gram phrases. These are calculated from combining 100 samples from each of the three text files into one corpus. Numbers, stopwords, whitespace, punctuation, and derogatory items have been removed. Although, it appears there remains extraneous characters yet to be cleaned.

```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}

create_wordcloud <- function(tdm, palette = "Dark2") {
    mtx = as.matrix(tdm)
    # get word counts in decreasing order
    word_freqs = sort(rowSums(mtx), decreasing = TRUE)
    # create a data frame with words and their frequencies
    dm = data.frame(word = names(word_freqs), freq = word_freqs)
	dm <- sqldf("select * from dm limit 100")
    # plot wordcloud
    wordcloud(dm$word, dm$freq, random.order = FALSE, colors = brewer.pal(8, palette))
}
create_wordcloud(tdm1, "Set1")
create_wordcloud(tdm2, "Set2")
create_wordcloud(tdm3, "Set3")

``` 


##  Observations from the analysis

- The data needs more cleaning to deal with encodings and foreign language characters
- Bias may exist in twitter text as it is a shorter form than news/blog text. Need to account for this and nullify it.
- Stemming appears useful as bi/tri grams are made of short words that have a better frequence for matching when stemmed.

## Future plans for creating a prediction algorithm and Shiny app

Using the data in this analyse a Katz back off n-gram model will used to build a predictive text input application.

The Katz back-off is a generative n-gram language model that estimates the conditional probability of a word given its history in the n-gram. It accomplishes this estimation by "backing-off" to models with smaller histories under certain conditions. By doing so, the model with the most reliable information about a given history is used to provide the better results.

This will be hosted in a ShinyApp web application that accepts user input and outputs the next predicted words.