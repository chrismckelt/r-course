---
title: "<b>Milestone Report</b>"
subtitle: "Use the arrow keys to navigate slides"
author: "Chris McKelt"
date: "October 2017"
output:
  revealjs::revealjs_presentation:
    theme: solarized
    highlight: pygments
    center: true
---

```{r setup, include=FALSE}
suppressMessages(rm(list = ls()))
options(encoding = "UTF-8")

options(stringsAsFactors = FALSE)
source('c:/dev/r-course/10-capstone/include.r')
library(pacman)
install_standard_packages()
p_load("tm")
p_load("SnowballC")
p_load("RColorBrewer")
p_load("wordcloud")
p_load("igraph")
p_load("fpc")
p_load("biclust")
p_load("cluster")
p_load("quanteda")
p_load("Rgraphviz")
p_load("Rcpp")
 
suppressMessages(setwd("c:/dev/r-course/10-capstone/"))

```

## Overview

This is a milestone report for the Coursera Data Science unit to build predictive model for text input.
The predictive model could be a combination of probabilistic models (N-grams, others), and rule-based models 


## Gather the data
```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}

download_zip_files <- function() {
    url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
    if (!file.exists("data.zip")) {
        save_file(url, "data.zip")
        unzip(zipfile = "c:/dev/r-course/10-capstone/data.zip", exdir = "c:/dev/r-course/10-capstone/final/")
    }
}

read_file <- function(path) {
    con <- file(path, encoding = "UTF-8" )
    data <- readLines(con)
    close(con)
    rsm(con)
    data
}

download_zip_files()

### Read files and adjust for encoding
data.blogs <- read_file("final/en_US/en_US.blogs.txt")
data.news <- read_file("final/en_US/en_US.news.txt")
data.twitter <- read_file("final/en_US/en_US.twitter.txt")
### Sample the data to speed up execution
sample.blogs <- sample(data.blogs, 20000)
sample.news <- sample(data.news, 20000)
sample.twitter <- sample(data.twitter, 20000)
sample.all <- sample(c(sample.blogs, sample.news, sample.twitter), size = 10000, replace = TRUE)
```


## Build and clean the corpus
Create a corpus document from the given texts and clean up to create a NGram model we can analyse.

```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}

    lst <- list(sample.blogs, sample.news, sample.twitter)
    corpus.data <- Corpus(VectorSource(lst))
    toEmpty <- content_transformer(function(x, pattern) gsub(pattern, "", x, fixed = TRUE))
    toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x, fixed = TRUE))

    corpus.data <- tm_map(corpus.data, tolower)
    corpus.data <- tm_map(corpus.data, removePunctuation)
    corpus.data <- tm_map(corpus.data, removeNumbers)
    corpus.data <- tm_map(corpus.data, removeWords, stopwords("english"))
    corpus.data <- tm_map(corpus.data, stripWhitespace)  
    corpus.data <- tm_map(corpus.data, toEmpty, "#\\w+")  
    corpus.data <- tm_map(corpus.data, toEmpty, "(\\b\\S+\\@\\S+\\..{1,3}(\\s)?\\b)")  
    corpus.data <- tm_map(corpus.data, toEmpty, "@\\w+")  
    corpus.data <- tm_map(corpus.data, toEmpty, "http[^[:space:]]*")  
    corpus.data <- tm_map(corpus.data, toSpace, "/|@|\\|")
``` 

## Explore the data

Create a DocumentTermMatrix and show the number of documents (3) and the total terms.
 
```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
    dtm <- DocumentTermMatrix(corpus.data)
    nDocs(dtm)
    nTerms(dtm)
``` 

Tokenize the sample to get with Uni and Bigrams to perform an N-grams analysis.  
This will allow us to see the most used words and expressions. 
 
```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
   
    tokenizer.uni <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
    tokenizer.bi <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
    
    matrix.uni <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.uni))
    matrix.bi <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.bi))

    freq.terms <- findFreqTerms(matrix.uni, 10)
    freq.terms
    freq.expressions <- findFreqTerms(matrix.bi, 10)
    sfreq.expressions
    plot(dtm, terms = freq.terms[1:5], corThreshold = 0.5)
``` 
Below we show the top 5 words used in the blogs,news and twitter texts.

```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
    #https://stackoverflow.com/questions/17294824/counting-words-in-a-single-document-from-corpus-in-r-and-putting-it-in-dataframe
    df <- as.data.frame(as.matrix(dtm))
    # and transpose for plotting
    df <- data.frame(t(df))
    setDT(df, keep.rownames = TRUE)[]

    cols <- c("word", "blog_count", "news_count", "twitter_count")
    colnames(df) <- cols
    top.blogwords <- sqldf("select word, blog_count from df order by blog_count desc limit 5")
    top.newswords <- sqldf("select word, news_count from df order by news_count desc limit 5")
    top.twitterwords <- sqldf("select word,twitter_count from df order by twitter_count desc limit 5")

    # Top blog words
    top.blogwords
    # Top news words
    top.newswords
    # Top twitter words
    top.twitterwords
``` 
