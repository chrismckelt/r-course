---
title: "<b>Milestone Report</b>"
subtitle: "Use the arrow keys to navigate slides"
author: "Chris McKelt"
date: "November 2017"
output:
  revealjs::revealjs_presentation:
    theme: solarized
    highlight: pygments
    center: true
---

```{r setup, include=FALSE}
suppressMessages(rm(list = ls()))
options(encoding = "UTF-8")

options(stringsAsFactors = FALSE)
source('c:/dev/r-course/10-capstone/include.r')
library(pacman)
install_standard_packages()
p_load("tm")
p_load("SnowballC")
p_load("RColorBrewer")
p_load("wordcloud")
p_load("igraph")
p_load("fpc")
p_load("biclust")
p_load("cluster")
p_load("quanteda")
p_load("Rgraphviz")
p_load("Rcpp")
p_load("stringi")
 
suppressMessages(setwd("c:/dev/r-course/10-capstone/"))

```

## Overview

This is a milestone report for the Coursera Data Science unit to build predictive model for text input.

The predictive model could be a combination of probabilistic models (N-grams, others), and rule-based models 

The data for analysis to be used is from

https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

The texts inside the zip file we will analyse are under the 'en_US' folder

![image](https://user-images.githubusercontent.com/662868/32357634-a02e21c4-c079-11e7-9136-8277964e9f8b.png)

## Gather the data

```{r, echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE}

download_zip_files <- function() {
    url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
    if (!file.exists("data.zip")) {
        save_file(url, "data.zip")
        unzip(zipfile = "c:/dev/r-course/10-capstone/data.zip", exdir = "c:/dev/r-course/10-capstone/final/")
    }
}

read_file <- function(path) {
    con <- file(path, encoding = "UTF-8" )
    data <- readLines(con)
    close(con)
    data
}

download_zip_files()
```

```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
### Read files and adjust for encoding
data.blogs <- read_file("final/en_US/en_US.blogs.txt")
data.news <- read_file("final/en_US/en_US.news.txt")
data.twitter <- read_file("final/en_US/en_US.twitter.txt")
### Sample the data to speed up execution
sample.blogs <- sample(data.blogs, 20000)
sample.news <- sample(data.news, 20000)
sample.twitter <- sample(data.twitter, 20000)
sample.all <- sample(c(sample.blogs, sample.news, sample.twitter), size = 10000, replace = TRUE)

```

# Data overview
```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
    data_summary <- data.frame(
    Blogs=c((summary(nchar(data.blogs)))),News =c((summary(nchar(data.news)))),
    Twitter =c((summary(nchar(data.twitter)))))
    kable(data_summary)
```

## Build and clean the corpus
Create a corpus document from the given texts and clean up to create a NGram model we can analyse.

```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}

    lst <- list(sample.blogs, sample.news, sample.twitter)
    corpus.data <- Corpus(VectorSource(lst))
    toEmpty <- content_transformer(function(x, pattern) gsub(pattern, "", x, fixed = TRUE))
    toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x, fixed = TRUE))

    corpus.data <- tm_map(corpus.data, tolower)
    corpus.data <- tm_map(corpus.data, removePunctuation)
    corpus.data <- tm_map(corpus.data, removeNumbers)
    corpus.data <- tm_map(corpus.data, removeWords, stopwords("english"))
    corpus.data <- tm_map(corpus.data, stripWhitespace)  
    corpus.data <- tm_map(corpus.data, toEmpty, "#\\w+")  
    corpus.data <- tm_map(corpus.data, toEmpty, "(\\b\\S+\\@\\S+\\..{1,3}(\\s)?\\b)")  
    corpus.data <- tm_map(corpus.data, toEmpty, "@\\w+")  
    corpus.data <- tm_map(corpus.data, toEmpty, "http[^[:space:]]*")  
    corpus.data <- tm_map(corpus.data, toSpace, "/|@|\\|")

    dtm <- DocumentTermMatrix(corpus.data)
    ###Total documents
    nDocs(dtm)
    ###Total terms
    nTerms(dtm)
``` 

## Explore the data
 
Tokenize the sample to get with Uni and Bigrams to perform an N-grams analysis.  
This will allow us to see the most used words and expressions. 
 
```{r, echo=FALSE,eval=TRUE, warning=FALSE, message=FALSE}
   
    tokenizer.uni <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
    tokenizer.bi <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
    tokenizer.tri <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))

    matrix.uni <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.uni))
    matrix.bi <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.bi))
    matrix.tri <- DocumentTermMatrix(corpus.data, control = list(tokenize = tokenizer.tri))

    freq.uni <- findFreqTerms(matrix.uni, 5,5)
    freq.bi <- findFreqTerms(matrix.bi,  5,5)
    freq.tri <- findFreqTerms(matrix.bi,  5,5)

``` 

## Below we show the top 5 words used in the blogs,news and twitter texts.

```{r, echo=FALSE, eval=TRUE, warning=FALSE, message=FALSE}
    df <- as.data.frame(as.matrix(dtm))
    df <- data.frame(t(df))
    setDT(df, keep.rownames = TRUE)[]

    cols <- c("word", "blog_count", "news_count", "twitter_count")
    colnames(df) <- cols
    top.blogwords <- sqldf("select word, blog_count from df order by blog_count desc limit 3")
    top.newswords <- sqldf("select word, news_count from df order by news_count desc limit 3")
    top.twitterwords <- sqldf("select word,twitter_count from df order by twitter_count desc limit 3")
``` 
Top 3 words for blog,news and twitter documents
```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}

    ### Top blog words
    top.blogwords
    ### Top news words
    top.newswords
    ### Top twitter words
    top.twitterwords
``` 
## Wordcloud
Analyse via a wordcloud can illustrate word frequencies very effectively as Unigram, Bigram, Trigram and Tetragram is as follows respectively:-
```{r, echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE}

   # par(mfrow=c(1,1))
    #wordcloud(matrix.uni, freq.uni, scale=c(9,1), max.words=40, 
    #random.order=FALSE, colors=brewer.pal(7, "Dark2"))
``` 